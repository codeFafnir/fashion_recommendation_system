{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788e950f",
   "metadata": {},
   "source": [
    "# BERT4Rec Experiment: Fashion Recommender on H&M Dataset\n",
    "\n",
    "This notebook implements a **BERT4Rec-style sequential recommender** on top of the existing H&M pipeline, optimized to run locally on a **MacBook M4 Air (16GB)**:\n",
    "\n",
    "- Start from processed data (transactions + features) under `fashion_recommender_candidate_generation_2`.\n",
    "- Build per-user interaction sequences with careful filtering and capping for memory efficiency.\n",
    "- Train a lightweight BERT4Rec model in **PyTorch** using the Apple Silicon **MPS** backend when available.\n",
    "- Evaluate ranking quality with **MAP@12** on the existing validation/test candidate sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb00e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Config ready\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "\"\"\"Core imports and configuration for BERT4Rec on H&M, optimized for M4 Air (16 GB).\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Paths and basic config\n",
    "# ---------------------------------------------------------------------------\n",
    "BASE_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2')\n",
    "PROC_PATH = BASE_PATH / 'fashion_recommender_candidate_generation_2'\n",
    "MODELS_PATH = PROC_PATH / 'models'\n",
    "FEATURES_PATH = BASE_PATH / 'fashion_recommender_features_2'\n",
    "\n",
    "# These can be tweaked if your local column names differ\n",
    "USER_COL = 'customer_id'\n",
    "ITEM_COL = 'article_id'\n",
    "TIME_COL = 't_dat'            # from *_transactions.parquet\n",
    "LABEL_COL = 'label'           # used in candidate sets for MAP@12\n",
    "\n",
    "# Device: prefer Apple Silicon GPU (MPS) when available\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "# For evaluation (MAP@12), force CPU to avoid missing MPS ops in Transformer\n",
    "EVAL_DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device for training: {DEVICE}\")\n",
    "print(f\"Using device for evaluation: {EVAL_DEVICE}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Sequence building hyperparameters tuned for 16 GB M4 Air\n",
    "# ---------------------------------------------------------------------------\n",
    "MAX_SEQ_LEN = 100        # cap history length per user for memory/speed\n",
    "MIN_USER_INTERACTIONS = 5  # drop very sparse users\n",
    "MAX_USERS = 300_000        # optional cap on number of users for experiments\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "print(\"Config ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106514c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train transactions: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/train_transactions.parquet\n",
      "Val transactions:   /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/val_transactions.parquet\n",
      "Training features:  /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/training_features.parquet\n",
      "Train transactions shape: (412156, 3)\n",
      "Val transactions shape:   (16480, 3)\n",
      "After aligning with training_features (head), train shape: (101761, 3), val shape: (3043, 3)\n",
      "Users: 11,766, Items (including specials): 14,571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fj/50pwwywx2v9fjh51s89qfxmc0000gn/T/ipykernel_23042/4042527318.py:68: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for user, group in train_tx.groupby(USER_COL, sort=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built sequences for 6,025 users with >= 5 interactions\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "\"\"\"Build per-user interaction sequences from transactions for BERT4Rec.\n",
    "\n",
    "We:\n",
    "- Use train_transactions.parquet as the source of positive interactions.\n",
    "- Optionally intersect users/items with training_features.parquet for consistency.\n",
    "- Filter users with very short histories and cap max sequence length.\n",
    "This keeps memory down while leveraging the rich H&M dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Paths to raw interaction data and features\n",
    "train_tx_path = PROC_PATH / 'train_transactions.parquet'\n",
    "val_tx_path = PROC_PATH / 'val_transactions.parquet'\n",
    "train_feat_path = PROC_PATH / 'training_features.parquet'  # used mainly for ID overlap if needed\n",
    "\n",
    "print(f\"Train transactions: {train_tx_path}\")\n",
    "print(f\"Val transactions:   {val_tx_path}\")\n",
    "print(f\"Training features:  {train_feat_path}\")\n",
    "\n",
    "# Load transactions with only necessary columns to save memory\n",
    "train_tx = pd.read_parquet(train_tx_path, columns=[USER_COL, ITEM_COL, TIME_COL])\n",
    "val_tx = pd.read_parquet(val_tx_path, columns=[USER_COL, ITEM_COL, TIME_COL])\n",
    "\n",
    "# Ensure proper dtypes\n",
    "for df in (train_tx, val_tx):\n",
    "    df[USER_COL] = df[USER_COL].astype('category')\n",
    "    df[ITEM_COL] = df[ITEM_COL].astype('category')\n",
    "    df[TIME_COL] = pd.to_datetime(df[TIME_COL])\n",
    "\n",
    "print(f\"Train transactions shape: {train_tx.shape}\")\n",
    "print(f\"Val transactions shape:   {val_tx.shape}\")\n",
    "\n",
    "# Optionally, intersect with users present in training_features to align with existing pipeline\n",
    "try:\n",
    "    train_feats_head = pd.read_parquet(train_feat_path, columns=[USER_COL, ITEM_COL]).head(1_000_000)\n",
    "    feat_users = set(train_feats_head[USER_COL].unique())\n",
    "    train_tx = train_tx[train_tx[USER_COL].isin(feat_users)]\n",
    "    val_tx = val_tx[val_tx[USER_COL].isin(feat_users)]\n",
    "    print(f\"After aligning with training_features (head), train shape: {train_tx.shape}, val shape: {val_tx.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not align with training_features (optional step), proceeding anyway: {e}\")\n",
    "\n",
    "# Build item and user integer vocabularies\n",
    "all_items = pd.concat([train_tx[ITEM_COL], val_tx[ITEM_COL]]).unique()\n",
    "item2idx = {item: idx + 3 for idx, item in enumerate(all_items)}  # reserve 0:[PAD], 1:[MASK], 2:[UNK]\n",
    "idx2item = {idx: item for item, idx in item2idx.items()}\n",
    "\n",
    "all_users = train_tx[USER_COL].unique()\n",
    "if MAX_USERS is not None and len(all_users) > MAX_USERS:\n",
    "    # Subsample users for feasibility\n",
    "    all_users = np.random.choice(all_users, size=MAX_USERS, replace=False)\n",
    "\n",
    "user_set = set(all_users)\n",
    "train_tx = train_tx[train_tx[USER_COL].isin(user_set)]\n",
    "val_tx = val_tx[val_tx[USER_COL].isin(user_set)]\n",
    "\n",
    "user2idx = {user: idx for idx, user in enumerate(all_users)}\n",
    "idx2user = {idx: user for user, idx in user2idx.items()}\n",
    "\n",
    "n_users = len(user2idx)\n",
    "n_items = len(item2idx) + 3  # including special tokens\n",
    "print(f\"Users: {n_users:,}, Items (including specials): {n_items:,}\")\n",
    "\n",
    "# Sort interactions by time and build sequences\n",
    "train_tx = train_tx.sort_values([USER_COL, TIME_COL])\n",
    "\n",
    "user_sequences = {}\n",
    "for user, group in train_tx.groupby(USER_COL, sort=False):\n",
    "    item_ids = [item2idx.get(i, 2) for i in group[ITEM_COL].tolist()]  # 2 = [UNK]\n",
    "    if len(item_ids) < MIN_USER_INTERACTIONS:\n",
    "        continue\n",
    "    # Keep only the most recent MAX_SEQ_LEN interactions\n",
    "    if len(item_ids) > MAX_SEQ_LEN:\n",
    "        item_ids = item_ids[-MAX_SEQ_LEN:]\n",
    "    user_sequences[user2idx[user]] = item_ids\n",
    "\n",
    "print(f\"Built sequences for {len(user_sequences):,} users with >= {MIN_USER_INTERACTIONS} interactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4c8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert4RecModel(\n",
      "  (item_embedding): Embedding(14571, 128, padding_idx=0)\n",
      "  (pos_embedding): Embedding(100, 128)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=128, out_features=14571, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "\"\"\"Dataset and BERT4Rec model definition.\n",
    "\n",
    "We implement a minimal BERT4Rec-style masked item prediction model:\n",
    "- Input: tokenized user sequence with [PAD]=0, [MASK]=1, [UNK]=2.\n",
    "- Randomly mask a fraction of positions and predict original items.\n",
    "- For inference, we typically mask the last position to get next-item scores.\n",
    "\"\"\"\n",
    "\n",
    "MASK_TOKEN_ID = 1\n",
    "PAD_TOKEN_ID = 0\n",
    "UNK_TOKEN_ID = 2\n",
    "\n",
    "\n",
    "class Bert4RecDataset(Dataset):\n",
    "    \"\"\"Generates masked sequences for BERT4Rec from pre-built user sequences.\n",
    "\n",
    "    user_sequences: dict[user_idx -> list[int]]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, user_sequences, max_seq_len=MAX_SEQ_LEN, mask_prob=0.15):\n",
    "        self.user_ids = list(user_sequences.keys())\n",
    "        self.sequences = user_sequences\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.user_ids[idx]\n",
    "        seq = self.sequences[user_id]\n",
    "\n",
    "        # Pad/truncate to max_seq_len\n",
    "        seq = seq[-self.max_seq_len :]\n",
    "        pad_len = self.max_seq_len - len(seq)\n",
    "        if pad_len > 0:\n",
    "            seq = [PAD_TOKEN_ID] * pad_len + seq\n",
    "\n",
    "        input_ids = np.array(seq, dtype=np.int64)\n",
    "        labels = np.full_like(input_ids, fill_value=-100)  # ignore index\n",
    "\n",
    "        # Apply BERT-style masking\n",
    "        mask = np.random.rand(len(input_ids)) < self.mask_prob\n",
    "        # don't mask PAD tokens\n",
    "        mask[input_ids == PAD_TOKEN_ID] = False\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            if not mask[i]:\n",
    "                continue\n",
    "            original_id = input_ids[i]\n",
    "            labels[i] = original_id\n",
    "            prob = np.random.rand()\n",
    "            if prob < 0.8:\n",
    "                input_ids[i] = MASK_TOKEN_ID\n",
    "            elif prob < 0.9:\n",
    "                # replace with random item id (excluding specials)\n",
    "                input_ids[i] = np.random.randint(3, n_items)\n",
    "            else:\n",
    "                # keep original\n",
    "                pass\n",
    "\n",
    "        return {\n",
    "            'user_id': np.int64(user_id),\n",
    "            'input_ids': torch.from_numpy(input_ids),\n",
    "            'labels': torch.from_numpy(labels),\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_bert4rec(batch):\n",
    "    input_ids = torch.stack([b['input_ids'] for b in batch], dim=0)\n",
    "    labels = torch.stack([b['labels'] for b in batch], dim=0)\n",
    "    user_ids = torch.tensor([b['user_id'] for b in batch], dtype=torch.long)\n",
    "    return {\n",
    "        'user_ids': user_ids,\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "\n",
    "class Bert4RecModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_items: int,\n",
    "        d_model: int = 128,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 2,\n",
    "        max_seq_len: int = MAX_SEQ_LEN,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_items = num_items\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.item_embedding = nn.Embedding(num_items, d_model, padding_idx=PAD_TOKEN_ID)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Prediction head: project hidden states back to item vocab\n",
    "        self.output_layer = nn.Linear(d_model, num_items)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"input_ids: (batch, seq_len)\"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "\n",
    "        x = self.item_embedding(input_ids) + self.pos_embedding(positions)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Generate padding mask: True for PAD positions\n",
    "        pad_mask = input_ids.eq(PAD_TOKEN_ID)\n",
    "\n",
    "        x = self.encoder(x, src_key_padding_mask=pad_mask)\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = Bert4RecModel(num_items=n_items, d_model=128, n_heads=4, n_layers=2, max_seq_len=MAX_SEQ_LEN)\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470155cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fe127c4e8a4a77bb88d0a7e383e57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train loss: 9.5985\n",
      "Loading candidate data from /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/val_data.parquet\n",
      "Validation MAP@12 evaluation failed (check paths/columns): The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ce3c5b9ecb45719ba65ae53ccce241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train loss: 9.3225\n",
      "Loading candidate data from /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/val_data.parquet\n",
      "Validation MAP@12 evaluation failed (check paths/columns): The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858cafa670c04a38bae45bff6d5002b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train loss: 9.2321\n",
      "Loading candidate data from /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/val_data.parquet\n",
      "Validation MAP@12 evaluation failed (check paths/columns): The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502663ea4e7b4e0298031c9b2c67eb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train loss: 9.1827\n",
      "Loading candidate data from /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/val_data.parquet\n",
      "Validation MAP@12 evaluation failed (check paths/columns): The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8e1385f98b443f99436d8b4593b409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train loss: 9.1577\n",
      "Loading candidate data from /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/val_data.parquet\n",
      "Validation MAP@12 evaluation failed (check paths/columns): The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
      "Loading candidate data from /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/test_data.parquet\n",
      "Test MAP@12 evaluation failed (check paths/columns): The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 5811a8d7da873dd699ff6687092c225caffcf1bb. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "\"\"\"Training loop and MAP@12 evaluation on validation/test candidate sets.\n",
    "\n",
    "We assume candidate sets in:\n",
    "- MODELS_PATH / 'val_data.parquet'\n",
    "- MODELS_PATH / 'test_data.parquet'\n",
    "with columns [USER_COL, ITEM_COL, LABEL_COL] where LABEL_COL is 1 for the\n",
    "true purchased item(s) and 0 otherwise.\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_dataloaders():\n",
    "    dataset = Bert4RecDataset(user_sequences, max_seq_len=MAX_SEQ_LEN, mask_prob=0.15)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_bert4rec,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "\n",
    "train_loader = get_dataloaders()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, scheduler=None, epoch: int = 1, total_epochs: int = 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch}/{total_epochs}\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)  # (batch, seq_len, n_items)\n",
    "\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability on small devices\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "def build_user_latest_context(val_df):\n",
    "    \"\"\"Build latest sequence context per user from train+val transactions.\n",
    "\n",
    "    For each user in val/test, we take their sequence from user_sequences\n",
    "    (based on train data). If needed, you can also append their earliest\n",
    "    val interactions for more context.\n",
    "    \"\"\"\n",
    "    users_in_val = val_df[USER_COL].unique()\n",
    "    user_context = {}\n",
    "    for user in users_in_val:\n",
    "        if user not in user2idx:\n",
    "            continue\n",
    "        uidx = user2idx[user]\n",
    "        seq = user_sequences.get(uidx, [])\n",
    "        # Pad/truncate\n",
    "        seq = seq[-MAX_SEQ_LEN:]\n",
    "        pad_len = MAX_SEQ_LEN - len(seq)\n",
    "        if pad_len > 0:\n",
    "            seq = [PAD_TOKEN_ID] * pad_len + seq\n",
    "        user_context[uidx] = np.array(seq, dtype=np.int64)\n",
    "    return user_context\n",
    "\n",
    "\n",
    "def score_candidates(model, candidate_df, user_context):\n",
    "    \"\"\"Score candidate (user, item) pairs using BERT4Rec next-item scores.\n",
    "\n",
    "    We:\n",
    "    - For each user, take their context sequence and mask the last position.\n",
    "    - Run model once per user to get logits over items at last position.\n",
    "    - Use the resulting scores to rank candidate items.\n",
    "\n",
    "    This function assumes `model` is already on `EVAL_DEVICE` (CPU).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Build mapping: user_idx -> list of (row_idx, item_idx)\n",
    "    user_to_items = defaultdict(list)\n",
    "    for row_idx, row in candidate_df.iterrows():\n",
    "        user = row[USER_COL]\n",
    "        item = row[ITEM_COL]\n",
    "        if user not in user2idx:\n",
    "            continue\n",
    "        uidx = user2idx[user]\n",
    "        item_idx = item2idx.get(item, UNK_TOKEN_ID)\n",
    "        user_to_items[uidx].append((row_idx, item_idx))\n",
    "\n",
    "    scores = np.zeros(len(candidate_df), dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for uidx, pairs in user_to_items.items():\n",
    "            context = user_context.get(uidx)\n",
    "            if context is None:\n",
    "                continue\n",
    "\n",
    "            # Mask last non-PAD position\n",
    "            seq = context.copy()\n",
    "            last_pos = np.where(seq != PAD_TOKEN_ID)[0]\n",
    "            if len(last_pos) == 0:\n",
    "                continue\n",
    "            last_pos = last_pos[-1]\n",
    "            original_token = seq[last_pos]\n",
    "            seq[last_pos] = MASK_TOKEN_ID\n",
    "\n",
    "            input_ids = torch.from_numpy(seq[None, :]).to(EVAL_DEVICE)\n",
    "            logits = model(input_ids)  # (1, seq_len, n_items)\n",
    "            logits_last = logits[0, last_pos]  # (n_items,)\n",
    "\n",
    "            # Convert to CPU numpy\n",
    "            logits_last = logits_last.detach().cpu().numpy()\n",
    "\n",
    "            for row_idx, item_idx in pairs:\n",
    "                if item_idx >= len(logits_last):\n",
    "                    scores[row_idx] = -1e9\n",
    "                else:\n",
    "                    scores[row_idx] = logits_last[item_idx]\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def map_at_k(df, user_col, label_col, score_col, k=12):\n",
    "    \"\"\"Compute MAP@k given a dataframe with per-(user, item) scores and labels.\"\"\"\n",
    "    df = df.sort_values([user_col, score_col], ascending=[True, False])\n",
    "\n",
    "    ap_sum = 0.0\n",
    "    n_users = 0\n",
    "\n",
    "    for user, group in df.groupby(user_col, sort=False):\n",
    "        labels = group[label_col].values\n",
    "        scores = group[score_col].values  # noqa: F841\n",
    "\n",
    "        # Indices of positives within top-k\n",
    "        topk = min(k, len(labels))\n",
    "        rel = labels[:topk]\n",
    "        if rel.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # AP@k\n",
    "        precisions = []\n",
    "        hits = 0\n",
    "        for i in range(topk):\n",
    "            if rel[i] == 1:\n",
    "                hits += 1\n",
    "                precisions.append(hits / (i + 1))\n",
    "        if precisions:\n",
    "            ap = np.mean(precisions)\n",
    "            ap_sum += ap\n",
    "            n_users += 1\n",
    "\n",
    "    return ap_sum / max(n_users, 1)\n",
    "\n",
    "\n",
    "def evaluate_map12(model, split='val'):\n",
    "    fname = 'val_data.parquet' if split == 'val' else 'test_data.parquet'\n",
    "    path = MODELS_PATH / fname\n",
    "    print(f\"Loading candidate data from {path}\")\n",
    "\n",
    "    candidate_df = pd.read_parquet(path)\n",
    "\n",
    "    # Move model to eval device (CPU) for scoring to avoid missing MPS ops\n",
    "    model_was_training = model.training\n",
    "    model.to(EVAL_DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Build user context from train sequences (could be cached)\n",
    "    user_context = build_user_latest_context(candidate_df)\n",
    "\n",
    "    # Score candidates\n",
    "    scores = score_candidates(model, candidate_df, user_context)\n",
    "    candidate_df = candidate_df.copy()\n",
    "    candidate_df['score'] = scores\n",
    "\n",
    "    map12 = map_at_k(candidate_df, user_col=USER_COL, label_col=LABEL_COL, score_col='score', k=12)\n",
    "    print(f\"{split.upper()} MAP@12: {map12:.6f}\")\n",
    "\n",
    "    # Move model back to training device if needed\n",
    "    model.to(DEVICE)\n",
    "    if model_was_training:\n",
    "        model.train()\n",
    "\n",
    "    return map12\n",
    "\n",
    "\n",
    "# Simple training loop over a few epochs\n",
    "EPOCHS = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS * len(train_loader))\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, epoch=epoch, total_epochs=EPOCHS)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} - Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Lightweight validation after each epoch\n",
    "    try:\n",
    "        evaluate_map12(model, split='val')\n",
    "    except Exception as e:\n",
    "        print(f\"Validation MAP@12 evaluation failed (check paths/columns): {e}\")\n",
    "\n",
    "# Final test evaluation (optional)\n",
    "try:\n",
    "    evaluate_map12(model, split='test')\n",
    "except Exception as e:\n",
    "    print(f\"Test MAP@12 evaluation failed (check paths/columns): {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc750df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "assignment2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
