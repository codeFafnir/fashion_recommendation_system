{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77906c9",
   "metadata": {},
   "source": [
    "### Stage 0: EDA and Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cfc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EDA: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for presentation-quality plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Configuration\n",
    "EDA_CONFIG = {\n",
    "    'DATA_PATH': Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/h-and-m-personalized-fashion-recommendations'),\n",
    "    'PROCESSED_PATH': Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2'),\n",
    "    'MODEL_PATH': Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models'),\n",
    "    'OUTPUT_DIR': Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/eda_plots')\n",
    "}\n",
    "\n",
    "EDA_CONFIG['OUTPUT_DIR'].mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"âœ“ EDA Configuration loaded\")\n",
    "print(f\"  Output directory: {EDA_CONFIG['OUTPUT_DIR']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EDA 1: RAW H&M DATASET ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA: RAW H&M DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load raw data\n",
    "print(\"\\n Loading raw dataset...\")\n",
    "try:\n",
    "    transactions = pd.read_csv(EDA_CONFIG['DATA_PATH'] / 'transactions_train.csv', \n",
    "                              dtype={'article_id': str, 'customer_id': str})\n",
    "    articles = pd.read_csv(EDA_CONFIG['DATA_PATH'] / 'articles.csv')\n",
    "    customers = pd.read_csv(EDA_CONFIG['DATA_PATH'] / 'customers.csv')\n",
    "    \n",
    "    print(f\"âœ“ Transactions: {len(transactions):,} rows\")\n",
    "    print(f\"âœ“ Articles: {len(articles):,} items\")\n",
    "    print(f\"âœ“ Customers: {len(customers):,} users\")\n",
    "    \n",
    "    # Convert date\n",
    "    transactions['t_dat'] = pd.to_datetime(transactions['t_dat'])\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nðŸ“ˆ Dataset Statistics:\")\n",
    "    print(f\"  Date range: {transactions['t_dat'].min()} to {transactions['t_dat'].max()}\")\n",
    "    print(f\"  Unique users: {transactions['customer_id'].nunique():,}\")\n",
    "    print(f\"  Unique items: {transactions['article_id'].nunique():,}\")\n",
    "    print(f\"  Total purchases: {len(transactions):,}\")\n",
    "    print(f\"  Avg purchases per user: {len(transactions) / transactions['customer_id'].nunique():.2f}\")\n",
    "    print(f\"  Avg purchases per item: {len(transactions) / transactions['article_id'].nunique():.2f}\")\n",
    "    \n",
    "    # 1. Transaction Volume Over Time\n",
    "    print(\"\\n Creating temporal analysis plots...\")\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Daily transaction volume\n",
    "    daily_trans = transactions.groupby(transactions['t_dat'].dt.date).size()\n",
    "    axes[0, 0].plot(daily_trans.index, daily_trans.values, linewidth=2)\n",
    "    axes[0, 0].set_title('Daily Transaction Volume', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Number of Transactions')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Weekly transaction volume\n",
    "    weekly_trans = transactions.groupby(transactions['t_dat'].dt.isocalendar().week).size()\n",
    "    axes[0, 1].bar(weekly_trans.index, weekly_trans.values, alpha=0.7)\n",
    "    axes[0, 1].set_title('Weekly Transaction Volume', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Week Number')\n",
    "    axes[0, 1].set_ylabel('Number of Transactions')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # User activity distribution\n",
    "    user_activity = transactions.groupby('customer_id').size()\n",
    "    axes[1, 0].hist(user_activity.values, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_title('User Purchase Activity Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Number of Purchases per User')\n",
    "    axes[1, 0].set_ylabel('Number of Users')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Item popularity distribution\n",
    "    item_popularity = transactions.groupby('article_id').size()\n",
    "    axes[1, 1].hist(item_popularity.values, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_title('Item Popularity Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Number of Purchases per Item')\n",
    "    axes[1, 1].set_ylabel('Number of Items')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '1_raw_dataset_temporal.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved: 1_raw_dataset_temporal.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Category Analysis\n",
    "    print(\"\\n Creating category analysis plots...\")\n",
    "    if 'article_id' in articles.columns and 'product_type_name' in articles.columns:\n",
    "        # Convert article_id to string in articles to match transactions\n",
    "        articles_for_merge = articles[['article_id', 'product_type_name', 'product_group_name']].copy()\n",
    "        articles_for_merge['article_id'] = articles_for_merge['article_id'].astype(str)\n",
    "        \n",
    "        # Merge transactions with articles\n",
    "        trans_articles = transactions.merge(articles_for_merge, \n",
    "                                           on='article_id', how='left')\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Top product types\n",
    "        top_product_types = trans_articles['product_type_name'].value_counts().head(15)\n",
    "        axes[0].barh(range(len(top_product_types)), top_product_types.values, alpha=0.7)\n",
    "        axes[0].set_yticks(range(len(top_product_types)))\n",
    "        axes[0].set_yticklabels(top_product_types.index, fontsize=9)\n",
    "        axes[0].set_title('Top 15 Product Types by Sales', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('Number of Transactions')\n",
    "        axes[0].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Top product groups\n",
    "        top_product_groups = trans_articles['product_group_name'].value_counts().head(15)\n",
    "        axes[1].barh(range(len(top_product_groups)), top_product_groups.values, alpha=0.7, color='green')\n",
    "        axes[1].set_yticks(range(len(top_product_groups)))\n",
    "        axes[1].set_yticklabels(top_product_groups.index, fontsize=9)\n",
    "        axes[1].set_title('Top 15 Product Groups by Sales', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Number of Transactions')\n",
    "        axes[1].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '2_raw_dataset_categories.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: 2_raw_dataset_categories.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. User Behavior Analysis\n",
    "    print(\"\\n Creating user behavior analysis...\")\n",
    "    user_stats = transactions.groupby('customer_id').agg({\n",
    "        'article_id': ['count', 'nunique'],\n",
    "        't_dat': ['min', 'max']\n",
    "    }).reset_index()\n",
    "    user_stats.columns = ['customer_id', 'total_purchases', 'unique_items', 'first_purchase', 'last_purchase']\n",
    "    user_stats['purchase_span_days'] = (user_stats['last_purchase'] - user_stats['first_purchase']).dt.days\n",
    "    user_stats['avg_items_per_purchase'] = user_stats['total_purchases'] / user_stats['unique_items']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Purchase count distribution\n",
    "    axes[0, 0].hist(user_stats['total_purchases'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_title('Distribution of Total Purchases per User', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Total Purchases')\n",
    "    axes[0, 0].set_ylabel('Number of Users')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Unique items distribution\n",
    "    axes[0, 1].hist(user_stats['unique_items'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_title('Distribution of Unique Items per User', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Unique Items Purchased')\n",
    "    axes[0, 1].set_ylabel('Number of Users')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Purchase span\n",
    "    axes[1, 0].hist(user_stats['purchase_span_days'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[1, 0].set_title('Distribution of Purchase Span (Days)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Days Between First and Last Purchase')\n",
    "    axes[1, 0].set_ylabel('Number of Users')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Scatter: Purchases vs Unique Items\n",
    "    axes[1, 1].scatter(user_stats['total_purchases'], user_stats['unique_items'], \n",
    "                       alpha=0.3, s=10, edgecolors='none')\n",
    "    axes[1, 1].set_title('Purchases vs Unique Items', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Total Purchases')\n",
    "    axes[1, 1].set_ylabel('Unique Items')\n",
    "    axes[1, 1].set_xscale('log')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '3_raw_dataset_user_behavior.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved: 3_raw_dataset_user_behavior.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n Raw Dataset EDA Complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error loading raw data: {e}\")\n",
    "    print(\"   Using processed data instead...\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee2e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EDA 2: TRAINING DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA: TRAINING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load training data\n",
    "print(\"\\n Loading training data...\")\n",
    "try:\n",
    "    train_data = pd.read_parquet(EDA_CONFIG['MODEL_PATH'] / 'train_data.parquet')\n",
    "    print(f\"âœ“ Loaded {len(train_data):,} training samples\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n Training Data Statistics:\")\n",
    "    print(f\"  Total samples: {len(train_data):,}\")\n",
    "    print(f\"  Unique users: {train_data['customer_id'].nunique():,}\")\n",
    "    print(f\"  Unique items: {train_data['article_id'].nunique():,}\")\n",
    "    print(f\"  Positive samples: {train_data['label'].sum():,} ({100*train_data['label'].mean():.2f}%)\")\n",
    "    print(f\"  Negative samples: {(train_data['label']==0).sum():,} ({100*(1-train_data['label'].mean()):.2f}%)\")\n",
    "    \n",
    "    # 1. Label Distribution\n",
    "    print(\"\\n Creating label distribution plots...\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Pie chart\n",
    "    label_counts = train_data['label'].value_counts()\n",
    "    axes[0].pie(label_counts.values, labels=['Negative (0)', 'Positive (1)'], \n",
    "                autopct='%1.1f%%', startangle=90, colors=['#ff9999', '#66b3ff'])\n",
    "    axes[0].set_title('Label Distribution (Pie Chart)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Bar chart\n",
    "    axes[1].bar(['Negative (0)', 'Positive (1)'], label_counts.values, \n",
    "                color=['#ff9999', '#66b3ff'], alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('Label Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Number of Samples')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    for i, v in enumerate(label_counts.values):\n",
    "        axes[1].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '4_train_label_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved: 4_train_label_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. User Activity in Training Data\n",
    "    print(\"\\n Creating user activity analysis...\")\n",
    "    user_train_stats = train_data.groupby('customer_id').agg({\n",
    "        'article_id': 'nunique',\n",
    "        'label': 'sum'\n",
    "    }).reset_index()\n",
    "    user_train_stats.columns = ['customer_id', 'candidate_items', 'positive_items']\n",
    "    user_train_stats['positive_ratio'] = user_train_stats['positive_items'] / user_train_stats['candidate_items']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Candidate items per user\n",
    "    axes[0, 0].hist(user_train_stats['candidate_items'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_title('Distribution of Candidate Items per User', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Number of Candidate Items')\n",
    "    axes[0, 0].set_ylabel('Number of Users')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Positive items per user\n",
    "    axes[0, 1].hist(user_train_stats['positive_items'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Distribution of Positive Items per User', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Number of Positive Items')\n",
    "    axes[0, 1].set_ylabel('Number of Users')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Positive ratio distribution\n",
    "    axes[1, 0].hist(user_train_stats['positive_ratio'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1, 0].set_title('Distribution of Positive Ratio per User', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Positive Ratio (Positive / Total Candidates)')\n",
    "    axes[1, 0].set_ylabel('Number of Users')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Scatter: Candidates vs Positives\n",
    "    axes[1, 1].scatter(user_train_stats['candidate_items'], user_train_stats['positive_items'],\n",
    "                       alpha=0.3, s=10, edgecolors='none')\n",
    "    axes[1, 1].set_title('Candidate Items vs Positive Items', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Number of Candidate Items')\n",
    "    axes[1, 1].set_ylabel('Number of Positive Items')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '5_train_user_activity.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved: 5_train_user_activity.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Item Popularity in Training Data\n",
    "    print(\"\\n Creating item popularity analysis...\")\n",
    "    item_train_stats = train_data.groupby('article_id').agg({\n",
    "        'customer_id': 'nunique',\n",
    "        'label': 'sum'\n",
    "    }).reset_index()\n",
    "    item_train_stats.columns = ['article_id', 'unique_users', 'positive_labels']\n",
    "    item_train_stats['positive_ratio'] = item_train_stats['positive_labels'] / item_train_stats['unique_users']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Items per user distribution\n",
    "    axes[0, 0].hist(item_train_stats['unique_users'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_title('Distribution of Users per Item', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Number of Users')\n",
    "    axes[0, 0].set_ylabel('Number of Items')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Positive labels per item\n",
    "    axes[0, 1].hist(item_train_stats['positive_labels'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('Distribution of Positive Labels per Item', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Number of Positive Labels')\n",
    "    axes[0, 1].set_ylabel('Number of Items')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Top items by positive labels\n",
    "    top_items = item_train_stats.nlargest(20, 'positive_labels')\n",
    "    axes[1, 0].barh(range(len(top_items)), top_items['positive_labels'].values, alpha=0.7)\n",
    "    axes[1, 0].set_yticks(range(len(top_items)))\n",
    "    axes[1, 0].set_yticklabels([f\"Item {idx}\" for idx in top_items['article_id'].values], fontsize=8)\n",
    "    axes[1, 0].set_title('Top 20 Items by Positive Labels', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Number of Positive Labels')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Scatter: Users vs Positive Labels\n",
    "    axes[1, 1].scatter(item_train_stats['unique_users'], item_train_stats['positive_labels'],\n",
    "                       alpha=0.3, s=10, edgecolors='none')\n",
    "    axes[1, 1].set_title('Users vs Positive Labels per Item', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Number of Users')\n",
    "    axes[1, 1].set_ylabel('Number of Positive Labels')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '6_train_item_popularity.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved: 6_train_item_popularity.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Feature Statistics (if available)\n",
    "    print(\"\\nðŸ“Š Creating feature statistics...\")\n",
    "    feature_cols = [col for col in train_data.columns if col not in \n",
    "                    ['customer_id', 'article_id', 'label', 'user_type', 'train_label', 'val_label']]\n",
    "    \n",
    "    if len(feature_cols) > 0:\n",
    "        # Select numeric features for analysis\n",
    "        numeric_features = train_data[feature_cols].select_dtypes(include=[np.number]).columns[:20]  # Top 20\n",
    "        \n",
    "        if len(numeric_features) > 0:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            \n",
    "            # Feature value ranges\n",
    "            feature_ranges = train_data[numeric_features].describe().loc[['min', 'max']].T\n",
    "            feature_ranges['range'] = feature_ranges['max'] - feature_ranges['min']\n",
    "            top_ranges = feature_ranges.nlargest(15, 'range')\n",
    "            \n",
    "            axes[0, 0].barh(range(len(top_ranges)), top_ranges['range'].values, alpha=0.7)\n",
    "            axes[0, 0].set_yticks(range(len(top_ranges)))\n",
    "            axes[0, 0].set_yticklabels(top_ranges.index, fontsize=8)\n",
    "            axes[0, 0].set_title('Top 15 Features by Value Range', fontsize=14, fontweight='bold')\n",
    "            axes[0, 0].set_xlabel('Value Range (Max - Min)')\n",
    "            axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Feature means\n",
    "            feature_means = train_data[numeric_features].mean().sort_values(ascending=False).head(15)\n",
    "            axes[0, 1].barh(range(len(feature_means)), feature_means.values, alpha=0.7, color='green')\n",
    "            axes[0, 1].set_yticks(range(len(feature_means)))\n",
    "            axes[0, 1].set_yticklabels(feature_means.index, fontsize=8)\n",
    "            axes[0, 1].set_title('Top 15 Features by Mean Value', fontsize=14, fontweight='bold')\n",
    "            axes[0, 1].set_xlabel('Mean Value')\n",
    "            axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Feature correlation with label (top correlated)\n",
    "            correlations = train_data[numeric_features].corrwith(train_data['label']).abs().sort_values(ascending=False).head(15)\n",
    "            axes[1, 0].barh(range(len(correlations)), correlations.values, alpha=0.7, color='orange')\n",
    "            axes[1, 0].set_yticks(range(len(correlations)))\n",
    "            axes[1, 0].set_yticklabels(correlations.index, fontsize=8)\n",
    "            axes[1, 0].set_title('Top 15 Features Correlated with Label', fontsize=14, fontweight='bold')\n",
    "            axes[1, 0].set_xlabel('Absolute Correlation with Label')\n",
    "            axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # Feature variance\n",
    "            feature_vars = train_data[numeric_features].var().sort_values(ascending=False).head(15)\n",
    "            axes[1, 1].barh(range(len(feature_vars)), feature_vars.values, alpha=0.7, color='purple')\n",
    "            axes[1, 1].set_yticks(range(len(feature_vars)))\n",
    "            axes[1, 1].set_yticklabels(feature_vars.index, fontsize=8)\n",
    "            axes[1, 1].set_title('Top 15 Features by Variance', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Variance')\n",
    "            axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '7_train_feature_statistics.png', dpi=300, bbox_inches='tight')\n",
    "            print(f\"âœ“ Saved: 7_train_feature_statistics.png\")\n",
    "            plt.close()\n",
    "    \n",
    "    print(\"\\n Training Data EDA Complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error loading training data: {e}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ccd1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EDA 3: VALIDATION DATA ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA: VALIDATION DATA\")\n",
    "print(\"=\"*80)\n",
    "# Load validation data\n",
    "print(\"\\n Loading validation data...\")\n",
    "try:\n",
    "    val_data = pd.read_parquet(EDA_CONFIG['MODEL_PATH'] / 'val_data.parquet')\n",
    "    print(f\"âœ“ Loaded {len(val_data):,} validation samples\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\n Validation Data Statistics:\")\n",
    "    print(f\"  Total samples: {len(val_data):,}\")\n",
    "    print(f\"  Unique users: {val_data['customer_id'].nunique():,}\")\n",
    "    print(f\"  Unique items: {val_data['article_id'].nunique():,}\")\n",
    "    print(f\"  Positive samples: {val_data['label'].sum():,} ({100*val_data['label'].mean():.2f}%)\")\n",
    "    print(f\"  Negative samples: {(val_data['label']==0).sum():,} ({100*(1-val_data['label'].mean()):.2f}%)\")\n",
    "    \n",
    "    # 1. Label Distribution Comparison\n",
    "    print(\"\\n Creating train vs validation comparison...\")\n",
    "    try:\n",
    "        train_data = pd.read_parquet(EDA_CONFIG['MODEL_PATH'] / 'train_data.parquet')\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Train label distribution\n",
    "        train_label_counts = train_data['label'].value_counts()\n",
    "        axes[0].bar(['Negative (0)', 'Positive (1)'], train_label_counts.values,\n",
    "                    color=['#ff9999', '#66b3ff'], alpha=0.7, edgecolor='black')\n",
    "        axes[0].set_title('Training Data Label Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_ylabel('Number of Samples')\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        for i, v in enumerate(train_label_counts.values):\n",
    "            axes[0].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Validation label distribution\n",
    "        val_label_counts = val_data['label'].value_counts()\n",
    "        axes[1].bar(['Negative (0)', 'Positive (1)'], val_label_counts.values,\n",
    "                    color=['#ff9999', '#66b3ff'], alpha=0.7, edgecolor='black')\n",
    "        axes[1].set_title('Validation Data Label Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_ylabel('Number of Samples')\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        for i, v in enumerate(val_label_counts.values):\n",
    "            axes[1].text(i, v, f'{v:,}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '8_train_val_label_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: 8_train_val_label_comparison.png\")\n",
    "        plt.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 2. Validation User Analysis\n",
    "    print(\"\\n Creating validation user analysis...\")\n",
    "    val_user_stats = val_data.groupby('customer_id').agg({\n",
    "        'article_id': 'nunique',\n",
    "        'label': 'sum'\n",
    "    }).reset_index()\n",
    "    val_user_stats.columns = ['customer_id', 'candidate_items', 'positive_items']\n",
    "    val_user_stats['positive_ratio'] = val_user_stats['positive_items'] / val_user_stats['candidate_items']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Candidate items per user\n",
    "    axes[0, 0].hist(val_user_stats['candidate_items'], bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "    axes[0, 0].set_title('Distribution of Candidate Items per User (Validation)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Number of Candidate Items')\n",
    "    axes[0, 0].set_ylabel('Number of Users')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Positive items per user\n",
    "    axes[0, 1].hist(val_user_stats['positive_items'], bins=50, edgecolor='black', alpha=0.7, color='teal')\n",
    "    axes[0, 1].set_title('Distribution of Positive Items per User (Validation)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Number of Positive Items')\n",
    "    axes[0, 1].set_ylabel('Number of Users')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Positive ratio distribution\n",
    "    axes[1, 0].hist(val_user_stats['positive_ratio'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[1, 0].set_title('Distribution of Positive Ratio per User (Validation)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Positive Ratio (Positive / Total Candidates)')\n",
    "    axes[1, 0].set_ylabel('Number of Users')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Box plot: Positive items distribution\n",
    "    axes[1, 1].boxplot([val_user_stats['positive_items'].values], vert=True, patch_artist=True,\n",
    "                        boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "    axes[1, 1].set_title('Distribution of Positive Items per User (Box Plot)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Number of Positive Items')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1, 1].set_xticklabels(['Validation Users'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '9_val_user_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved: 9_val_user_analysis.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Train vs Validation Comparison\n",
    "    print(\"\\n Creating train vs validation comparison...\")\n",
    "    try:\n",
    "        train_data = pd.read_parquet(EDA_CONFIG['MODEL_PATH'] / 'train_data.parquet')\n",
    "        train_user_stats = train_data.groupby('customer_id').agg({\n",
    "            'article_id': 'nunique',\n",
    "            'label': 'sum'\n",
    "        }).reset_index()\n",
    "        train_user_stats.columns = ['customer_id', 'candidate_items', 'positive_items']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Candidate items comparison\n",
    "        axes[0, 0].hist([train_user_stats['candidate_items'], val_user_stats['candidate_items']],\n",
    "                        bins=30, alpha=0.7, label=['Train', 'Validation'], edgecolor='black')\n",
    "        axes[0, 0].set_title('Candidate Items per User: Train vs Validation', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Number of Candidate Items')\n",
    "        axes[0, 0].set_ylabel('Number of Users')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Positive items comparison\n",
    "        axes[0, 1].hist([train_user_stats['positive_items'], val_user_stats['positive_items']],\n",
    "                        bins=30, alpha=0.7, label=['Train', 'Validation'], edgecolor='black')\n",
    "        axes[0, 1].set_title('Positive Items per User: Train vs Validation', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Number of Positive Items')\n",
    "        axes[0, 1].set_ylabel('Number of Users')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Summary statistics comparison\n",
    "        comparison_stats = pd.DataFrame({\n",
    "            'Train': [\n",
    "                train_data['label'].mean(),\n",
    "                train_user_stats['candidate_items'].mean(),\n",
    "                train_user_stats['positive_items'].mean(),\n",
    "                len(train_data)\n",
    "            ],\n",
    "            'Validation': [\n",
    "                val_data['label'].mean(),\n",
    "                val_user_stats['candidate_items'].mean(),\n",
    "                val_user_stats['positive_items'].mean(),\n",
    "                len(val_data)\n",
    "            ]\n",
    "        }, index=['Positive Ratio', 'Avg Candidates/User', 'Avg Positives/User', 'Total Samples'])\n",
    "        \n",
    "        comparison_stats.plot(kind='bar', ax=axes[1, 0], alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_title('Summary Statistics: Train vs Validation', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Value')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Sample size comparison\n",
    "        size_comparison = pd.DataFrame({\n",
    "            'Dataset': ['Train', 'Validation'],\n",
    "            'Samples': [len(train_data), len(val_data)],\n",
    "            'Users': [train_data['customer_id'].nunique(), val_data['customer_id'].nunique()],\n",
    "            'Items': [train_data['article_id'].nunique(), val_data['article_id'].nunique()]\n",
    "        })\n",
    "        \n",
    "        x = np.arange(len(size_comparison))\n",
    "        width = 0.25\n",
    "        axes[1, 1].bar(x - width, size_comparison['Samples']/1000, width, label='Samples (K)', alpha=0.7)\n",
    "        axes[1, 1].bar(x, size_comparison['Users']/1000, width, label='Users (K)', alpha=0.7)\n",
    "        axes[1, 1].bar(x + width, size_comparison['Items']/1000, width, label='Items (K)', alpha=0.7)\n",
    "        axes[1, 1].set_title('Dataset Size Comparison', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Count (Thousands)')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(size_comparison['Dataset'])\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EDA_CONFIG['OUTPUT_DIR'] / '10_train_val_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: 10_train_val_comparison.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\" Could not create comparison: {e}\")\n",
    "    \n",
    "    print(\"\\n Validation Data EDA Complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error loading validation data: {e}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EDA SUMMARY: ALL GENERATED PLOTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA SUMMARY: ALL GENERATED PLOTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import os\n",
    "\n",
    "# List all generated plots\n",
    "plot_files = sorted([f for f in os.listdir(EDA_CONFIG['OUTPUT_DIR']) if f.endswith('.png')])\n",
    "\n",
    "print(f\"\\n Total plots generated: {len(plot_files)}\")\n",
    "print(f\"\\n Output directory: {EDA_CONFIG['OUTPUT_DIR']}\")\n",
    "print(\"\\n Generated Plots:\")\n",
    "\n",
    "for i, plot_file in enumerate(plot_files, 1):\n",
    "    file_path = EDA_CONFIG['OUTPUT_DIR'] / plot_file\n",
    "    file_size = file_path.stat().st_size / 1024  # KB\n",
    "    print(f\"  {i:2d}. {plot_file:40s} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" EDA COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n All plots are saved in high resolution (300 DPI) and ready for presentation!\")\n",
    "print(f\"   Location: {EDA_CONFIG['OUTPUT_DIR']}\")\n",
    "print(\"\\n Plot Categories:\")\n",
    "print(\"   â€¢ Raw Dataset: Temporal patterns, categories, user behavior\")\n",
    "print(\"   â€¢ Training Data: Label distribution, user activity, item popularity, features\")\n",
    "print(\"   â€¢ Validation Data: User analysis, train/val comparison\")\n",
    "print(\"\\n All plots use presentation-quality styling with:\")\n",
    "print(\"   â€¢ Professional color schemes\")\n",
    "print(\"   â€¢ Clear labels and titles\")\n",
    "print(\"   â€¢ Grid lines for readability\")\n",
    "print(\"   â€¢ High resolution (300 DPI) for printing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2b57935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Baseline models configuration loaded\n",
      "  Model path: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BASELINE MODELS: IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reuse MAP@12 evaluation function\n",
    "def calculate_map_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Mean Average Precision at K (MAP@K)\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    aps = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        hits = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        for i, pred_item in enumerate(pred_items):\n",
    "            if pred_item in true_items:\n",
    "                hits += 1\n",
    "                precision_sum += hits / (i + 1)\n",
    "        \n",
    "        if hits > 0:\n",
    "            # Denominator must be the number of relevant items we evaluate at K,\n",
    "            # i.e. min(#true_items, K). This keeps AP in [0, 1].\n",
    "            denom = min(len(true_items), k)\n",
    "            ap = precision_sum / denom\n",
    "            aps.append(ap)\n",
    "    \n",
    "    return np.mean(aps) if len(aps) > 0 else 0.0\n",
    "\n",
    "def evaluate_map_at_12(df, predictions):\n",
    "    \"\"\"Evaluate MAP@12 on validation set\"\"\"\n",
    "    grouped = df.groupby('customer_id')\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for customer_id, group in grouped:\n",
    "        true_items = set(group[group['label'] == 1]['article_id'].values)\n",
    "        y_true.append(true_items)\n",
    "        \n",
    "        customer_df = group.copy()\n",
    "        customer_df['pred_score'] = predictions[:len(customer_df)]\n",
    "        customer_df = customer_df.sort_values('pred_score', ascending=False)\n",
    "        pred_items = customer_df['article_id'].values.tolist()\n",
    "        y_pred.append(pred_items)\n",
    "        \n",
    "        predictions = predictions[len(customer_df):]\n",
    "    \n",
    "    return calculate_map_at_k(y_true, y_pred, k=12)\n",
    "\n",
    "# Configuration\n",
    "BASELINE_CONFIG = {\n",
    "    'DATA_PATH': Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2'),\n",
    "    'MODEL_PATH': Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models'),\n",
    "    'RANDOM_STATE': 42\n",
    "}\n",
    "\n",
    "print(\"âœ“ Baseline models configuration loaded\")\n",
    "print(f\"  Model path: {BASELINE_CONFIG['MODEL_PATH']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74151952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATA FOR BASELINE MODELS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Loading training transactions...\n",
      "âœ“ Loaded 412,156 training transactions\n",
      "\n",
      "ðŸ“Š Loading validation data...\n",
      "âœ“ Loaded 121,164 validation samples\n",
      "  Unique users: 7,132\n",
      "  Unique items: 13,925\n",
      "\n",
      "ðŸ“ˆ Data Statistics:\n",
      "  Training users: 47,543\n",
      "  Training items: 15,932\n",
      "  Validation users: 7,132\n",
      "  Validation items: 13,925\n",
      "\n",
      "âœ“ Created mappings:\n",
      "  User mappings: 47,543\n",
      "  Item mappings: 15,932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA FOR BASELINE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA FOR BASELINE MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load training transactions for building user-item matrix\n",
    "print(\"\\nðŸ“Š Loading training transactions...\")\n",
    "train_transactions = pd.read_parquet(BASELINE_CONFIG['DATA_PATH'] / 'train_transactions.parquet')\n",
    "print(f\"âœ“ Loaded {len(train_transactions):,} training transactions\")\n",
    "\n",
    "# Load validation data for evaluation\n",
    "print(\"\\nðŸ“Š Loading validation data...\")\n",
    "val_data = pd.read_parquet(BASELINE_CONFIG['MODEL_PATH'] / 'val_data.parquet')\n",
    "print(f\"âœ“ Loaded {len(val_data):,} validation samples\")\n",
    "print(f\"  Unique users: {val_data['customer_id'].nunique():,}\")\n",
    "print(f\"  Unique items: {val_data['article_id'].nunique():,}\")\n",
    "\n",
    "# Get unique users and items\n",
    "all_users = sorted(train_transactions['customer_id'].unique().tolist())\n",
    "all_items = sorted(train_transactions['article_id'].unique().tolist())\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Data Statistics:\")\n",
    "print(f\"  Training users: {len(all_users):,}\")\n",
    "print(f\"  Training items: {len(all_items):,}\")\n",
    "print(f\"  Validation users: {val_data['customer_id'].nunique():,}\")\n",
    "print(f\"  Validation items: {val_data['article_id'].nunique():,}\")\n",
    "\n",
    "# Create user and item mappings\n",
    "user_to_idx = {user: idx for idx, user in enumerate(all_users)}\n",
    "item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "\n",
    "print(f\"\\nâœ“ Created mappings:\")\n",
    "print(f\"  User mappings: {len(user_to_idx):,}\")\n",
    "print(f\"  Item mappings: {len(item_to_idx):,}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "524263f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BASELINE 1: POPULARITY-BASED RECOMMENDATION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Calculating item popularity...\n",
      "âœ“ Calculated popularity for 15,932 items\n",
      "  Most popular item: 706016001 (585 purchases)\n",
      "\n",
      "ðŸ“Š Generating popularity-based predictions...\n",
      "\n",
      "âœ“ Popularity-Based Baseline Results:\n",
      "  MAP@12: 0.418288\n",
      "Saved predictions to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/baseline_popularity_predictions.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BASELINE 1: POPULARITY-BASED RECOMMENDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE 1: POPULARITY-BASED RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate item popularity from training data\n",
    "print(\"\\nðŸ“Š Calculating item popularity...\")\n",
    "item_popularity = train_transactions.groupby('article_id').size().reset_index(name='popularity')\n",
    "item_popularity = item_popularity.sort_values('popularity', ascending=False)\n",
    "\n",
    "print(f\"âœ“ Calculated popularity for {len(item_popularity):,} items\")\n",
    "print(f\"  Most popular item: {item_popularity.iloc[0]['article_id']} ({item_popularity.iloc[0]['popularity']} purchases)\")\n",
    "\n",
    "# Create popularity scores for validation candidates\n",
    "print(\"\\nðŸ“Š Generating popularity-based predictions...\")\n",
    "val_data_pop = val_data.copy()\n",
    "val_data_pop = val_data_pop.merge(\n",
    "    item_popularity[['article_id', 'popularity']],\n",
    "    on='article_id',\n",
    "    how='left'\n",
    ")\n",
    "val_data_pop['pred_score'] = val_data_pop['popularity'].fillna(0)\n",
    "\n",
    "# Evaluate\n",
    "map12_popularity = evaluate_map_at_12(val_data_pop, val_data_pop['pred_score'].values)\n",
    "\n",
    "print(f\"\\nâœ“ Popularity-Based Baseline Results:\")\n",
    "print(f\"  MAP@12: {map12_popularity:.6f}\")\n",
    "\n",
    "# Save predictions\n",
    "pred_path_pop = BASELINE_CONFIG['MODEL_PATH'] / 'baseline_popularity_predictions.parquet'\n",
    "val_data_pop[['customer_id', 'article_id', 'label', 'pred_score']].to_parquet(pred_path_pop, index=False)\n",
    "print(f\"Saved predictions to {pred_path_pop}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE 2: USER-BASED COLLABORATIVE FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE 2: USER-BASED COLLABORATIVE FILTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build user-item interaction matrix\n",
    "print(\"\\n Building user-item interaction matrix...\")\n",
    "train_transactions['user_idx'] = train_transactions['customer_id'].map(user_to_idx)\n",
    "train_transactions['item_idx'] = train_transactions['article_id'].map(item_to_idx)\n",
    "\n",
    "# Create sparse matrix (binary: 1 if user purchased item, 0 otherwise)\n",
    "rows = train_transactions['user_idx'].values\n",
    "cols = train_transactions['item_idx'].values\n",
    "data = np.ones(len(train_transactions), dtype=np.float32)\n",
    "\n",
    "user_item_matrix = csr_matrix((data, (rows, cols)), shape=(len(all_users), len(all_items)))\n",
    "print(f\"âœ“ Created user-item matrix: {user_item_matrix.shape}\")\n",
    "print(f\"  Sparsity: {(1 - user_item_matrix.nnz / (len(all_users) * len(all_items))) * 100:.2f}%\")\n",
    "\n",
    "# Compute user-user similarity matrix (cosine similarity)\n",
    "print(\"\\n Computing user-user similarity...\")\n",
    "print(\"  This may take a few minutes...\")\n",
    "user_similarity = cosine_similarity(user_item_matrix, dense_output=False)\n",
    "print(f\"Computed user similarity matrix: {user_similarity.shape}\")\n",
    "\n",
    "# Generate predictions for validation users\n",
    "print(\"\\n Generating user-based CF predictions...\")\n",
    "val_users = val_data['customer_id'].unique()\n",
    "val_predictions = []\n",
    "\n",
    "for user in tqdm(val_users, desc=\"Predicting for users\"):\n",
    "    if user not in user_to_idx:\n",
    "        # Cold-start user: use popularity\n",
    "        user_predictions = np.zeros(len(val_data[val_data['customer_id'] == user]))\n",
    "    else:\n",
    "        user_idx = user_to_idx[user]\n",
    "        \n",
    "        # Get similar users (top 50)\n",
    "        user_sim = user_similarity[user_idx].toarray().flatten()\n",
    "        similar_users_idx = np.argsort(user_sim)[::-1][1:51]  # Exclude self, top 50\n",
    "        \n",
    "        # Get items purchased by similar users\n",
    "        similar_users_items = user_item_matrix[similar_users_idx].sum(axis=0).A1\n",
    "        \n",
    "        # Get user's candidate items\n",
    "        user_candidates = val_data[val_data['customer_id'] == user]['article_id'].values\n",
    "        user_candidate_idx = [item_to_idx.get(item, -1) for item in user_candidates]\n",
    "        \n",
    "        # Score candidates based on similar users' purchases\n",
    "        user_scores = []\n",
    "        for item_idx in user_candidate_idx:\n",
    "            if item_idx >= 0:\n",
    "                score = similar_users_items[item_idx]\n",
    "            else:\n",
    "                score = 0\n",
    "            user_scores.append(score)\n",
    "        \n",
    "        user_predictions = np.array(user_scores)\n",
    "    \n",
    "    val_predictions.extend(user_predictions)\n",
    "\n",
    "val_data_ubcf = val_data.copy()\n",
    "val_data_ubcf['pred_score'] = val_predictions\n",
    "\n",
    "# Evaluate\n",
    "map12_ubcf = evaluate_map_at_12(val_data_ubcf, val_data_ubcf['pred_score'].values)\n",
    "\n",
    "print(f\"\\n User-Based CF Results:\")\n",
    "print(f\"  MAP@12: {map12_ubcf:.6f}\")\n",
    "\n",
    "# Save predictions\n",
    "pred_path_ubcf = BASELINE_CONFIG['MODEL_PATH'] / 'baseline_ubcf_predictions.parquet'\n",
    "val_data_ubcf[['customer_id', 'article_id', 'label', 'pred_score']].to_parquet(pred_path_ubcf, index=False)\n",
    "print(f\" Saved predictions to {pred_path_ubcf}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29428747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE 3: ITEM-BASED COLLABORATIVE FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE 3: ITEM-BASED COLLABORATIVE FILTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute item-item similarity matrix (cosine similarity on transposed matrix)\n",
    "print(\"\\n Computing item-item similarity...\")\n",
    "print(\"  This may take a few minutes...\")\n",
    "item_item_matrix = user_item_matrix.T  # Transpose: items x users\n",
    "item_similarity = cosine_similarity(item_item_matrix, dense_output=False)\n",
    "print(f\"âœ“ Computed item similarity matrix: {item_similarity.shape}\")\n",
    "\n",
    "# Generate predictions for validation users\n",
    "print(\"\\n Generating item-based CF predictions...\")\n",
    "val_users = val_data['customer_id'].unique()\n",
    "val_predictions = []\n",
    "\n",
    "for user in tqdm(val_users, desc=\"Predicting for users\"):\n",
    "    if user not in user_to_idx:\n",
    "        # Cold-start user: use popularity\n",
    "        user_predictions = np.zeros(len(val_data[val_data['customer_id'] == user]))\n",
    "    else:\n",
    "        user_idx = user_to_idx[user]\n",
    "        \n",
    "        # Get user's purchased items\n",
    "        user_items = user_item_matrix[user_idx].nonzero()[1]  # Items user purchased\n",
    "        \n",
    "        if len(user_items) == 0:\n",
    "            # User with no purchases: use popularity\n",
    "            user_predictions = np.zeros(len(val_data[val_data['customer_id'] == user]))\n",
    "        else:\n",
    "            # Get user's candidate items\n",
    "            user_candidates = val_data[val_data['customer_id'] == user]['article_id'].values\n",
    "            user_candidate_idx = [item_to_idx.get(item, -1) for item in user_candidates]\n",
    "            \n",
    "            # Score candidates based on similarity to user's purchased items\n",
    "            user_scores = []\n",
    "            for candidate_idx in user_candidate_idx:\n",
    "                if candidate_idx >= 0:\n",
    "                    # Average similarity to all items user purchased\n",
    "                    similarities = item_similarity[candidate_idx, user_items].toarray().flatten()\n",
    "                    score = similarities.mean() if len(similarities) > 0 else 0\n",
    "                else:\n",
    "                    score = 0\n",
    "                user_scores.append(score)\n",
    "            \n",
    "            user_predictions = np.array(user_scores)\n",
    "    \n",
    "    val_predictions.extend(user_predictions)\n",
    "\n",
    "val_data_ibcf = val_data.copy()\n",
    "val_data_ibcf['pred_score'] = val_predictions\n",
    "\n",
    "# Evaluate\n",
    "map12_ibcf = evaluate_map_at_12(val_data_ibcf, val_data_ibcf['pred_score'].values)\n",
    "\n",
    "print(f\"\\n Item-Based CF Results:\")\n",
    "print(f\"  MAP@12: {map12_ibcf:.6f}\")\n",
    "\n",
    "# Save predictions\n",
    "pred_path_ibcf = BASELINE_CONFIG['MODEL_PATH'] / 'baseline_ibcf_predictions.parquet'\n",
    "val_data_ibcf[['customer_id', 'article_id', 'label', 'pred_score']].to_parquet(pred_path_ibcf, index=False)\n",
    "print(f\" Saved predictions to {pred_path_ibcf}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b899fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE 4: MATRIX FACTORIZATION (SVD)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE 4: MATRIX FACTORIZATION (SVD)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Apply SVD for dimensionality reduction\n",
    "print(\"\\n Applying Truncated SVD...\")\n",
    "n_components = 50  # Number of latent factors\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=BASELINE_CONFIG['RANDOM_STATE'])\n",
    "user_factors = svd.fit_transform(user_item_matrix)\n",
    "item_factors = svd.components_.T  # Transpose to get items x factors\n",
    "\n",
    "print(f\"âœ“ SVD decomposition complete\")\n",
    "print(f\"  User factors shape: {user_factors.shape}\")\n",
    "print(f\"  Item factors shape: {item_factors.shape}\")\n",
    "print(f\"  Explained variance: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Generate predictions for validation users\n",
    "print(\"\\n Generating SVD-based predictions...\")\n",
    "val_users = val_data['customer_id'].unique()\n",
    "val_predictions = []\n",
    "\n",
    "for user in tqdm(val_users, desc=\"Predicting for users\"):\n",
    "    if user not in user_to_idx:\n",
    "        # Cold-start user: use popularity\n",
    "        user_predictions = np.zeros(len(val_data[val_data['customer_id'] == user]))\n",
    "    else:\n",
    "        user_idx = user_to_idx[user]\n",
    "        user_vector = user_factors[user_idx]  # User's latent factors\n",
    "        \n",
    "        # Get user's candidate items\n",
    "        user_candidates = val_data[val_data['customer_id'] == user]['article_id'].values\n",
    "        user_candidate_idx = [item_to_idx.get(item, -1) for item in user_candidates]\n",
    "        \n",
    "        # Score candidates using dot product\n",
    "        user_scores = []\n",
    "        for candidate_idx in user_candidate_idx:\n",
    "            if candidate_idx >= 0:\n",
    "                item_vector = item_factors[candidate_idx]  # Item's latent factors\n",
    "                score = np.dot(user_vector, item_vector)  # Dot product = predicted rating\n",
    "            else:\n",
    "                score = 0\n",
    "            user_scores.append(score)\n",
    "        \n",
    "        user_predictions = np.array(user_scores)\n",
    "    \n",
    "    val_predictions.extend(user_predictions)\n",
    "\n",
    "val_data_svd = val_data.copy()\n",
    "val_data_svd['pred_score'] = val_predictions\n",
    "\n",
    "# Evaluate\n",
    "map12_svd = evaluate_map_at_12(val_data_svd, val_data_svd['pred_score'].values)\n",
    "\n",
    "print(f\"\\n SVD Results:\")\n",
    "print(f\"  MAP@12: {map12_svd:.6f}\")\n",
    "\n",
    "# Save predictions\n",
    "pred_path_svd = BASELINE_CONFIG['MODEL_PATH'] / 'baseline_svd_predictions.parquet'\n",
    "val_data_svd[['customer_id', 'article_id', 'label', 'pred_score']].to_parquet(pred_path_svd, index=False)\n",
    "print(f\" Saved predictions to {pred_path_svd}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54105973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CF EVALUATION ON TEST DATA\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CF EVALUATION ON TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# We expect test_data.parquet to be created by the Neural Tower / model training pipeline\n",
    "# at the same MODEL_PATH used for val_data\n",
    "\n",
    "test_data_path = EnsembleConfig.MODEL_PATH / 'test_data.parquet'\n",
    "if not test_data_path.exists():\n",
    "    print(f\"\\nâš ï¸  test_data.parquet not found at {test_data_path}\")\n",
    "    print(\"   Skipping CF evaluation on test set. Run the data-splitting step to create it.\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Loading test data from {test_data_path}...\")\n",
    "    test_data = pd.read_parquet(test_data_path)\n",
    "    print(f\"âœ“ Loaded {len(test_data):,} test samples\")\n",
    "\n",
    "    # Generate CF predictions on test_data (same procedure as for val_data)\n",
    "    print(\"\\nðŸ“Š Generating CF predictions on test data...\")\n",
    "    cf_test_predictions = {}\n",
    "    test_grouped = test_data.groupby('customer_id')\n",
    "\n",
    "    for model_name in ['svd', 'svdpp', 'als', 'nmf', 'user_cf', 'item_cf']:\n",
    "        if model_name not in cf_models or cf_models[model_name] is None:\n",
    "            print(f\"\\nâš ï¸  Skipping {model_name} (model not available)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  Generating {model_name} predictions on test set...\")\n",
    "        predictions = []\n",
    "        for user_id, group in tqdm(test_grouped, desc=f'{model_name} (test)', leave=False):\n",
    "            user_predictions = []\n",
    "            for _, row in group.iterrows():\n",
    "                item_id = row['article_id']\n",
    "\n",
    "                if model_name == 'svd':\n",
    "                    pred = predict_svd(user_id, item_id, cf_models['svd'])\n",
    "                elif model_name == 'svdpp':\n",
    "                    if HAS_SURPRISE and cf_models['svdpp'] is not None:\n",
    "                        try:\n",
    "                            pred = cf_models['svdpp'].predict(str(user_id), str(item_id)).est\n",
    "                        except Exception:\n",
    "                            pred = 0.0\n",
    "                    else:\n",
    "                        # Fallback: use SVD-style prediction\n",
    "                        pred = predict_svd(user_id, item_id, cf_models['svd'])\n",
    "                elif model_name == 'als':\n",
    "                    pred = predict_als(user_id, item_id, cf_models['als'])\n",
    "                elif model_name == 'nmf':\n",
    "                    pred = predict_nmf(user_id, item_id, cf_models['nmf'])\n",
    "                elif model_name == 'user_cf':\n",
    "                    pred = predict_user_cf(user_id, item_id, cf_models['user_cf'])\n",
    "                elif model_name == 'item_cf':\n",
    "                    pred = predict_item_cf(user_id, item_id, cf_models['item_cf'])\n",
    "                else:\n",
    "                    pred = 0.0\n",
    "\n",
    "                user_predictions.append(pred)\n",
    "\n",
    "            predictions.extend(user_predictions)\n",
    "\n",
    "        cf_test_predictions[model_name] = np.array(predictions)\n",
    "\n",
    "        # Evaluate MAP@12 on test_data\n",
    "        map12_score = evaluate_map_at_12(test_data, cf_test_predictions[model_name])\n",
    "        print(f\"    Test MAP@12: {map12_score:.6f}\")\n",
    "\n",
    "    print(\"\\nâœ… CF test evaluation complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c66d65",
   "metadata": {},
   "source": [
    "### Stage 1: Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d746cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4100b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths (adjust based on your Kaggle dataset location)\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/h-and-m-personalized-fashion-recommendations')\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_2')\n",
    "    \n",
    "     # Temporal configuration\n",
    "    N_TRAIN_WEEKS = 11  # Number of weeks for training\n",
    "    N_VAL_WEEKS = 1     # Validation week\n",
    "    TOTAL_WEEKS = 24    # Total weeks to consider (16-24 range)\n",
    "    \n",
    "    # User sampling configuration\n",
    "    TARGET_USERS = 50000  # Target number of users to sample\n",
    "    MIN_USER_PURCHASES = 1  # Minimum purchases across all weeks for a user\n",
    "    \n",
    "    # Cold start configuration\n",
    "    INCLUDE_COLD_START = True  # Include users with limited history\n",
    "    COLD_START_RATIO = 0.15  # 15% of sampled users will be cold start\n",
    "    COLD_START_MAX_PURCHASES = 1  # Users with <= this many purchases are \"cold start\"\n",
    "    \n",
    "    # Stratification configuration\n",
    "    STRATIFY_BY_ACTIVITY = True  # Stratify users by activity level\n",
    "    ACTIVITY_BINS = [0, 5, 10, 20, 50, np.inf]  # Purchase count bins\n",
    "    ACTIVITY_LABELS = ['low', 'medium', 'high', 'very_high', 'extreme']\n",
    "    \n",
    "    # Item filtering\n",
    "    MIN_ITEM_PURCHASES = 5  # Minimum purchases for an item to be included\n",
    "    \n",
    "    # Memory optimization\n",
    "    CHUNK_SIZE = 500_000  # Process transactions in chunks\n",
    "    \n",
    "    # Random seed\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()\n",
    "config.OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "np.random.seed(config.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50955d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduce memory usage of a dataframe by optimizing dtypes\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Skip datetime and object columns\n",
    "        if col_type == object or pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            continue\n",
    "            \n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "        \n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Memory usage decreased from {start_mem:.2f} MB to {end_mem:.2f} MB '\n",
    "              f'({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"Pretty print section headers\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad57ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "Loading transactions...\n",
      "âœ“ Loaded 31,788,324 transactions\n",
      "  Date range: 2018-09-20 00:00:00 to 2020-09-22 00:00:00\n",
      "  Unique customers: 1,362,281\n",
      "  Unique articles: 104,547\n",
      "\n",
      "Loading customers...\n",
      "âœ“ Loaded 1,371,980 customers\n",
      "\n",
      "Loading articles...\n",
      "âœ“ Loaded 105,542 articles\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: LOAD AND EXPLORE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 1: LOADING DATA\")\n",
    "\n",
    "# Load transactions\n",
    "print(\"Loading transactions...\")\n",
    "transactions = pd.read_csv(\n",
    "    config.DATA_PATH / 'transactions_train.csv',\n",
    "    dtype={\n",
    "        'article_id': 'int32',\n",
    "        'price': 'float32',\n",
    "        'sales_channel_id': 'int8'\n",
    "    },\n",
    "    parse_dates=['t_dat']\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded {len(transactions):,} transactions\")\n",
    "print(f\"  Date range: {transactions['t_dat'].min()} to {transactions['t_dat'].max()}\")\n",
    "print(f\"  Unique customers: {transactions['customer_id'].nunique():,}\")\n",
    "print(f\"  Unique articles: {transactions['article_id'].nunique():,}\")\n",
    "\n",
    "# Load customers\n",
    "print(\"\\nLoading customers...\")\n",
    "customers = pd.read_csv(\n",
    "    config.DATA_PATH / 'customers.csv',\n",
    "    dtype={\n",
    "        'FN': 'float32',\n",
    "        'Active': 'float32',\n",
    "        'age': 'float32'\n",
    "    }\n",
    ")\n",
    "customers = reduce_mem_usage(customers, verbose=False)\n",
    "print(f\"âœ“ Loaded {len(customers):,} customers\")\n",
    "\n",
    "# Load articles\n",
    "print(\"\\nLoading articles...\")\n",
    "articles = pd.read_csv(\n",
    "    config.DATA_PATH / 'articles.csv',\n",
    "    dtype={'article_id': 'int32'}\n",
    ")\n",
    "articles = reduce_mem_usage(articles, verbose=False)\n",
    "print(f\"âœ“ Loaded {len(articles):,} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "006b29ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 2: SELECTING TEMPORAL WINDOW\n",
      "================================================================================\n",
      "Last transaction date: 2020-09-22 00:00:00\n",
      "\n",
      "Using 24 weeks of data for sampling\n",
      "Window: 2020-04-07 to 2020-09-22\n",
      "\n",
      "Filtering transactions from 2020-04-07 onwards...\n",
      "âœ“ Retained 7,561,154 transactions (7.56M)\n",
      "  Week range: 0 to 24\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: TEMPORAL WINDOW SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 2: SELECTING TEMPORAL WINDOW\")\n",
    "\n",
    "# Get the last date in transactions\n",
    "max_date = transactions['t_dat'].max()\n",
    "print(f\"Last transaction date: {max_date}\")\n",
    "\n",
    "# Calculate cutoff dates for the full window\n",
    "window_start = max_date - timedelta(weeks=config.TOTAL_WEEKS)\n",
    "print(f\"\\nUsing {config.TOTAL_WEEKS} weeks of data for sampling\")\n",
    "print(f\"Window: {window_start.date()} to {max_date.date()}\")\n",
    "\n",
    "# Filter transactions to our window\n",
    "print(f\"\\nFiltering transactions from {window_start.date()} onwards...\")\n",
    "transactions = transactions[transactions['t_dat'] >= window_start].copy()\n",
    "print(f\"âœ“ Retained {len(transactions):,} transactions ({len(transactions)/1e6:.2f}M)\")\n",
    "\n",
    "# Add week number (relative to window start)\n",
    "transactions['week'] = ((transactions['t_dat'] - window_start).dt.days // 7).astype(np.int8)\n",
    "print(f\"  Week range: {transactions['week'].min()} to {transactions['week'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7e0e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 3: USER-BASED STRATIFIED SAMPLING\n",
      "================================================================================\n",
      "Calculating user activity metrics...\n",
      "Total users in window: 719,806\n",
      "  Avg purchases per user: 10.50\n",
      "  Avg active weeks per user: 2.61\n",
      "\n",
      "User segments:\n",
      "  Cold start users (â‰¤1 purchases): 72,280\n",
      "  Regular users (â‰¥1 purchases): 719,806\n",
      "\n",
      "Sampling targets:\n",
      "  Cold start: 7,500 users (15.0%)\n",
      "  Regular: 42,500 users (85.0%)\n",
      "\n",
      "âœ“ Sampled 7,500 cold start users\n",
      "\n",
      "Regular user activity distribution:\n",
      "  low: 331,386 users (46.0%)\n",
      "  medium: 161,171 users (22.4%)\n",
      "  high: 133,525 users (18.6%)\n",
      "  very_high: 79,738 users (11.1%)\n",
      "  extreme: 13,986 users (1.9%)\n",
      "\n",
      "Performing stratified sampling to get 42,500 regular users...\n",
      "\n",
      "Samples per activity level:\n",
      "  low: 19,566 users\n",
      "  medium: 9,516 users\n",
      "  high: 7,884 users\n",
      "  very_high: 4,708 users\n",
      "  extreme: 826 users\n",
      "\n",
      "âœ“ Total selected users: 49,576\n",
      "  - Cold start: 7,500 (15.1%)\n",
      "  - Regular: 42,500 (85.7%)\n",
      "\n",
      "Sampled users statistics:\n",
      "  Avg purchases: 9.14\n",
      "  Median purchases: 5.00\n",
      "  Min purchases: 1\n",
      "  Max purchases: 342\n",
      "  Avg active weeks: 2.38\n",
      "  Purchases std: 13.05\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: USER-BASED STRATIFIED SAMPLING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 3: USER-BASED STRATIFIED SAMPLING\")\n",
    "\n",
    "# Calculate user activity across all weeks\n",
    "print(\"Calculating user activity metrics...\")\n",
    "user_activity = transactions.groupby('customer_id').agg({\n",
    "    'article_id': 'count',  # Total purchases\n",
    "    'week': ['min', 'max', 'nunique']  # Week span and diversity\n",
    "}).reset_index()\n",
    "\n",
    "user_activity.columns = ['customer_id', 'total_purchases', 'first_week', 'last_week', 'active_weeks']\n",
    "user_activity['week_span'] = user_activity['last_week'] - user_activity['first_week'] + 1\n",
    "\n",
    "print(f\"Total users in window: {len(user_activity):,}\")\n",
    "print(f\"  Avg purchases per user: {user_activity['total_purchases'].mean():.2f}\")\n",
    "print(f\"  Avg active weeks per user: {user_activity['active_weeks'].mean():.2f}\")\n",
    "\n",
    "# Separate cold start and regular users\n",
    "if config.INCLUDE_COLD_START:\n",
    "    cold_start_users = user_activity[\n",
    "        user_activity['total_purchases'] <= config.COLD_START_MAX_PURCHASES\n",
    "    ].copy()\n",
    "    regular_users = user_activity[\n",
    "        user_activity['total_purchases'] >= config.MIN_USER_PURCHASES\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nUser segments:\")\n",
    "    print(f\"  Cold start users (â‰¤{config.COLD_START_MAX_PURCHASES} purchases): {len(cold_start_users):,}\")\n",
    "    print(f\"  Regular users (â‰¥{config.MIN_USER_PURCHASES} purchases): {len(regular_users):,}\")\n",
    "    \n",
    "    # Calculate target counts\n",
    "    n_cold_start_target = int(config.TARGET_USERS * config.COLD_START_RATIO)\n",
    "    n_regular_target = config.TARGET_USERS - n_cold_start_target\n",
    "    \n",
    "    print(f\"\\nSampling targets:\")\n",
    "    print(f\"  Cold start: {n_cold_start_target:,} users ({config.COLD_START_RATIO*100:.1f}%)\")\n",
    "    print(f\"  Regular: {n_regular_target:,} users ({(1-config.COLD_START_RATIO)*100:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    # Filter users with minimum activity\n",
    "    regular_users = user_activity[\n",
    "        user_activity['total_purchases'] >= config.MIN_USER_PURCHASES\n",
    "    ].copy()\n",
    "    cold_start_users = pd.DataFrame()\n",
    "    n_cold_start_target = 0\n",
    "    n_regular_target = config.TARGET_USERS\n",
    "    \n",
    "    print(f\"\\nUsers with >= {config.MIN_USER_PURCHASES} purchases: {len(regular_users):,}\")\n",
    "\n",
    "# Sample cold start users (if enabled)\n",
    "sampled_cold_start = []\n",
    "if config.INCLUDE_COLD_START and len(cold_start_users) > 0:\n",
    "    n_cold_sample = min(n_cold_start_target, len(cold_start_users))\n",
    "    sampled_cold_start = cold_start_users['customer_id'].sample(\n",
    "        n=n_cold_sample, \n",
    "        random_state=config.RANDOM_STATE\n",
    "    ).tolist()\n",
    "    print(f\"\\nâœ“ Sampled {len(sampled_cold_start):,} cold start users\")\n",
    "\n",
    "# Sample regular users with stratification\n",
    "if config.STRATIFY_BY_ACTIVITY and len(regular_users) > 0:\n",
    "    regular_users['activity_level'] = pd.cut(\n",
    "        regular_users['total_purchases'],\n",
    "        bins=config.ACTIVITY_BINS,\n",
    "        labels=config.ACTIVITY_LABELS\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRegular user activity distribution:\")\n",
    "    activity_dist = regular_users['activity_level'].value_counts().sort_index()\n",
    "    for level, count in activity_dist.items():\n",
    "        print(f\"  {level}: {count:,} users ({100*count/len(regular_users):.1f}%)\")\n",
    "    \n",
    "    # Stratified sampling for regular users\n",
    "    print(f\"\\nPerforming stratified sampling to get {n_regular_target:,} regular users...\")\n",
    "    \n",
    "    # Calculate samples per stratum (proportional)\n",
    "    samples_per_stratum = (activity_dist / activity_dist.sum() * n_regular_target).round().astype(int)\n",
    "    \n",
    "    # Adjust for rounding errors\n",
    "    diff = n_regular_target - samples_per_stratum.sum()\n",
    "    if diff != 0:\n",
    "        largest_stratum = samples_per_stratum.idxmax()\n",
    "        samples_per_stratum[largest_stratum] += diff\n",
    "    \n",
    "    print(\"\\nSamples per activity level:\")\n",
    "    for level, n_samples in samples_per_stratum.items():\n",
    "        print(f\"  {level}: {n_samples:,} users\")\n",
    "    \n",
    "    # Sample from each stratum\n",
    "    sampled_regular = []\n",
    "    for level in config.ACTIVITY_LABELS:\n",
    "        stratum_users = regular_users[regular_users['activity_level'] == level]['customer_id']\n",
    "        n_sample = min(samples_per_stratum[level], len(stratum_users))\n",
    "        if n_sample > 0:\n",
    "            sampled = stratum_users.sample(n=n_sample, random_state=config.RANDOM_STATE)\n",
    "            sampled_regular.extend(sampled.tolist())\n",
    "\n",
    "else:\n",
    "    # Simple random sampling for regular users\n",
    "    print(f\"\\nPerforming random sampling to get {n_regular_target:,} regular users...\")\n",
    "    n_sample = min(n_regular_target, len(regular_users))\n",
    "    sampled_regular = regular_users['customer_id'].sample(\n",
    "        n=n_sample, \n",
    "        random_state=config.RANDOM_STATE\n",
    "    ).tolist()\n",
    "\n",
    "# Combine both groups\n",
    "selected_users = set(sampled_cold_start + sampled_regular)\n",
    "\n",
    "print(f\"\\nâœ“ Total selected users: {len(selected_users):,}\")\n",
    "if config.INCLUDE_COLD_START:\n",
    "    print(f\"  - Cold start: {len(sampled_cold_start):,} ({100*len(sampled_cold_start)/len(selected_users):.1f}%)\")\n",
    "    print(f\"  - Regular: {len(sampled_regular):,} ({100*len(sampled_regular)/len(selected_users):.1f}%)\")\n",
    "\n",
    "# Verify sampling quality\n",
    "sampled_activity = user_activity[user_activity['customer_id'].isin(selected_users)]\n",
    "print(f\"\\nSampled users statistics:\")\n",
    "print(f\"  Avg purchases: {sampled_activity['total_purchases'].mean():.2f}\")\n",
    "print(f\"  Median purchases: {sampled_activity['total_purchases'].median():.2f}\")\n",
    "print(f\"  Min purchases: {sampled_activity['total_purchases'].min():.0f}\")\n",
    "print(f\"  Max purchases: {sampled_activity['total_purchases'].max():.0f}\")\n",
    "print(f\"  Avg active weeks: {sampled_activity['active_weeks'].mean():.2f}\")\n",
    "print(f\"  Purchases std: {sampled_activity['total_purchases'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d35452de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 4: FILTERING TRANSACTIONS TO SAMPLED USERS\n",
      "================================================================================\n",
      "âœ“ Retained 453,143 transactions\n",
      "  Reduction: 0.0% (based on sampled users)\n",
      "\n",
      "Temporal splits (from 24 week window):\n",
      "  Training:   2020-06-29 to 2020-09-14 (11 weeks)\n",
      "  Validation: 2020-09-15 to 2020-09-22 (1 week)\n",
      "\n",
      "Dataset split:\n",
      "  Training transactions: 436,663\n",
      "  Validation transactions: 16,480\n",
      "  Users in validation: 4,943 (10.0% of sampled)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: FILTER TRANSACTIONS TO SAMPLED USERS\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 4: FILTERING TRANSACTIONS TO SAMPLED USERS\")\n",
    "\n",
    "# Filter transactions\n",
    "transactions = transactions[transactions['customer_id'].isin(selected_users)].copy()\n",
    "print(f\"âœ“ Retained {len(transactions):,} transactions\")\n",
    "print(f\"  Reduction: {100 * (1 - len(transactions) / len(transactions)):.1f}% (based on sampled users)\")\n",
    "\n",
    "# Now create train/val split\n",
    "val_end_date = max_date\n",
    "val_start_date = val_end_date - timedelta(weeks=config.N_VAL_WEEKS)\n",
    "train_end_date = val_start_date - timedelta(days=1)\n",
    "train_start_date = train_end_date - timedelta(weeks=config.N_TRAIN_WEEKS)\n",
    "\n",
    "print(f\"\\nTemporal splits (from {config.TOTAL_WEEKS} week window):\")\n",
    "print(f\"  Training:   {train_start_date.date()} to {train_end_date.date()} ({config.N_TRAIN_WEEKS} weeks)\")\n",
    "print(f\"  Validation: {val_start_date.date()} to {val_end_date.date()} ({config.N_VAL_WEEKS} week)\")\n",
    "\n",
    "# Split transactions\n",
    "train_transactions = transactions[transactions['t_dat'] <= train_end_date].copy()\n",
    "val_transactions = transactions[transactions['t_dat'] > train_end_date].copy()\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training transactions: {len(train_transactions):,}\")\n",
    "print(f\"  Validation transactions: {len(val_transactions):,}\")\n",
    "\n",
    "# Check how many sampled users appear in validation\n",
    "val_users = set(val_transactions['customer_id'].unique())\n",
    "print(f\"  Users in validation: {len(val_users):,} ({100*len(val_users)/len(selected_users):.1f}% of sampled)\")\n",
    "\n",
    "del transactions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06bd869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 5: ITEM FILTERING\n",
      "================================================================================\n",
      "Unique items in training: 28,951\n",
      "Items with >= 5 purchases: 14,851\n",
      "Items in validation: 5,730\n",
      "\n",
      "Total selected items: 16,616\n",
      "\n",
      "After item filtering:\n",
      "  Training transactions: 412,156\n",
      "  Validation transactions: 16,480\n",
      "  Articles retained: 16,616\n",
      "  Customers retained: 49,576\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: ITEM FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 5: ITEM FILTERING\")\n",
    "\n",
    "# Count purchases per item in training window\n",
    "item_counts = train_transactions['article_id'].value_counts()\n",
    "print(f\"Unique items in training: {len(item_counts):,}\")\n",
    "\n",
    "# Keep items with minimum purchases\n",
    "valid_items = set(item_counts[item_counts >= config.MIN_ITEM_PURCHASES].index)\n",
    "print(f\"Items with >= {config.MIN_ITEM_PURCHASES} purchases: {len(valid_items):,}\")\n",
    "\n",
    "# Also include all items from validation (even if rare in training)\n",
    "val_items = set(val_transactions['article_id'].unique())\n",
    "print(f\"Items in validation: {len(val_items):,}\")\n",
    "\n",
    "# Combine\n",
    "selected_items = valid_items.union(val_items)\n",
    "print(f\"\\nTotal selected items: {len(selected_items):,}\")\n",
    "\n",
    "# Filter transactions\n",
    "train_transactions = train_transactions[train_transactions['article_id'].isin(selected_items)].copy()\n",
    "val_transactions = val_transactions[val_transactions['article_id'].isin(selected_items)].copy()\n",
    "\n",
    "print(f\"\\nAfter item filtering:\")\n",
    "print(f\"  Training transactions: {len(train_transactions):,}\")\n",
    "print(f\"  Validation transactions: {len(val_transactions):,}\")\n",
    "\n",
    "# Filter articles and customers tables\n",
    "articles = articles[articles['article_id'].isin(selected_items)].copy()\n",
    "customers = customers[customers['customer_id'].isin(selected_users)].copy()\n",
    "\n",
    "print(f\"  Articles retained: {len(articles):,}\")\n",
    "print(f\"  Customers retained: {len(customers):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae3ed3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 6: MEMORY OPTIMIZATION\n",
      "================================================================================\n",
      "Before optimization:\n",
      "  train_transactions: 57.78 MB\n",
      "  val_transactions: 2.31 MB\n",
      "  customers: 18.34 MB\n",
      "  articles: 17.70 MB\n",
      "Memory usage decreased from 13.36 MB to 13.36 MB (0.0% reduction)\n",
      "Memory usage decreased from 0.53 MB to 0.53 MB (0.0% reduction)\n",
      "\n",
      "After optimization:\n",
      "  train_transactions: 57.78 MB\n",
      "  val_transactions: 2.31 MB\n",
      "  customers: 12.94 MB\n",
      "  articles: 16.92 MB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: DATA TYPE OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 6: MEMORY OPTIMIZATION\")\n",
    "\n",
    "print(\"Before optimization:\")\n",
    "print(f\"  train_transactions: {train_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  val_transactions: {val_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  customers: {customers.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  articles: {articles.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Optimize transactions\n",
    "train_transactions = reduce_mem_usage(train_transactions)\n",
    "val_transactions = reduce_mem_usage(val_transactions)\n",
    "\n",
    "# Convert categorical columns\n",
    "for col in ['product_code', 'product_type_no', 'graphical_appearance_no', \n",
    "            'colour_group_code', 'perceived_colour_value_id', 'perceived_colour_master_id',\n",
    "            'department_no', 'index_code', 'index_group_no', 'section_no', 'garment_group_no']:\n",
    "    if col in articles.columns:\n",
    "        articles[col] = articles[col].astype('category')\n",
    "\n",
    "# Optimize customer categoricals\n",
    "for col in ['club_member_status', 'fashion_news_frequency', 'postal_code']:\n",
    "    if col in customers.columns:\n",
    "        customers[col] = customers[col].astype('category')\n",
    "\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(f\"  train_transactions: {train_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  val_transactions: {val_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  customers: {customers.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  articles: {articles.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88760620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 7: DATA VALIDATION & EDA\n",
      "================================================================================\n",
      "Data validation:\n",
      "  âœ“ No null customer_ids in train: True\n",
      "  âœ“ No null article_ids in train: True\n",
      "\n",
      "Weekly activity distribution (sampled users):\n",
      "  Week 0: 4,926 active users\n",
      "  Week 1: 3,240 active users\n",
      "  Week 2: 3,976 active users\n",
      "  Week 3: 3,284 active users\n",
      "  Week 4: 3,771 active users\n",
      "  Week 5: 4,157 active users\n",
      "  Week 6: 5,213 active users\n",
      "  Week 7: 5,419 active users\n",
      "  Week 8: 4,900 active users\n",
      "  Week 9: 4,432 active users\n",
      "  Week 10: 7,090 active users\n",
      "  Week 11: 7,371 active users\n",
      "  Week 12: 5,377 active users\n",
      "  Week 13: 4,692 active users\n",
      "  Week 14: 4,717 active users\n",
      "  Week 15: 4,910 active users\n",
      "  Week 16: 4,985 active users\n",
      "  Week 17: 4,756 active users\n",
      "  Week 18: 4,366 active users\n",
      "  Week 19: 4,289 active users\n",
      "  Week 20: 5,006 active users\n",
      "  Week 21: 4,634 active users\n",
      "  Week 22: 4,647 active users\n",
      "\n",
      "Purchase distribution in validation week:\n",
      "  Mean purchases per user: 3.33\n",
      "  Median purchases per user: 2\n",
      "  Users with 1 purchase: 1,689\n",
      "  Users with 2-5 purchases: 2,430\n",
      "  Users with 6+ purchases: 824\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: DATA VALIDATION & EDA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 7: DATA VALIDATION & EDA\")\n",
    "\n",
    "# Validation checks\n",
    "print(\"Data validation:\")\n",
    "print(f\"  âœ“ No null customer_ids in train: {train_transactions['customer_id'].isnull().sum() == 0}\")\n",
    "print(f\"  âœ“ No null article_ids in train: {train_transactions['article_id'].isnull().sum() == 0}\")\n",
    "\n",
    "# Weekly distribution\n",
    "print(\"\\nWeekly activity distribution (sampled users):\")\n",
    "weekly_users = train_transactions.groupby('week')['customer_id'].nunique()\n",
    "for week, n_users in weekly_users.items():\n",
    "    print(f\"  Week {week}: {n_users:,} active users\")\n",
    "\n",
    "# Purchase distribution\n",
    "print(\"\\nPurchase distribution in validation week:\")\n",
    "if len(val_transactions) > 0:\n",
    "    val_user_purchases = val_transactions.groupby('customer_id').size()\n",
    "    print(f\"  Mean purchases per user: {val_user_purchases.mean():.2f}\")\n",
    "    print(f\"  Median purchases per user: {val_user_purchases.median():.0f}\")\n",
    "    print(f\"  Users with 1 purchase: {(val_user_purchases == 1).sum():,}\")\n",
    "    print(f\"  Users with 2-5 purchases: {((val_user_purchases >= 2) & (val_user_purchases <= 5)).sum():,}\")\n",
    "    print(f\"  Users with 6+ purchases: {(val_user_purchases >= 6).sum():,}\")\n",
    "else:\n",
    "    print(\"  âš ï¸ No validation transactions for sampled users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34849df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 8: CREATING VALIDATION GROUND TRUTH\n",
      "================================================================================\n",
      "Validation ground truth:\n",
      "  Users: 4,943\n",
      "  Total purchases: 16,480\n",
      "  Avg purchases per user: 3.33\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: CREATE VALIDATION GROUND TRUTH\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 8: CREATING VALIDATION GROUND TRUTH\")\n",
    "\n",
    "# Create validation ground truth\n",
    "if len(val_transactions) > 0:\n",
    "    val_ground_truth = (\n",
    "        val_transactions\n",
    "        .groupby('customer_id')['article_id']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "        .rename(columns={'article_id': 'purchased_articles'})\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation ground truth:\")\n",
    "    print(f\"  Users: {len(val_ground_truth):,}\")\n",
    "    print(f\"  Total purchases: {val_ground_truth['purchased_articles'].apply(len).sum():,}\")\n",
    "    print(f\"  Avg purchases per user: {val_ground_truth['purchased_articles'].apply(len).mean():.2f}\")\n",
    "else:\n",
    "    val_ground_truth = pd.DataFrame(columns=['customer_id', 'purchased_articles'])\n",
    "    print(\"âš ï¸ Empty validation ground truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "65aee445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 9: SAVING PROCESSED DATA\n",
      "================================================================================\n",
      "Saving files...\n",
      "  âœ“ train_transactions.parquet (412,156 rows)\n",
      "  âœ“ val_transactions.parquet (16,480 rows)\n",
      "  âœ“ customers.parquet (49,576 rows)\n",
      "  âœ“ articles.parquet (16,616 rows)\n",
      "  âœ“ val_ground_truth.parquet (4,943 rows)\n",
      "  âœ“ user_activity_stats.parquet (49,576 rows)\n",
      "  âœ“ metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: SAVE PROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 9: SAVING PROCESSED DATA\")\n",
    "\n",
    "# Save to parquet\n",
    "print(\"Saving files...\")\n",
    "\n",
    "train_transactions.to_parquet(config.OUTPUT_PATH / 'train_transactions.parquet', index=False)\n",
    "print(f\"  âœ“ train_transactions.parquet ({len(train_transactions):,} rows)\")\n",
    "\n",
    "val_transactions.to_parquet(config.OUTPUT_PATH / 'val_transactions.parquet', index=False)\n",
    "print(f\"  âœ“ val_transactions.parquet ({len(val_transactions):,} rows)\")\n",
    "\n",
    "customers.to_parquet(config.OUTPUT_PATH / 'customers.parquet', index=False)\n",
    "print(f\"  âœ“ customers.parquet ({len(customers):,} rows)\")\n",
    "\n",
    "articles.to_parquet(config.OUTPUT_PATH / 'articles.parquet', index=False)\n",
    "print(f\"  âœ“ articles.parquet ({len(articles):,} rows)\")\n",
    "\n",
    "val_ground_truth.to_parquet(config.OUTPUT_PATH / 'val_ground_truth.parquet', index=False)\n",
    "print(f\"  âœ“ val_ground_truth.parquet ({len(val_ground_truth):,} rows)\")\n",
    "\n",
    "# Save user activity for analysis\n",
    "sampled_activity.to_parquet(config.OUTPUT_PATH / 'user_activity_stats.parquet', index=False)\n",
    "print(f\"  âœ“ user_activity_stats.parquet ({len(sampled_activity):,} rows)\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_weeks': config.TOTAL_WEEKS,\n",
    "    'train_weeks': config.N_TRAIN_WEEKS,\n",
    "    'val_weeks': config.N_VAL_WEEKS,\n",
    "    'train_start_date': str(train_start_date.date()),\n",
    "    'train_end_date': str(train_end_date.date()),\n",
    "    'val_start_date': str(val_start_date.date()),\n",
    "    'val_end_date': str(val_end_date.date()),\n",
    "    'target_users': config.TARGET_USERS,\n",
    "    'actual_users': len(selected_users),\n",
    "    'users_in_validation': len(val_users),\n",
    "    'cold_start_users': len(sampled_cold_start) if config.INCLUDE_COLD_START else 0,\n",
    "    'regular_users': len(sampled_regular) if config.INCLUDE_COLD_START else len(selected_users),\n",
    "    'cold_start_ratio': config.COLD_START_RATIO if config.INCLUDE_COLD_START else 0,\n",
    "    'n_items': len(selected_items),\n",
    "    'n_train_transactions': len(train_transactions),\n",
    "    'n_val_transactions': len(val_transactions),\n",
    "    'stratified': config.STRATIFY_BY_ACTIVITY,\n",
    "    'min_user_purchases': config.MIN_USER_PURCHASES,\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(config.OUTPUT_PATH / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"  âœ“ metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PREPROCESSING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Final dataset summary:\n",
      "  ðŸ“… Total weeks considered: 24\n",
      "  ðŸ“… Training weeks: 11\n",
      "  ðŸ“… Validation weeks: 1\n",
      "  ðŸ‘¥ Target users: 50,000\n",
      "  ðŸ‘¥ Actual sampled users: 49,576\n",
      "  â„ï¸  Cold start users: 7,500 (15.1%)\n",
      "  ðŸ”¥ Regular users: 42,500 (85.7%)\n",
      "  ðŸ‘¥ Users in validation: 4,943 (10.0%)\n",
      "  ðŸ›ï¸  Items: 16,616\n",
      "  ðŸ“Š Train transactions: 412,156\n",
      "  ðŸ“Š Val transactions: 16,480\n",
      "  ðŸ“Š Avg transactions per user (train): 8.31\n",
      "\n",
      "  ðŸ“ˆ Sampling was stratified by user activity level\n",
      "  â„ï¸  Cold start users included for testing recommendations with limited history\n",
      "\n",
      "âœ… Ready for Stage 2: Recall Strategies!\n",
      "\n",
      "Next steps:\n",
      "  1. Review the saved files in /kaggle/working/\n",
      "  2. Check metadata.json for dataset info\n",
      "  3. Analyze user_activity_stats.parquet for sampling quality\n",
      "  4. Proceed to Stage 2 when ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"PREPROCESSING COMPLETE!\")\n",
    "\n",
    "print(\"\\nFinal dataset summary:\")\n",
    "print(f\"Total weeks considered: {config.TOTAL_WEEKS}\")\n",
    "print(f\"Training weeks: {config.N_TRAIN_WEEKS}\")\n",
    "print(f\"Validation weeks: {config.N_VAL_WEEKS}\")\n",
    "print(f\"Target users: {config.TARGET_USERS:,}\")\n",
    "print(f\"Actual sampled users: {len(selected_users):,}\")\n",
    "if config.INCLUDE_COLD_START:\n",
    "    print(f\"Cold start users: {len(sampled_cold_start):,} ({100*len(sampled_cold_start)/len(selected_users):.1f}%)\")\n",
    "    print(f\"Regular users: {len(sampled_regular):,} ({100*len(sampled_regular)/len(selected_users):.1f}%)\")\n",
    "print(f\"Users in validation: {len(val_users):,} ({100*len(val_users)/len(selected_users):.1f}%)\")\n",
    "print(f\"Items: {len(selected_items):,}\")\n",
    "print(f\"Train transactions: {len(train_transactions):,}\")\n",
    "print(f\"Val transactions: {len(val_transactions):,}\")\n",
    "print(f\"Avg transactions per user (train): {len(train_transactions)/len(selected_users):.2f}\")\n",
    "\n",
    "if config.STRATIFY_BY_ACTIVITY:\n",
    "    print(\"\\n Sampling was stratified by user activity level\")\n",
    "if config.INCLUDE_COLD_START:\n",
    "    print(f\"Cold start users included for testing recommendations with limited history\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a91595",
   "metadata": {},
   "source": [
    "### Stage 2: Generating Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8b514906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from text_features import integrate_text_features_stage2\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e79aefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    print(f\"  ðŸ’¾ Memory: {mem:.2f} GB\")\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Aggressive garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "98b7f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_2')\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    \n",
    "    # Recall configuration - REDUCED for memory\n",
    "    N_REPURCHASE_CANDIDATES = 25  # Reduced from 30\n",
    "    N_POPULARITY_CANDIDATES = 25  # Reduced from 30\n",
    "    N_COPURCHASE_CANDIDATES = 15  # Reduced from 20\n",
    "    N_USERKNN_CANDIDATES = 15     # Reduced from 20\n",
    "    N_CATEGORY_CANDIDATES = 15    # Reduced from 20\n",
    "    \n",
    "    # Processing parameters\n",
    "    USER_CHUNK_SIZE = 1000  # Process users in chunks\n",
    "    BASKET_CHUNK_SIZE = 5000  # Process baskets in chunks\n",
    "    \n",
    "    # EMERGENCY MODE: Use only recent data for repurchase\n",
    "    USE_RECENT_ONLY_REPURCHASE = True  # Set to True if kernel keeps crashing\n",
    "    REPURCHASE_RECENT_WEEKS = 8  # Only use last 8 weeks for repurchase\n",
    "    \n",
    "    # Item-to-Item CF parameters\n",
    "    MIN_ITEM_SUPPORT = 3\n",
    "    MAX_ITEM_NEIGHBORS = 30  # Reduced from 50\n",
    "    \n",
    "    # User-KNN parameters (ONLY for validation users)\n",
    "    N_SIMILAR_USERS = 20  # Reduced from 30\n",
    "    MIN_COMMON_ITEMS = 2\n",
    "    \n",
    "    # Time decay\n",
    "    REPURCHASE_DECAY_RATE = 0.05\n",
    "    POPULARITY_WINDOW_WEEKS = 2\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4ae37e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def time_decay_score(days_ago, decay_rate=0.05):\n",
    "    \"\"\"Vectorized time decay\"\"\"\n",
    "    return np.exp(-decay_rate * days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3188332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading data files...\n",
      "âœ“ Train transactions: 412,156\n",
      "  ðŸ’¾ Memory: 0.35 GB\n",
      "âœ“ Users: 47,543, Items: 15,932, Val users: 4,943\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"LOADING DATA\")\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "train_transactions = pd.read_parquet(config.DATA_PATH / 'train_transactions.parquet')\n",
    "val_ground_truth = pd.read_parquet(config.DATA_PATH / 'val_ground_truth.parquet')\n",
    "articles = pd.read_parquet(config.DATA_PATH / 'articles.parquet')\n",
    "\n",
    "print(f\"âœ“ Train transactions: {len(train_transactions):,}\")\n",
    "print_memory()\n",
    "\n",
    "all_users = train_transactions['customer_id'].unique()\n",
    "all_items = train_transactions['article_id'].unique()\n",
    "val_users = set(val_ground_truth['customer_id'].unique())\n",
    "max_date = train_transactions['t_dat'].max()\n",
    "\n",
    "print(f\"âœ“ Users: {len(all_users):,}, Items: {len(all_items):,}, Val users: {len(val_users):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2f84c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 1: REPURCHASE (CHUNKED)\n",
      "================================================================================\n",
      "\n",
      "Processing in chunks to save memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79af4e2f8ae4298a6b6d4aea1dcb48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User chunks:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated 325,177 repurchase candidates\n",
      "  ðŸ’¾ Memory: 1.36 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 1: REPURCHASE - CHUNKED PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 1: REPURCHASE (CHUNKED)\")\n",
    "\n",
    "print(\"Processing in chunks to save memory...\")\n",
    "repurchase_chunks = []\n",
    "\n",
    "# Split users into chunks\n",
    "user_chunks = np.array_split(all_users, max(1, len(all_users) // config.USER_CHUNK_SIZE))\n",
    "\n",
    "for i, user_chunk in enumerate(tqdm(user_chunks, desc=\"User chunks\")):\n",
    "    # Filter transactions for this chunk\n",
    "    chunk_trans = train_transactions[\n",
    "        train_transactions['customer_id'].isin(user_chunk)\n",
    "    ].copy()\n",
    "    \n",
    "    # Get last purchase per user-item\n",
    "    user_item_last = (\n",
    "        chunk_trans\n",
    "        .groupby(['customer_id', 'article_id'], as_index=False)['t_dat']\n",
    "        .max()\n",
    "    )\n",
    "    \n",
    "    # Calculate scores (vectorized) - with NaN handling\n",
    "    user_item_last['days_ago'] = (max_date - user_item_last['t_dat']).dt.days\n",
    "    \n",
    "    # Drop any NaN values before converting to int\n",
    "    user_item_last = user_item_last.dropna(subset=['days_ago'])\n",
    "    \n",
    "    # Now safe to convert to int\n",
    "    user_item_last['days_ago'] = user_item_last['days_ago'].astype(np.int16)\n",
    "    user_item_last['repurchase_score'] = time_decay_score(\n",
    "        user_item_last['days_ago'].values, \n",
    "        config.REPURCHASE_DECAY_RATE\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    # Get top N per user\n",
    "    top_candidates = (\n",
    "        user_item_last\n",
    "        .sort_values(['customer_id', 'repurchase_score'], ascending=[True, False])\n",
    "        .groupby('customer_id', as_index=False)\n",
    "        .head(config.N_REPURCHASE_CANDIDATES)\n",
    "        [['customer_id', 'article_id', 'repurchase_score']]\n",
    "    )\n",
    "    \n",
    "    repurchase_chunks.append(top_candidates)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk_trans, user_item_last, top_candidates\n",
    "    force_garbage_collection()\n",
    "\n",
    "# Combine chunks\n",
    "repurchase_candidates = pd.concat(repurchase_chunks, ignore_index=True)\n",
    "del repurchase_chunks\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"âœ“ Generated {len(repurchase_candidates):,} repurchase candidates\")\n",
    "print_memory()\n",
    "\n",
    "# Save intermediate result\n",
    "repurchase_candidates.to_parquet(config.OUTPUT_PATH / 'temp_repurchase.parquet', index=False)\n",
    "del repurchase_candidates\n",
    "force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cab3709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 2: POPULARITY\n",
      "================================================================================\n",
      "\n",
      "Using 32,152 recent transactions\n",
      "âœ“ Top 25 popular items\n",
      "Creating popularity candidates in chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610c0f1fb31b42ca878e0029d3139da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Popularity chunks:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated 1,188,575 popularity candidates\n",
      "  ðŸ’¾ Memory: 1.47 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 2: POPULARITY\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 2: POPULARITY\")\n",
    "\n",
    "cutoff_date = max_date - timedelta(weeks=config.POPULARITY_WINDOW_WEEKS)\n",
    "recent_trans = train_transactions[train_transactions['t_dat'] >= cutoff_date].copy()\n",
    "\n",
    "print(f\"Using {len(recent_trans):,} recent transactions\")\n",
    "\n",
    "# Vectorized calculations\n",
    "recent_trans['days_ago'] = (max_date - recent_trans['t_dat']).dt.days\n",
    "\n",
    "# Drop NaN values\n",
    "recent_trans = recent_trans.dropna(subset=['days_ago'])\n",
    "\n",
    "# Convert to int\n",
    "recent_trans['days_ago'] = recent_trans['days_ago'].astype(np.int16)\n",
    "recent_trans['weight'] = time_decay_score(recent_trans['days_ago'].values, 0.1).astype(np.float32)\n",
    "\n",
    "# Aggregate\n",
    "item_popularity = (\n",
    "    recent_trans\n",
    "    .groupby('article_id', as_index=False)\n",
    "    .agg({'weight': 'sum', 'customer_id': 'nunique'})\n",
    "    .rename(columns={'weight': 'weighted_purchases', 'customer_id': 'unique_buyers'})\n",
    ")\n",
    "\n",
    "item_popularity['popularity_score'] = (\n",
    "    0.7 * item_popularity['weighted_purchases'] + \n",
    "    0.3 * item_popularity['unique_buyers']\n",
    ")\n",
    "item_popularity['popularity_score'] = (\n",
    "    item_popularity['popularity_score'] / item_popularity['popularity_score'].max()\n",
    ").astype(np.float32)\n",
    "\n",
    "# Get top items\n",
    "top_items = item_popularity.nlargest(config.N_POPULARITY_CANDIDATES, 'popularity_score')\n",
    "\n",
    "print(f\"âœ“ Top {len(top_items)} popular items\")\n",
    "\n",
    "# Create candidates - CHUNKED\n",
    "print(\"Creating popularity candidates in chunks...\")\n",
    "pop_chunks = []\n",
    "\n",
    "for user_chunk in tqdm(np.array_split(all_users, 20), desc=\"Popularity chunks\"):\n",
    "    chunk_df = pd.DataFrame({\n",
    "        'customer_id': np.repeat(user_chunk, len(top_items)),\n",
    "        'article_id': np.tile(top_items['article_id'].values, len(user_chunk))\n",
    "    })\n",
    "    \n",
    "    rank_penalty = np.tile(1 - np.arange(len(top_items)) * 0.01, len(user_chunk))\n",
    "    scores = np.tile(top_items['popularity_score'].values, len(user_chunk))\n",
    "    chunk_df['popularity_score'] = (scores * rank_penalty).astype(np.float32)\n",
    "    \n",
    "    pop_chunks.append(chunk_df)\n",
    "\n",
    "popularity_candidates = pd.concat(pop_chunks, ignore_index=True)\n",
    "del pop_chunks, recent_trans\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"âœ“ Generated {len(popularity_candidates):,} popularity candidates\")\n",
    "print_memory()\n",
    "\n",
    "# Save\n",
    "popularity_candidates.to_parquet(config.OUTPUT_PATH / 'temp_popularity.parquet', index=False)\n",
    "item_popularity.to_parquet(config.OUTPUT_PATH / 'item_popularity.parquet', index=False)\n",
    "del popularity_candidates\n",
    "force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b0be7540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 3: CO-PURCHASE (Item-to-Item CF)\n",
      "================================================================================\n",
      "\n",
      "Building co-purchase matrix...\n",
      "  Baskets with 2+ items: 83,983\n",
      "Computing co-purchase frequencies...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bdc51b143544808d1a25443a37e9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing baskets:   0%|          | 0/83983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Built co-purchase matrix for 15,868 items\n",
      "Computing item-to-item similarity scores...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7706dee3464d258e8be7f25c869886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities:   0%|          | 0/15868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Computed similarities for 10,603 items\n",
      "Saving item-to-item similarity matrix...\n",
      "  âœ“ Saved item_to_items.pkl (10,603 items)\n",
      "Generating co-purchase candidates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f6e2cc16274c8e91eeafa954592434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User co-purchase recommendations:   0%|          | 0/47543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated 612,807 co-purchase candidates\n",
      "  Users with candidates: 45,344\n",
      "\n",
      "Saving co-purchase candidates...\n",
      "âœ“ Saved temp_copurchase.parquet (612,807 rows)\n",
      "âœ“ Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 3: CO-PURCHASE (Item-to-Item CF) - WITH PARQUET SAVING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 3: CO-PURCHASE (Item-to-Item CF)\")\n",
    "\n",
    "# Check if already computed\n",
    "if (config.OUTPUT_PATH / 'temp_copurchase.parquet').exists():\n",
    "    print(\"âš¡ Found existing co-purchase candidates, loading...\")\n",
    "    copurchase_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_copurchase.parquet')\n",
    "    print(f\"âœ“ Loaded {len(copurchase_candidates):,} co-purchase candidates\")\n",
    "    print(f\"  Users with candidates: {copurchase_candidates['customer_id'].nunique():,}\")\n",
    "else:\n",
    "    print(\"Building co-purchase matrix...\")\n",
    "\n",
    "    # Create item-to-item co-purchase matrix\n",
    "    # Group by transaction/basket (same user, same day)\n",
    "    train_transactions['basket_id'] = (\n",
    "        train_transactions['customer_id'].astype(str) + '_' + \n",
    "        train_transactions['t_dat'].astype(str)\n",
    "    )\n",
    "\n",
    "    # Get baskets with multiple items\n",
    "    basket_items = (\n",
    "        train_transactions\n",
    "        .groupby('basket_id')['article_id']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Filter baskets with at least 2 items\n",
    "    basket_items = basket_items[basket_items['article_id'].apply(len) >= 2]\n",
    "    print(f\"  Baskets with 2+ items: {len(basket_items):,}\")\n",
    "\n",
    "    # Build co-purchase counts\n",
    "    print(\"Computing co-purchase frequencies...\")\n",
    "    copurchase_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for items in tqdm(basket_items['article_id'], desc=\"Processing baskets\"):\n",
    "        # For each pair of items in the basket\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i + 1, len(items)):\n",
    "                item1, item2 = items[i], items[j]\n",
    "                copurchase_counts[item1][item2] += 1\n",
    "                copurchase_counts[item2][item1] += 1\n",
    "\n",
    "    print(f\"âœ“ Built co-purchase matrix for {len(copurchase_counts):,} items\")\n",
    "\n",
    "    # Convert to item-to-item similarity scores\n",
    "    print(\"Computing item-to-item similarity scores...\")\n",
    "    item_to_items = {}\n",
    "\n",
    "    for item1 in tqdm(copurchase_counts.keys(), desc=\"Computing similarities\"):\n",
    "        # Get co-purchased items\n",
    "        copurchased = copurchase_counts[item1]\n",
    "        \n",
    "        # Filter by minimum support\n",
    "        copurchased = {\n",
    "            item2: count \n",
    "            for item2, count in copurchased.items() \n",
    "            if count >= config.MIN_ITEM_SUPPORT\n",
    "        }\n",
    "        \n",
    "        if copurchased:\n",
    "            # Sort by count and take top K\n",
    "            top_items = sorted(\n",
    "                copurchased.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:config.MAX_ITEM_NEIGHBORS]\n",
    "            \n",
    "            # Normalize scores\n",
    "            max_count = top_items[0][1]\n",
    "            item_to_items[item1] = [\n",
    "                (item2, count / max_count) \n",
    "                for item2, count in top_items\n",
    "            ]\n",
    "\n",
    "    print(f\"âœ“ Computed similarities for {len(item_to_items):,} items\")\n",
    "\n",
    "    # Save item-to-item similarity matrix for potential reuse\n",
    "    print(\"Saving item-to-item similarity matrix...\")\n",
    "    import pickle\n",
    "    with open(config.OUTPUT_PATH / 'item_to_items.pkl', 'wb') as f:\n",
    "        pickle.dump(item_to_items, f)\n",
    "    print(f\"  âœ“ Saved item_to_items.pkl ({len(item_to_items):,} items)\")\n",
    "\n",
    "    # Generate co-purchase candidates for each user\n",
    "    print(\"Generating co-purchase candidates...\")\n",
    "    copurchase_candidates = []\n",
    "\n",
    "    # Get recent purchases for each user (last 10)\n",
    "    user_recent_items = (\n",
    "        train_transactions\n",
    "        .sort_values('t_dat', ascending=False)\n",
    "        .groupby('customer_id')['article_id']\n",
    "        .apply(lambda x: list(x.unique()[:10]))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    for user in tqdm(all_users, desc=\"User co-purchase recommendations\"):\n",
    "        if user not in user_recent_items:\n",
    "            continue\n",
    "        \n",
    "        user_items = user_recent_items[user]\n",
    "        candidate_scores = defaultdict(float)\n",
    "        \n",
    "        # Aggregate scores from all user's items\n",
    "        for user_item in user_items:\n",
    "            if user_item in item_to_items:\n",
    "                for similar_item, score in item_to_items[user_item]:\n",
    "                    if similar_item not in user_items:  # Don't recommend already purchased\n",
    "                        candidate_scores[similar_item] += score\n",
    "        \n",
    "        # Get top N candidates\n",
    "        if candidate_scores:\n",
    "            top_candidates = sorted(\n",
    "                candidate_scores.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:config.N_COPURCHASE_CANDIDATES]\n",
    "            \n",
    "            for item, score in top_candidates:\n",
    "                copurchase_candidates.append({\n",
    "                    'customer_id': user,\n",
    "                    'article_id': item,\n",
    "                    'copurchase_score': score\n",
    "                })\n",
    "\n",
    "    copurchase_candidates = pd.DataFrame(copurchase_candidates)\n",
    "    print(f\"âœ“ Generated {len(copurchase_candidates):,} co-purchase candidates\")\n",
    "    print(f\"  Users with candidates: {copurchase_candidates['customer_id'].nunique():,}\")\n",
    "\n",
    "    # Save to parquet\n",
    "    print(\"\\nSaving co-purchase candidates...\")\n",
    "    copurchase_candidates.to_parquet(config.OUTPUT_PATH / 'temp_copurchase.parquet', index=False)\n",
    "    print(f\"âœ“ Saved temp_copurchase.parquet ({len(copurchase_candidates):,} rows)\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del basket_items, copurchase_counts, item_to_items, user_recent_items\n",
    "    force_garbage_collection()\n",
    "    print(\"âœ“ Memory cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c9998ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 4: USER-KNN COLLABORATIVE FILTERING\n",
      "================================================================================\n",
      "\n",
      "Building user-item matrix...\n",
      "  Matrix size: 47,543 users x 15,932 items\n",
      "Populating user-item matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabed002e19f4ae7aa185d449f7766b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building matrix:   0%|          | 0/62015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Matrix density: 0.0072%\n",
      "Saving user-item matrix...\n",
      "  âœ“ Saved user_item_matrix.npz\n",
      "  âœ“ Saved index mappings\n",
      "Computing user similarities for 4,943 validation users...\n",
      "Computing cosine similarities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b129eb784cd34016ab1c72fe048f2b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Similarity batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated 30,107 user-KNN candidates\n",
      "  Users with candidates: 2,018\n",
      "\n",
      "Saving user-KNN candidates...\n",
      "âœ“ Saved temp_userknn.parquet (30,107 rows)\n",
      "âœ“ Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 4: USER-KNN COLLABORATIVE FILTERING - WITH PARQUET SAVING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 4: USER-KNN COLLABORATIVE FILTERING\")\n",
    "\n",
    "# Check if already computed\n",
    "if (config.OUTPUT_PATH / 'temp_userknn.parquet').exists():\n",
    "    print(\"âš¡ Found existing user-KNN candidates, loading...\")\n",
    "    userknn_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_userknn.parquet')\n",
    "    print(f\"âœ“ Loaded {len(userknn_candidates):,} user-KNN candidates\")\n",
    "    print(f\"  Users with candidates: {userknn_candidates['customer_id'].nunique():,}\")\n",
    "else:\n",
    "    print(\"Building user-item matrix...\")\n",
    "\n",
    "    # Create sparse user-item matrix (binary: 1 if purchased, 0 otherwise)\n",
    "    # Map users and items to indices\n",
    "    user_to_idx = {user: idx for idx, user in enumerate(all_users)}\n",
    "    item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
    "\n",
    "    # Create matrix\n",
    "    n_users = len(all_users)\n",
    "    n_items = len(all_items)\n",
    "\n",
    "    print(f\"  Matrix size: {n_users:,} users x {n_items:,} items\")\n",
    "\n",
    "    # Use last 4 weeks for user similarity (more recent = more relevant)\n",
    "    recent_date = max_date - timedelta(weeks=4)\n",
    "    recent_user_items = train_transactions[train_transactions['t_dat'] >= recent_date].copy()\n",
    "\n",
    "    user_item_matrix = lil_matrix((n_users, n_items), dtype=np.int8)\n",
    "\n",
    "    print(\"Populating user-item matrix...\")\n",
    "    for _, row in tqdm(recent_user_items.iterrows(), total=len(recent_user_items), desc=\"Building matrix\"):\n",
    "        user_idx = user_to_idx[row['customer_id']]\n",
    "        item_idx = item_to_idx[row['article_id']]\n",
    "        user_item_matrix[user_idx, item_idx] = 1\n",
    "\n",
    "    # Convert to CSR for efficient operations\n",
    "    user_item_matrix = user_item_matrix.tocsr()\n",
    "    print(f\"âœ“ Matrix density: {user_item_matrix.nnz / (n_users * n_items) * 100:.4f}%\")\n",
    "\n",
    "    # Save user-item matrix for potential reuse\n",
    "    print(\"Saving user-item matrix...\")\n",
    "    from scipy.sparse import save_npz\n",
    "    save_npz(config.OUTPUT_PATH / 'user_item_matrix.npz', user_item_matrix)\n",
    "    print(f\"  âœ“ Saved user_item_matrix.npz\")\n",
    "\n",
    "    # Save user/item mappings\n",
    "    import pickle\n",
    "    with open(config.OUTPUT_PATH / 'user_to_idx.pkl', 'wb') as f:\n",
    "        pickle.dump(user_to_idx, f)\n",
    "    with open(config.OUTPUT_PATH / 'item_to_idx.pkl', 'wb') as f:\n",
    "        pickle.dump(item_to_idx, f)\n",
    "    print(f\"  âœ“ Saved index mappings\")\n",
    "\n",
    "    # Compute user-user similarity (only for validation users to save memory)\n",
    "    print(f\"Computing user similarities for {len(val_users):,} validation users...\")\n",
    "\n",
    "    val_user_indices = [user_to_idx[user] for user in val_users if user in user_to_idx]\n",
    "    val_user_matrix = user_item_matrix[val_user_indices]\n",
    "\n",
    "    # Normalize rows\n",
    "    val_user_matrix_norm = normalize(val_user_matrix, norm='l2', axis=1)\n",
    "    user_item_matrix_norm = normalize(user_item_matrix, norm='l2', axis=1)\n",
    "\n",
    "    # Compute similarity (batch processing to avoid memory issues)\n",
    "    print(\"Computing cosine similarities...\")\n",
    "    batch_size = 1000\n",
    "    userknn_candidates = []\n",
    "\n",
    "    for i in tqdm(range(0, len(val_user_indices), batch_size), desc=\"Similarity batches\"):\n",
    "        batch_indices = val_user_indices[i:i+batch_size]\n",
    "        batch_matrix = val_user_matrix_norm[i:i+batch_size]\n",
    "        \n",
    "        # Compute similarity with all users\n",
    "        similarities = cosine_similarity(batch_matrix, user_item_matrix_norm)\n",
    "        \n",
    "        # For each user in batch\n",
    "        for j, user_idx in enumerate(batch_indices):\n",
    "            user = all_users[user_idx]\n",
    "            user_sims = similarities[j]\n",
    "            \n",
    "            # Get top similar users (exclude self)\n",
    "            similar_user_indices = np.argsort(user_sims)[::-1][1:config.N_SIMILAR_USERS+1]\n",
    "            \n",
    "            # Get items purchased by similar users\n",
    "            candidate_scores = defaultdict(float)\n",
    "            user_purchased = set(\n",
    "                train_transactions[train_transactions['customer_id'] == user]['article_id']\n",
    "            )\n",
    "            \n",
    "            for sim_user_idx in similar_user_indices:\n",
    "                sim_score = user_sims[sim_user_idx]\n",
    "                if sim_score < 0.01:  # Skip very dissimilar users\n",
    "                    continue\n",
    "                \n",
    "                sim_user = all_users[sim_user_idx]\n",
    "                sim_user_items = train_transactions[\n",
    "                    train_transactions['customer_id'] == sim_user\n",
    "                ]['article_id'].unique()\n",
    "                \n",
    "                for item in sim_user_items:\n",
    "                    if item not in user_purchased:\n",
    "                        candidate_scores[item] += sim_score\n",
    "            \n",
    "            # Get top N candidates\n",
    "            if candidate_scores:\n",
    "                top_candidates = sorted(\n",
    "                    candidate_scores.items(), \n",
    "                    key=lambda x: x[1], \n",
    "                    reverse=True\n",
    "                )[:config.N_USERKNN_CANDIDATES]\n",
    "                \n",
    "                for item, score in top_candidates:\n",
    "                    userknn_candidates.append({\n",
    "                        'customer_id': user,\n",
    "                        'article_id': item,\n",
    "                        'userknn_score': score\n",
    "                    })\n",
    "\n",
    "    userknn_candidates = pd.DataFrame(userknn_candidates)\n",
    "    print(f\"âœ“ Generated {len(userknn_candidates):,} user-KNN candidates\")\n",
    "    print(f\"  Users with candidates: {userknn_candidates['customer_id'].nunique():,}\")\n",
    "\n",
    "    # Save to parquet\n",
    "    print(\"\\nSaving user-KNN candidates...\")\n",
    "    userknn_candidates.to_parquet(config.OUTPUT_PATH / 'temp_userknn.parquet', index=False)\n",
    "    print(f\"âœ“ Saved temp_userknn.parquet ({len(userknn_candidates):,} rows)\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del user_item_matrix, val_user_matrix, val_user_matrix_norm, user_item_matrix_norm\n",
    "    del user_to_idx, item_to_idx, recent_user_items\n",
    "    force_garbage_collection()\n",
    "    print(\"âœ“ Memory cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6dea05b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 5: CATEGORY-BASED RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Computing user category preferences...\n",
      "âœ“ Computed preferences for 47,543 users\n",
      "Saving user category preferences...\n",
      "  âœ“ Saved user_category_preferences.parquet\n",
      "Saving category popular items...\n",
      "  âœ“ Saved category_popular_items.parquet\n",
      "Generating category-based candidates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dcfb5d931f4f5fab3b305b9d57c3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Category recommendations:   0%|          | 0/142629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated 1,409,632 category-based candidates\n",
      "  Users with candidates: 47,543\n",
      "\n",
      "Saving category candidates...\n",
      "âœ“ Saved temp_category.parquet (1,409,632 rows)\n",
      "âœ“ Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 5: CATEGORY-BASED RECOMMENDATIONS - WITH PARQUET SAVING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 5: CATEGORY-BASED RECOMMENDATIONS\")\n",
    "\n",
    "# Check if already computed\n",
    "if (config.OUTPUT_PATH / 'temp_category.parquet').exists():\n",
    "    print(\"âš¡ Found existing category candidates, loading...\")\n",
    "    category_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_category.parquet')\n",
    "    print(f\"âœ“ Loaded {len(category_candidates):,} category candidates\")\n",
    "    print(f\"  Users with candidates: {category_candidates['customer_id'].nunique():,}\")\n",
    "else:\n",
    "    print(\"Computing user category preferences...\")\n",
    "\n",
    "    # Get user's category preferences\n",
    "    user_categories = (\n",
    "        train_transactions\n",
    "        .merge(articles[['article_id', 'product_type_no', 'product_group_name']], on='article_id')\n",
    "        .groupby(['customer_id', 'product_type_no'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # Get top 3 categories per user\n",
    "    user_top_categories = (\n",
    "        user_categories\n",
    "        .sort_values(['customer_id', 'count'], ascending=[True, False])\n",
    "        .groupby('customer_id')\n",
    "        .head(3)\n",
    "    )\n",
    "\n",
    "    print(f\"âœ“ Computed preferences for {user_top_categories['customer_id'].nunique():,} users\")\n",
    "\n",
    "    # Save user category preferences for potential reuse\n",
    "    print(\"Saving user category preferences...\")\n",
    "    user_top_categories.to_parquet(config.OUTPUT_PATH / 'user_category_preferences.parquet', index=False)\n",
    "    print(f\"  âœ“ Saved user_category_preferences.parquet\")\n",
    "\n",
    "    # Get popular items per category\n",
    "    category_popular_items = (\n",
    "        train_transactions[train_transactions['t_dat'] >= cutoff_date]\n",
    "        .merge(articles[['article_id', 'product_type_no']], on='article_id')\n",
    "        .groupby(['product_type_no', 'article_id'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "        .sort_values(['product_type_no', 'count'], ascending=[True, False])\n",
    "        .groupby('product_type_no')\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    # Save category popular items for potential reuse\n",
    "    print(\"Saving category popular items...\")\n",
    "    category_popular_items.to_parquet(config.OUTPUT_PATH / 'category_popular_items.parquet', index=False)\n",
    "    print(f\"  âœ“ Saved category_popular_items.parquet\")\n",
    "\n",
    "    print(\"Generating category-based candidates...\")\n",
    "    category_candidates = []\n",
    "\n",
    "    for _, row in tqdm(user_top_categories.iterrows(), total=len(user_top_categories), desc=\"Category recommendations\"):\n",
    "        user = row['customer_id']\n",
    "        category = row['product_type_no']\n",
    "        \n",
    "        # Get popular items in this category\n",
    "        category_items = category_popular_items[\n",
    "            category_popular_items['product_type_no'] == category\n",
    "        ]['article_id'].tolist()\n",
    "        \n",
    "        # Get user's purchased items\n",
    "        user_items = set(\n",
    "            train_transactions[train_transactions['customer_id'] == user]['article_id']\n",
    "        )\n",
    "        \n",
    "        # Recommend items not yet purchased\n",
    "        for rank, item in enumerate(category_items):\n",
    "            if item not in user_items and rank < config.N_CATEGORY_CANDIDATES:\n",
    "                category_candidates.append({\n",
    "                    'customer_id': user,\n",
    "                    'article_id': item,\n",
    "                    'category_score': 1.0 / (rank + 1)  # Rank-based score\n",
    "                })\n",
    "\n",
    "    category_candidates = pd.DataFrame(category_candidates)\n",
    "    print(f\"âœ“ Generated {len(category_candidates):,} category-based candidates\")\n",
    "    print(f\"  Users with candidates: {category_candidates['customer_id'].nunique():,}\")\n",
    "\n",
    "    # Save to parquet\n",
    "    print(\"\\nSaving category candidates...\")\n",
    "    category_candidates.to_parquet(config.OUTPUT_PATH / 'temp_category.parquet', index=False)\n",
    "    print(f\"âœ“ Saved temp_category.parquet ({len(category_candidates):,} rows)\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del user_categories, user_top_categories, category_popular_items\n",
    "    force_garbage_collection()\n",
    "    print(\"âœ“ Memory cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7a9ef084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 6: TEXT SIMILARITY RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "  STAGE 2 ENHANCEMENT: TEXT-BASED CANDIDATES\n",
      "================================================================================\n",
      "\n",
      "Creating text corpus from articles...\n",
      "  Available text columns: 12/12\n",
      "    Processed 0 articles...\n",
      "  âœ“ Created corpus for 16,616 articles\n",
      "\n",
      "Computing text embeddings...\n",
      "  Valid documents: 16,616\n",
      "  Computing TF-IDF...\n",
      "  âœ“ TF-IDF shape: (16616, 100)\n",
      "  Reducing to 20 dimensions...\n",
      "  âœ“ Embeddings shape: (16616, 20)\n",
      "  Explained variance: 0.760\n",
      "\n",
      "Computing user text preferences...\n",
      "  Building user preference vectors...\n",
      "  âœ“ Computed preferences for 47,543 users\n",
      "\n",
      "Generating text similarity candidates...\n",
      "  Processing 47,543 users...\n",
      "    Processed 10,000 users...\n",
      "    Processed 20,000 users...\n",
      "    Processed 30,000 users...\n",
      "    Processed 40,000 users...\n",
      "  âœ“ Generated 713,145 text similarity candidates\n",
      "\n",
      "âœ“ Saved text similarity candidates to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/temp_text_similarity.parquet\n",
      "\n",
      "Saving embeddings for Stage 3...\n",
      "âœ“ Saved embeddings\n",
      "âœ“ Saved 713,145 text similarity candidates\n"
     ]
    }
   ],
   "source": [
    "print_section(\"STRATEGY 6: TEXT SIMILARITY RECOMMENDATIONS\")\n",
    "\n",
    "# Import the text feature module (save the artifact code as text_features.py)\n",
    "from text_features import integrate_text_features_stage2\n",
    "\n",
    "# Generate text-based candidates\n",
    "text_candidates, article_embeddings, user_embeddings, text_cols = integrate_text_features_stage2(\n",
    "    all_users=all_users,\n",
    "    train_transactions=train_transactions,\n",
    "    articles=articles,\n",
    "    output_path=config.OUTPUT_PATH\n",
    ")\n",
    "\n",
    "# Save for later use\n",
    "if text_candidates is not None and len(text_candidates) > 0:\n",
    "    text_candidates.to_parquet(config.OUTPUT_PATH / 'temp_text_similarity.parquet', index=False)\n",
    "    print(f\"âœ“ Saved {len(text_candidates):,} text similarity candidates\")\n",
    "else:\n",
    "    print(\"âš ï¸  No text candidates generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b1c67e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  COMBINING ALL RECALL STRATEGIES\n",
      "================================================================================\n",
      "\n",
      "Loading candidates from parquet files...\n",
      "  Loading repurchase candidates...\n",
      "    âœ“ 325,177 candidates\n",
      "  Loading popularity candidates...\n",
      "    âœ“ 1,188,575 candidates\n",
      "  Loading co-purchase candidates...\n",
      "    âœ“ 612,807 candidates\n",
      "  Loading user-KNN candidates...\n",
      "    âœ“ 30,107 candidates\n",
      "  Loading category candidates...\n",
      "    âœ“ 1,409,632 candidates\n",
      "  Loading text similarity candidates...\n",
      "    âœ“ 713,145 candidates\n",
      "\n",
      "Merging candidates from all strategies...\n",
      "âœ“ Total unique user-item pairs: 4,044,442\n",
      "\n",
      "Candidate statistics:\n",
      "  Candidates per user: 85.07\n",
      "  Avg strategies per candidate: 1.06\n",
      "\n",
      "  Candidates by number of strategies:\n",
      "    1 strategies: 3,816,416 (94.4%)\n",
      "    2 strategies: 221,301 (5.5%)\n",
      "    3 strategies: 6,479 (0.2%)\n",
      "    4 strategies: 242 (0.0%)\n",
      "    5 strategies: 4 (0.0%)\n",
      "\n",
      "Saving merged candidates...\n",
      "âœ“ Saved to all_candidates_merged.parquet (4,044,442 rows)\n",
      "\n",
      "Cleaning up temporary dataframes...\n",
      "âœ“ Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "print_section(\"COMBINING ALL RECALL STRATEGIES\")\n",
    "\n",
    "print(\"Loading candidates from parquet files...\")\n",
    "\n",
    "# Load all candidates (existing code)\n",
    "print(\"  Loading repurchase candidates...\")\n",
    "repurchase_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_repurchase.parquet')\n",
    "print(f\"    âœ“ {len(repurchase_candidates):,} candidates\")\n",
    "\n",
    "print(\"  Loading popularity candidates...\")\n",
    "popularity_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_popularity.parquet')\n",
    "print(f\"    âœ“ {len(popularity_candidates):,} candidates\")\n",
    "\n",
    "print(\"  Loading co-purchase candidates...\")\n",
    "copurchase_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_copurchase.parquet')\n",
    "print(f\"    âœ“ {len(copurchase_candidates):,} candidates\")\n",
    "\n",
    "print(\"  Loading user-KNN candidates...\")\n",
    "userknn_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_userknn.parquet')\n",
    "print(f\"    âœ“ {len(userknn_candidates):,} candidates\")\n",
    "\n",
    "print(\"  Loading category candidates...\")\n",
    "category_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_category.parquet')\n",
    "print(f\"    âœ“ {len(category_candidates):,} candidates\")\n",
    "\n",
    "# NEW: Load text similarity candidates\n",
    "if (config.OUTPUT_PATH / 'temp_text_similarity.parquet').exists():\n",
    "    print(\"  Loading text similarity candidates...\")\n",
    "    text_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_text_similarity.parquet')\n",
    "    print(f\"    âœ“ {len(text_candidates):,} candidates\")\n",
    "    has_text_candidates = True\n",
    "else:\n",
    "    print(\"  âš ï¸  No text similarity candidates found\")\n",
    "    has_text_candidates = False\n",
    "\n",
    "print(\"\\nMerging candidates from all strategies...\")\n",
    "\n",
    "# Start with repurchase candidates (existing code)\n",
    "all_candidates = repurchase_candidates.copy()\n",
    "\n",
    "# Merge popularity\n",
    "all_candidates = all_candidates.merge(\n",
    "    popularity_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_pop')\n",
    ")\n",
    "\n",
    "# Merge co-purchase\n",
    "all_candidates = all_candidates.merge(\n",
    "    copurchase_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_cop')\n",
    ")\n",
    "\n",
    "# Merge user-KNN\n",
    "all_candidates = all_candidates.merge(\n",
    "    userknn_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_knn')\n",
    ")\n",
    "\n",
    "# Merge category\n",
    "all_candidates = all_candidates.merge(\n",
    "    category_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_cat')\n",
    ")\n",
    "\n",
    "# NEW: Merge text similarity\n",
    "if has_text_candidates:\n",
    "    all_candidates = all_candidates.merge(\n",
    "        text_candidates,\n",
    "        on=['customer_id', 'article_id'],\n",
    "        how='outer',\n",
    "        suffixes=('', '_text')\n",
    "    )\n",
    "\n",
    "# Fill NaN scores with 0\n",
    "score_columns = [\n",
    "    'repurchase_score', 'popularity_score', 'copurchase_score', \n",
    "    'userknn_score', 'category_score'\n",
    "]\n",
    "\n",
    "# NEW: Add text similarity score\n",
    "if has_text_candidates:\n",
    "    score_columns.append('text_similarity_score')\n",
    "\n",
    "all_candidates[score_columns] = all_candidates[score_columns].fillna(0)\n",
    "\n",
    "print(f\"âœ“ Total unique user-item pairs: {len(all_candidates):,}\")\n",
    "\n",
    "# Count how many strategies recommend each item\n",
    "all_candidates['n_strategies'] = (all_candidates[score_columns] > 0).sum(axis=1)\n",
    "\n",
    "print(\"\\nCandidate statistics:\")\n",
    "print(f\"  Candidates per user: {len(all_candidates) / all_candidates['customer_id'].nunique():.2f}\")\n",
    "print(f\"  Avg strategies per candidate: {all_candidates['n_strategies'].mean():.2f}\")\n",
    "print(\"\\n  Candidates by number of strategies:\")\n",
    "for n in sorted(all_candidates['n_strategies'].unique()):\n",
    "    count = (all_candidates['n_strategies'] == n).sum()\n",
    "    pct = count / len(all_candidates) * 100\n",
    "    print(f\"    {n} strategies: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Save the merged candidates\n",
    "print(\"\\nSaving merged candidates...\")\n",
    "all_candidates.to_parquet(config.OUTPUT_PATH / 'all_candidates_merged.parquet', index=False)\n",
    "print(f\"âœ“ Saved to all_candidates_merged.parquet ({len(all_candidates):,} rows)\")\n",
    "\n",
    "# Clean up to save memory\n",
    "print(\"\\nCleaning up temporary dataframes...\")\n",
    "del repurchase_candidates, popularity_candidates, copurchase_candidates\n",
    "del userknn_candidates, category_candidates\n",
    "if has_text_candidates:\n",
    "    del text_candidates\n",
    "gc.collect()\n",
    "print(\"âœ“ Memory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9375a",
   "metadata": {},
   "source": [
    "### Stage 3: Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "06f8f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "312e9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    print(f\"  ðŸ’¾ Memory: {mem:.2f} GB\")\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Aggressive garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3f1e40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2')\n",
    "    \n",
    "    # Processing\n",
    "    CHUNK_SIZE = 50000  # Process candidates in chunks\n",
    "    \n",
    "    # Feature engineering windows\n",
    "    RECENT_DAYS = 7  # Last week\n",
    "    MEDIUM_DAYS = 30  # Last month\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec87e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading preprocessed data...\n",
      "âœ“ Loaded all_candidates_merged.parquet\n",
      "âœ“ Train transactions: 412,156\n",
      "âœ“ Articles: 16,616\n",
      "âœ“ Customers: 49,576\n",
      "âœ“ Candidates: 4,044,442\n",
      "  ðŸ’¾ Memory: 1.56 GB\n",
      "âœ“ Max date: 2020-09-14\n",
      "âœ“ Item popularity scores: 7,115\n",
      "âœ“ Available recall scores in candidates: ['repurchase_score', 'popularity_score', 'copurchase_score', 'userknn_score', 'category_score', 'text_similarity_score']\n",
      "âœ“ Co-purchase scores available in candidates\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "print_section(\"LOADING DATA\")\n",
    "\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Check which candidate file exists\n",
    "if (config.DATA_PATH / 'all_candidates_merged.parquet').exists():\n",
    "    candidates = pd.read_parquet(config.DATA_PATH / 'all_candidates_merged.parquet')\n",
    "    print(f\"âœ“ Loaded all_candidates_merged.parquet\")\n",
    "elif (config.DATA_PATH / 'recall_candidates.parquet').exists():\n",
    "    candidates = pd.read_parquet(config.DATA_PATH / 'recall_candidates.parquet')\n",
    "    print(f\"âœ“ Loaded recall_candidates.parquet\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find candidates file (all_candidates_merged.parquet or recall_candidates.parquet)\")\n",
    "\n",
    "train_transactions = pd.read_parquet(config.DATA_PATH / 'train_transactions.parquet')\n",
    "articles = pd.read_parquet(config.DATA_PATH / 'articles.parquet')\n",
    "customers = pd.read_parquet(config.DATA_PATH / 'customers.parquet')\n",
    "\n",
    "print(f\"âœ“ Train transactions: {len(train_transactions):,}\")\n",
    "print(f\"âœ“ Articles: {len(articles):,}\")\n",
    "print(f\"âœ“ Customers: {len(customers):,}\")\n",
    "print(f\"âœ“ Candidates: {len(candidates):,}\")\n",
    "print_memory()\n",
    "\n",
    "# Get max date\n",
    "max_date = train_transactions['t_dat'].max()\n",
    "print(f\"âœ“ Max date: {max_date.date()}\")\n",
    "\n",
    "# Load item popularity from recall stage\n",
    "item_popularity = pd.read_parquet(config.DATA_PATH / 'item_popularity.parquet')\n",
    "print(f\"âœ“ Item popularity scores: {len(item_popularity):,}\")\n",
    "\n",
    "# Check what score columns are available in candidates\n",
    "available_scores = [col for col in candidates.columns if 'score' in col.lower()]\n",
    "print(f\"âœ“ Available recall scores in candidates: {available_scores}\")\n",
    "\n",
    "# Item-to-item CF is already in the candidates as 'copurchase_score'\n",
    "# We don't need to load a separate pkl file\n",
    "has_copurchase_score = 'copurchase_score' in candidates.columns\n",
    "if has_copurchase_score:\n",
    "    print(f\"âœ“ Co-purchase scores available in candidates\")\n",
    "else:\n",
    "    print(\"âš ï¸  Co-purchase scores not found in candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdaa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PART 1: USER FEATURES\n",
      "================================================================================\n",
      "\n",
      "âš¡ Found existing user_features.parquet, loading...\n",
      "âœ“ Loaded 18 user features from disk\n",
      "  ðŸ’¾ Memory: 1.42 GB\n"
     ]
    }
   ],
   "source": [
    "# PART 1: USER FEATURES (20-25 features)\n",
    "\n",
    "print_section(\"PART 1: USER FEATURES\")\n",
    "\n",
    "# Check if user features already exist\n",
    "if (config.OUTPUT_PATH / 'user_features.parquet').exists():\n",
    "    print(\"âš¡ Found existing user_features.parquet, loading...\")\n",
    "    user_stats = pd.read_parquet(config.OUTPUT_PATH / 'user_features.parquet')\n",
    "    print(f\"âœ“ Loaded {len(user_stats.columns)-1} user features from disk\")\n",
    "    print_memory()\n",
    "else:\n",
    "    print(\"Computing user-level features...\")\n",
    "\n",
    "    # Basic user statistics\n",
    "    print(\"  [1/5] Basic purchase statistics...\")\n",
    "    user_stats = train_transactions.groupby('customer_id').agg({\n",
    "        'article_id': 'count',  # Total purchases\n",
    "        'price': ['mean', 'std', 'min', 'max'],  # Price statistics\n",
    "        't_dat': ['min', 'max']  # First and last purchase dates\n",
    "    }).reset_index()\n",
    "\n",
    "    user_stats.columns = ['customer_id', 'n_purchases', 'avg_price', 'std_price', \n",
    "                          'min_price', 'max_price', 'first_purchase_date', 'last_purchase_date']\n",
    "\n",
    "    # Calculate days since first/last purchase\n",
    "    user_stats['days_since_first_purchase'] = (\n",
    "        max_date - user_stats['first_purchase_date']\n",
    "    ).dt.days.astype(np.int16)\n",
    "\n",
    "    user_stats['days_since_last_purchase'] = (\n",
    "        max_date - user_stats['last_purchase_date']\n",
    "    ).dt.days.astype(np.int16)\n",
    "\n",
    "    # Purchase frequency\n",
    "    user_stats['purchase_frequency'] = (\n",
    "        user_stats['n_purchases'] / (user_stats['days_since_first_purchase'] + 1)\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Drop date columns (not needed anymore)\n",
    "    user_stats = user_stats.drop(['first_purchase_date', 'last_purchase_date'], axis=1)\n",
    "\n",
    "    print(f\"  - Created {len(user_stats.columns)-1} basic features\")\n",
    "\n",
    "    # Recent activity features\n",
    "    print(\"  [2/5] Recent activity features...\")\n",
    "    recent_cutoff = max_date - timedelta(days=config.RECENT_DAYS)\n",
    "    recent_transactions = train_transactions[train_transactions['t_dat'] >= recent_cutoff]\n",
    "\n",
    "    user_recent_stats = recent_transactions.groupby('customer_id').agg({\n",
    "        'article_id': 'count',\n",
    "        'price': 'mean'\n",
    "    }).reset_index()\n",
    "    user_recent_stats.columns = ['customer_id', 'n_purchases_last_week', 'avg_price_last_week']\n",
    "\n",
    "    # Merge with main stats\n",
    "    user_stats = user_stats.merge(user_recent_stats, on='customer_id', how='left')\n",
    "    user_stats['n_purchases_last_week'] = user_stats['n_purchases_last_week'].fillna(0).astype(np.int16)\n",
    "    user_stats['avg_price_last_week'] = user_stats['avg_price_last_week'].fillna(0).astype(np.float32)\n",
    "\n",
    "    # Is user active recently\n",
    "    user_stats['is_active_last_week'] = (user_stats['n_purchases_last_week'] > 0).astype(np.int8)\n",
    "\n",
    "    del recent_transactions, user_recent_stats\n",
    "    force_garbage_collection()\n",
    "\n",
    "    print(f\"  - Total features so far: {len(user_stats.columns)-1}\")\n",
    "\n",
    "    # Diversity features\n",
    "    print(\"  [3/5] Diversity features...\")\n",
    "    user_diversity = train_transactions.groupby('customer_id').agg({\n",
    "        'article_id': 'nunique',\n",
    "    }).reset_index()\n",
    "    user_diversity.columns = ['customer_id', 'n_unique_articles']\n",
    "\n",
    "    # Category diversity\n",
    "    user_cat_diversity = (\n",
    "        train_transactions\n",
    "        .merge(articles[['article_id', 'product_type_no']], on='article_id')\n",
    "        .groupby('customer_id')['product_type_no']\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "    )\n",
    "    user_cat_diversity.columns = ['customer_id', 'n_unique_categories']\n",
    "\n",
    "    user_stats = user_stats.merge(user_diversity, on='customer_id', how='left')\n",
    "    user_stats = user_stats.merge(user_cat_diversity, on='customer_id', how='left')\n",
    "\n",
    "    user_stats['exploration_rate'] = (\n",
    "        user_stats['n_unique_articles'] / user_stats['n_purchases']\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    del user_diversity, user_cat_diversity\n",
    "    force_garbage_collection()\n",
    "\n",
    "    print(f\"  - Total features so far: {len(user_stats.columns)-1}\")\n",
    "\n",
    "    # Customer demographic features\n",
    "    print(\"  [4/5] Demographic features...\")\n",
    "    customer_features = customers[['customer_id', 'age', 'FN', 'Active']].copy()\n",
    "\n",
    "    # Merge with user stats\n",
    "    user_stats = user_stats.merge(customer_features, on='customer_id', how='left')\n",
    "\n",
    "    # Fill missing values\n",
    "    user_stats['age'] = user_stats['age'].fillna(user_stats['age'].median()).astype(np.float32)\n",
    "    user_stats['FN'] = user_stats['FN'].fillna(0).astype(np.float32)\n",
    "    user_stats['Active'] = user_stats['Active'].fillna(0).astype(np.float32)\n",
    "\n",
    "    del customer_features\n",
    "    force_garbage_collection()\n",
    "\n",
    "    print(f\"  - Total features so far: {len(user_stats.columns)-1}\")\n",
    "\n",
    "    # Purchase trend\n",
    "    print(\"  [5/5] Purchase trend features...\")\n",
    "    # Split into two periods and compare\n",
    "    mid_date = max_date - timedelta(days=config.MEDIUM_DAYS // 2)\n",
    "    old_cutoff = max_date - timedelta(days=config.MEDIUM_DAYS)\n",
    "\n",
    "    recent_period = train_transactions[train_transactions['t_dat'] >= mid_date]\n",
    "    old_period = train_transactions[\n",
    "        (train_transactions['t_dat'] >= old_cutoff) & (train_transactions['t_dat'] < mid_date)\n",
    "    ]\n",
    "\n",
    "    user_recent_count = recent_period.groupby('customer_id').size().reset_index(name='purchases_recent_period')\n",
    "    user_old_count = old_period.groupby('customer_id').size().reset_index(name='purchases_old_period')\n",
    "\n",
    "    user_trend = user_recent_count.merge(user_old_count, on='customer_id', how='outer').fillna(0)\n",
    "    user_trend['purchase_trend'] = (\n",
    "        (user_trend['purchases_recent_period'] - user_trend['purchases_old_period']) / \n",
    "        (user_trend['purchases_old_period'] + 1)\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    user_stats = user_stats.merge(\n",
    "        user_trend[['customer_id', 'purchase_trend']], \n",
    "        on='customer_id', \n",
    "        how='left'\n",
    "    )\n",
    "    user_stats['purchase_trend'] = user_stats['purchase_trend'].fillna(0).astype(np.float32)\n",
    "\n",
    "    del recent_period, old_period, user_recent_count, user_old_count, user_trend\n",
    "    force_garbage_collection()\n",
    "\n",
    "    # Convert to optimal dtypes\n",
    "    for col in user_stats.columns:\n",
    "        if col != 'customer_id':\n",
    "            if user_stats[col].dtype == 'float64':\n",
    "                user_stats[col] = user_stats[col].astype(np.float32)\n",
    "            elif user_stats[col].dtype == 'int64':\n",
    "                user_stats[col] = user_stats[col].astype(np.int32)\n",
    "\n",
    "    print(f\"âœ“ Created {len(user_stats.columns)-1} user features\")\n",
    "    print_memory()\n",
    "\n",
    "    # Save user features for reuse\n",
    "    print(\"\\nSaving user features...\")\n",
    "    user_stats.to_parquet(config.OUTPUT_PATH / 'user_features.parquet', index=False)\n",
    "    print(f\"âœ“ Saved user_features.parquet ({len(user_stats):,} rows, {len(user_stats.columns)-1} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9835b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PART 2: ITEM FEATURES\n",
      "================================================================================\n",
      "\n",
      "âš¡ Found existing item_features.parquet, loading...\n",
      "âœ“ Loaded 19 item features from disk\n",
      "  ðŸ’¾ Memory: 1.42 GB\n",
      "  [1/4] Basic item statistics...\n",
      "  - Created 7 basic features\n",
      "  [2/4] Recent popularity features...\n",
      "  - Total features so far: 9\n",
      "  [3/4] Sales trend features...\n",
      "  - Total features so far: 10\n",
      "  [4/4] Article metadata features...\n",
      "âœ“ Created 19 item features\n",
      "  ðŸ’¾ Memory: 2.07 GB\n",
      "\n",
      "Saving item features...\n",
      "âœ“ Saved item_features.parquet (15,932 rows, 19 features)\n"
     ]
    }
   ],
   "source": [
    "# PART 2: ITEM FEATURES (20-25 features)\n",
    "\n",
    "print_section(\"PART 2: ITEM FEATURES\")\n",
    "\n",
    "# Check if item features already exist\n",
    "if (config.OUTPUT_PATH / 'item_features.parquet').exists():\n",
    "    print(\"âš¡ Found existing item_features.parquet, loading...\")\n",
    "    item_stats = pd.read_parquet(config.OUTPUT_PATH / 'item_features.parquet')\n",
    "    print(f\"âœ“ Loaded {len(item_stats.columns)-1} item features from disk\")\n",
    "    print_memory()\n",
    "else:\n",
    "    print(\"Computing item-level features...\")\n",
    "\n",
    "# Basic item statistics\n",
    "print(\"  [1/4] Basic item statistics...\")\n",
    "item_stats = train_transactions.groupby('article_id').agg({\n",
    "    'customer_id': 'nunique',  # Number of unique buyers\n",
    "    'price': ['mean', 'std'],\n",
    "    't_dat': ['min', 'max', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "item_stats.columns = ['article_id', 'n_unique_buyers', 'avg_price', 'std_price',\n",
    "                      'first_sale_date', 'last_sale_date', 'total_sales']\n",
    "\n",
    "# Days since first/last sale\n",
    "item_stats['days_since_first_sale'] = (\n",
    "    max_date - item_stats['first_sale_date']\n",
    ").dt.days.astype(np.int16)\n",
    "\n",
    "item_stats['days_since_last_sale'] = (\n",
    "    max_date - item_stats['last_sale_date']\n",
    ").dt.days.astype(np.int16)\n",
    "\n",
    "# Sales frequency\n",
    "item_stats['sales_frequency'] = (\n",
    "    item_stats['total_sales'] / (item_stats['days_since_first_sale'] + 1)\n",
    ").astype(np.float32)\n",
    "\n",
    "item_stats = item_stats.drop(['first_sale_date', 'last_sale_date'], axis=1)\n",
    "\n",
    "print(f\"  - Created {len(item_stats.columns)-1} basic features\")\n",
    "\n",
    "# Recent popularity\n",
    "print(\"  [2/4] Recent popularity features...\")\n",
    "recent_cutoff = max_date - timedelta(days=config.RECENT_DAYS)\n",
    "recent_sales = train_transactions[train_transactions['t_dat'] >= recent_cutoff]\n",
    "\n",
    "item_recent_stats = recent_sales.groupby('article_id').agg({\n",
    "    'customer_id': ['count', 'nunique']\n",
    "}).reset_index()\n",
    "item_recent_stats.columns = ['article_id', 'sales_last_week', 'buyers_last_week']\n",
    "\n",
    "item_stats = item_stats.merge(item_recent_stats, on='article_id', how='left')\n",
    "item_stats['sales_last_week'] = item_stats['sales_last_week'].fillna(0).astype(np.int16)\n",
    "item_stats['buyers_last_week'] = item_stats['buyers_last_week'].fillna(0).astype(np.int16)\n",
    "\n",
    "del recent_sales, item_recent_stats\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"  - Total features so far: {len(item_stats.columns)-1}\")\n",
    "\n",
    "# Sales trend\n",
    "print(\"  [3/4] Sales trend features...\")\n",
    "mid_date = max_date - timedelta(days=config.MEDIUM_DAYS // 2)\n",
    "old_cutoff = max_date - timedelta(days=config.MEDIUM_DAYS)\n",
    "\n",
    "recent_period = train_transactions[train_transactions['t_dat'] >= mid_date]\n",
    "old_period = train_transactions[\n",
    "    (train_transactions['t_dat'] >= old_cutoff) & (train_transactions['t_dat'] < mid_date)\n",
    "]\n",
    "\n",
    "item_recent_count = recent_period.groupby('article_id').size().reset_index(name='sales_recent_period')\n",
    "item_old_count = old_period.groupby('article_id').size().reset_index(name='sales_old_period')\n",
    "\n",
    "item_trend = item_recent_count.merge(item_old_count, on='article_id', how='outer').fillna(0)\n",
    "item_trend['sales_trend'] = (\n",
    "    (item_trend['sales_recent_period'] - item_trend['sales_old_period']) / \n",
    "    (item_trend['sales_old_period'] + 1)\n",
    ").astype(np.float32)\n",
    "\n",
    "item_stats = item_stats.merge(\n",
    "    item_trend[['article_id', 'sales_trend']], \n",
    "    on='article_id', \n",
    "    how='left'\n",
    ")\n",
    "item_stats['sales_trend'] = item_stats['sales_trend'].fillna(0).astype(np.float32)\n",
    "\n",
    "del recent_period, old_period, item_recent_count, item_old_count, item_trend\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"  - Total features so far: {len(item_stats.columns)-1}\")\n",
    "\n",
    "# Merge with article metadata\n",
    "print(\"  [4/4] Article metadata features...\")\n",
    "article_features = articles[[\n",
    "    'article_id', 'product_type_no', 'graphical_appearance_no',\n",
    "    'colour_group_code', 'perceived_colour_value_id', \n",
    "    'department_no', 'index_group_no', 'section_no', 'garment_group_no'\n",
    "]].copy()\n",
    "\n",
    "item_stats = item_stats.merge(article_features, on='article_id', how='left')\n",
    "\n",
    "del article_features\n",
    "force_garbage_collection()\n",
    "\n",
    "# Add popularity scores from recall stage\n",
    "item_stats = item_stats.merge(\n",
    "    item_popularity[['article_id', 'popularity_score']], \n",
    "    on='article_id', \n",
    "    how='left'\n",
    ")\n",
    "item_stats['popularity_score'] = item_stats['popularity_score'].fillna(0).astype(np.float32)\n",
    "\n",
    "# Convert to optimal dtypes\n",
    "for col in item_stats.columns:\n",
    "    if col != 'article_id':\n",
    "        if item_stats[col].dtype == 'float64':\n",
    "            item_stats[col] = item_stats[col].astype(np.float32)\n",
    "        elif item_stats[col].dtype == 'int64':\n",
    "            item_stats[col] = item_stats[col].astype(np.int32)\n",
    "\n",
    "print(f\"âœ“ Created {len(item_stats.columns)-1} item features\")\n",
    "print_memory()\n",
    "\n",
    "# Save item features for reuse\n",
    "print(\"\\nSaving item features...\")\n",
    "item_stats.to_parquet(config.OUTPUT_PATH / 'item_features.parquet', index=False)\n",
    "print(f\"âœ“ Saved item_features.parquet ({len(item_stats):,} rows, {len(item_stats.columns)-1} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PART 3: USER-ITEM INTERACTION FEATURES\n",
      "================================================================================\n",
      "\n",
      "Computing interaction features in chunks...\n",
      "  [1/3] Building user purchase history...\n",
      "  [1.5/3] Saving user purchase history...\n",
      "  âœ“ Saved user_purchase_history.parquet\n",
      "Saving additional user statistics...\n",
      "  âœ“ Saved user_top_categories.parquet\n",
      "  âœ“ Saved user_price_stats.parquet\n",
      "  [2/3] Processing candidates in chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499cd1bb40394ee9b528b2cf2dc87d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature chunks:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [3/3] Combining feature chunks...\n",
      "âœ“ Created interaction features for 4,044,442 candidates\n",
      "  ðŸ’¾ Memory: 2.43 GB\n",
      "\n",
      "================================================================================\n",
      "  MERGING ALL FEATURES\n",
      "================================================================================\n",
      "\n",
      "Merging user, item, and interaction features...\n",
      "  âœ“ Merged user features\n",
      "  âœ“ Merged item features\n",
      "Filling missing values...\n",
      "\n",
      "âœ“ Total features: 55 (excluding customer_id, article_id)\n",
      "âœ“ Total candidate-feature pairs: 4,044,442\n",
      "  ðŸ’¾ Memory: 1.84 GB\n"
     ]
    }
   ],
   "source": [
    "# PART 3: USER-ITEM INTERACTION FEATURES (CHUNKED)\n",
    "\n",
    "print_section(\"PART 3: USER-ITEM INTERACTION FEATURES\")\n",
    "\n",
    "print(\"Computing interaction features in chunks...\")\n",
    "\n",
    "# Precompute user purchase history for fast lookup\n",
    "print(\"  [1/3] Building user purchase history...\")\n",
    "user_purchases = (\n",
    "    train_transactions\n",
    "    .groupby('customer_id')['article_id']\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "user_purchase_list = (\n",
    "    train_transactions\n",
    "    .sort_values('t_dat', ascending=False)\n",
    "    .groupby('customer_id')['article_id']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Save user purchase history for reuse\n",
    "print(\"  [1.5/3] Saving user purchase history...\")\n",
    "user_purchase_df = pd.DataFrame([\n",
    "    {'customer_id': user, 'purchased_articles': list(items)}\n",
    "    for user, items in user_purchases.items()\n",
    "])\n",
    "user_purchase_df.to_parquet(config.OUTPUT_PATH / 'user_purchase_history.parquet', index=False)\n",
    "print(f\"  âœ“ Saved user_purchase_history.parquet\")\n",
    "\n",
    "del user_purchase_df\n",
    "force_garbage_collection()\n",
    "\n",
    "# User category preferences\n",
    "user_categories = (\n",
    "    train_transactions\n",
    "    .merge(articles[['article_id', 'product_type_no']], on='article_id')\n",
    "    .groupby(['customer_id', 'product_type_no'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "user_top_category = (\n",
    "    user_categories\n",
    "    .sort_values(['customer_id', 'count'], ascending=[True, False])\n",
    "    .groupby('customer_id')\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    [['customer_id', 'product_type_no']]\n",
    "    .rename(columns={'product_type_no': 'top_category'})\n",
    ")\n",
    "\n",
    "# User price preferences\n",
    "user_price_stats = train_transactions.groupby('customer_id')['price'].agg(['mean', 'std']).reset_index()\n",
    "user_price_stats.columns = ['customer_id', 'user_avg_price', 'user_std_price']\n",
    "\n",
    "# Save additional user stats\n",
    "print(\"Saving additional user statistics...\")\n",
    "user_top_category.to_parquet(config.OUTPUT_PATH / 'user_top_categories.parquet', index=False)\n",
    "user_price_stats.to_parquet(config.OUTPUT_PATH / 'user_price_stats.parquet', index=False)\n",
    "print(f\"  âœ“ Saved user_top_categories.parquet\")\n",
    "print(f\"  âœ“ Saved user_price_stats.parquet\")\n",
    "\n",
    "print(\"  [2/3] Processing candidates in chunks...\")\n",
    "\n",
    "# Split candidates into chunks\n",
    "n_chunks = max(1, len(candidates) // config.CHUNK_SIZE)\n",
    "candidate_chunks = np.array_split(candidates, n_chunks)\n",
    "\n",
    "feature_chunks = []\n",
    "\n",
    "for chunk_idx, chunk in enumerate(tqdm(candidate_chunks, desc=\"Feature chunks\")):\n",
    "    # Start with the chunk\n",
    "    chunk_features = chunk.copy()\n",
    "    \n",
    "    # Has user purchased this exact item before?\n",
    "    chunk_features['has_purchased_item'] = chunk_features.apply(\n",
    "        lambda row: 1 if row['article_id'] in user_purchases.get(row['customer_id'], set()) else 0,\n",
    "        axis=1\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    # If purchased before, get days since last purchase\n",
    "    def days_since_purchase(row):\n",
    "        user_items = user_purchase_list.get(row['customer_id'], [])\n",
    "        if row['article_id'] in user_items:\n",
    "            # Get position of first occurrence (most recent due to sort)\n",
    "            try:\n",
    "                idx = user_items.index(row['article_id'])\n",
    "                # Approximate days (assuming 1 purchase per day on average)\n",
    "                return min(idx, 365)\n",
    "            except:\n",
    "                return 365\n",
    "        return 365\n",
    "    \n",
    "    chunk_features['days_since_item_purchase'] = chunk_features.apply(\n",
    "        days_since_purchase, axis=1\n",
    "    ).astype(np.int16)\n",
    "    \n",
    "    # Merge with item stats to get item metadata\n",
    "    chunk_features = chunk_features.merge(\n",
    "        item_stats[['article_id', 'product_type_no', 'avg_price', 'popularity_score']], \n",
    "        on='article_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Has user purchased items from this category?\n",
    "    chunk_features = chunk_features.merge(user_top_category, on='customer_id', how='left')\n",
    "    chunk_features['category_match'] = (\n",
    "        chunk_features['product_type_no'] == chunk_features['top_category']\n",
    "    ).astype(np.int8)\n",
    "    chunk_features = chunk_features.drop(['product_type_no', 'top_category'], axis=1)\n",
    "    \n",
    "    # Price match features\n",
    "    chunk_features = chunk_features.merge(user_price_stats, on='customer_id', how='left')\n",
    "    chunk_features['price_vs_user_avg'] = (\n",
    "        (chunk_features['avg_price'] - chunk_features['user_avg_price']) / \n",
    "        (chunk_features['user_std_price'] + 0.01)\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    chunk_features['is_cheaper_than_usual'] = (\n",
    "        chunk_features['avg_price'] < chunk_features['user_avg_price']\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    chunk_features = chunk_features.drop(['user_avg_price', 'user_std_price', 'avg_price'], axis=1)\n",
    "    \n",
    "    # Co-purchase score is already in candidates, but create derived features\n",
    "    if 'copurchase_score' in chunk_features.columns:\n",
    "        # Normalize copurchase score by user's max copurchase score\n",
    "        user_max_copurchase = chunk_features.groupby('customer_id')['copurchase_score'].transform('max')\n",
    "        chunk_features['copurchase_score_normalized'] = (\n",
    "            chunk_features['copurchase_score'] / (user_max_copurchase + 0.001)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Binary: has any copurchase signal\n",
    "        chunk_features['has_copurchase_signal'] = (\n",
    "            chunk_features['copurchase_score'] > 0\n",
    "        ).astype(np.int8)\n",
    "    \n",
    "    # Recall strategy coverage (how many strategies recommended this item)\n",
    "    # Already in candidates as 'n_strategies'\n",
    "    \n",
    "    # Rank features (rank within each recall strategy)\n",
    "    for score_col in ['repurchase_score', 'popularity_score', 'copurchase_score', \n",
    "                      'userknn_score', 'category_score']:\n",
    "        if score_col in chunk_features.columns:\n",
    "            chunk_features[f'{score_col}_rank'] = (\n",
    "                chunk_features.groupby('customer_id')[score_col]\n",
    "                .rank(method='dense', ascending=False)\n",
    "                .astype(np.int16)\n",
    "            )\n",
    "    \n",
    "    # Overall candidate rank (by combined score if available)\n",
    "    if 'n_strategies' in chunk_features.columns:\n",
    "        chunk_features['overall_rank'] = (\n",
    "            chunk_features.groupby('customer_id')['n_strategies']\n",
    "            .rank(method='dense', ascending=False)\n",
    "            .astype(np.int16)\n",
    "        )\n",
    "    \n",
    "    # Clean up\n",
    "    chunk_features = chunk_features.fillna(0)\n",
    "    \n",
    "    feature_chunks.append(chunk_features)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk_features\n",
    "    if chunk_idx % 10 == 0:\n",
    "        force_garbage_collection()\n",
    "\n",
    "print(\"  [3/3] Combining feature chunks...\")\n",
    "all_features = pd.concat(feature_chunks, ignore_index=True)\n",
    "del feature_chunks\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"âœ“ Created interaction features for {len(all_features):,} candidates\")\n",
    "print_memory()\n",
    "\n",
    "# MERGE ALL FEATURES\n",
    "\n",
    "print_section(\"MERGING ALL FEATURES\")\n",
    "\n",
    "print(\"Merging user, item, and interaction features...\")\n",
    "\n",
    "# Merge user features\n",
    "all_features = all_features.merge(user_stats, on='customer_id', how='left')\n",
    "print(f\"  âœ“ Merged user features\")\n",
    "\n",
    "# Merge item features (already partially merged, merge remaining)\n",
    "remaining_item_cols = [col for col in item_stats.columns if col not in all_features.columns]\n",
    "remaining_item_cols.append('article_id')\n",
    "all_features = all_features.merge(item_stats[remaining_item_cols], on='article_id', how='left')\n",
    "print(f\"  âœ“ Merged item features\")\n",
    "\n",
    "# Fill any remaining NaNs (handle categorical columns separately)\n",
    "print(\"Filling missing values...\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = all_features.select_dtypes(include=['category']).columns.tolist()\n",
    "numerical_cols = all_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Fill numerical columns with 0\n",
    "if numerical_cols:\n",
    "    all_features[numerical_cols] = all_features[numerical_cols].fillna(0)\n",
    "\n",
    "# Fill categorical columns with their mode or a default value\n",
    "for col in categorical_cols:\n",
    "    if all_features[col].isna().any():\n",
    "        # Get the most frequent category\n",
    "        mode_value = all_features[col].mode()\n",
    "        if len(mode_value) > 0:\n",
    "            all_features[col] = all_features[col].fillna(mode_value[0])\n",
    "        else:\n",
    "            # If no mode, convert to string and fill with 'unknown'\n",
    "            all_features[col] = all_features[col].astype(str).fillna('unknown')\n",
    "\n",
    "print(f\"\\nâœ“ Total features: {len(all_features.columns) - 2} (excluding customer_id, article_id)\")\n",
    "print(f\"âœ“ Total candidate-feature pairs: {len(all_features):,}\")\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e1f090f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PART 4: TEXT-BASED SEMANTIC FEATURES\n",
      "================================================================================\n",
      "\n",
      "Found saved text embeddings, integrating text features...\n",
      "\n",
      "================================================================================\n",
      "  STAGE 3 ENHANCEMENT: TEXT-BASED FEATURES\n",
      "================================================================================\n",
      "\n",
      "Loading saved embeddings...\n",
      "  âœ“ Loaded 16,616 article embeddings\n",
      "  âœ“ Loaded 47,543 user embeddings\n",
      "\n",
      "Enhancing features with text semantics...\n",
      "\n",
      "Creating category encoding features...\n",
      "  Processing 7 categorical columns...\n",
      "  âœ“ Created 14 category encoding features\n",
      "\n",
      "Computing semantic diversity features...\n",
      "  âœ“ Computed diversity for 47,543 users\n",
      "\n",
      "  Merging category features...\n",
      "  Merging semantic diversity features...\n",
      "  Computing user-item text similarities in chunks...\n",
      "    Processing chunk 1/80...\n",
      "    Processing chunk 11/80...\n",
      "    Processing chunk 21/80...\n",
      "    Processing chunk 31/80...\n",
      "    Processing chunk 41/80...\n",
      "    Processing chunk 51/80...\n",
      "    Processing chunk 61/80...\n",
      "    Processing chunk 71/80...\n",
      "  âœ“ Text enhancement complete\n",
      "\n",
      "âœ“ Enhanced features with text semantics\n",
      "  Total features now: 72\n",
      "  ðŸ’¾ Memory: 3.28 GB\n"
     ]
    }
   ],
   "source": [
    "print_section(\"PART 4: TEXT-BASED SEMANTIC FEATURES\")\n",
    "\n",
    "# Import the text feature module\n",
    "from text_features import integrate_text_features_stage3\n",
    "\n",
    "# Check if embeddings exist\n",
    "embeddings_path = config.DATA_PATH  # Or wherever you ran Stage 2\n",
    "\n",
    "if (embeddings_path / 'article_embeddings.pkl').exists():\n",
    "    print(\"Found saved text embeddings, integrating text features...\")\n",
    "    \n",
    "    all_features = integrate_text_features_stage3(\n",
    "        all_features=all_features,\n",
    "        articles=articles,\n",
    "        train_transactions=train_transactions,\n",
    "        embeddings_path=embeddings_path\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Enhanced features with text semantics\")\n",
    "    print(f\"  Total features now: {len(all_features.columns) - 2}\")\n",
    "    print_memory()\n",
    "else:\n",
    "    print(\"âš ï¸  Text embeddings not found, skipping text features\")\n",
    "    print(\"   Run Stage 2 with text feature integration first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1835012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  SAVING FEATURES\n",
      "================================================================================\n",
      "\n",
      "Saving feature matrix...\n",
      "  âœ“ Text semantic features detected in feature matrix\n",
      "âœ“ Saved training_features.parquet (288.07 MB)\n",
      "âœ“ Saved feature_names.txt (72 features)\n",
      "âœ“ Saved feature_metadata.json\n",
      "\n",
      "================================================================================\n",
      "FEATURE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total features: 72\n",
      "\n",
      "Feature breakdown:\n",
      "  - User features: 23\n",
      "  - Item features: 24\n",
      "  - Interaction features: 10\n",
      "  - Text semantic features: 4 âœ“\n",
      "\n",
      "  Text features included:\n",
      "    â€¢ text_similarity_score\n",
      "    â€¢ semantic_diversity\n",
      "    â€¢ semantic_range\n",
      "    â€¢ user_item_text_similarity\n",
      "\n",
      "  Sample user features:\n",
      "    â€¢ repurchase_score\n",
      "    â€¢ copurchase_score\n",
      "    â€¢ userknn_score\n",
      "    â€¢ has_purchased_item\n",
      "    â€¢ days_since_item_purchase\n",
      "\n",
      "  Sample item features:\n",
      "    â€¢ n_unique_articles\n",
      "    â€¢ total_sales\n",
      "    â€¢ sales_frequency\n",
      "    â€¢ sales_last_week\n",
      "    â€¢ product_type_no\n",
      "\n",
      "  Sample interaction features:\n",
      "    â€¢ popularity_score_x\n",
      "    â€¢ category_score\n",
      "    â€¢ text_similarity_score\n",
      "    â€¢ n_strategies\n",
      "    â€¢ popularity_score_y\n",
      "\n",
      "================================================================================\n",
      "SAVED FILES\n",
      "================================================================================\n",
      "\n",
      "Intermediate feature files (for reuse):\n",
      "  âœ“ user_features.parquet - User-level features\n",
      "  âœ“ item_features.parquet - Item-level features\n",
      "  âœ“ user_purchase_history.parquet - User purchase history\n",
      "  âœ“ user_top_categories.parquet - User category preferences\n",
      "  âœ“ user_price_stats.parquet - User price statistics\n",
      "\n",
      "Final output files:\n",
      "  âœ“ training_features.parquet - Complete feature matrix for training\n",
      "  âœ“ feature_names.txt - List of all feature names\n",
      "  âœ“ feature_metadata.json - Feature metadata and info\n",
      "\n",
      "================================================================================\n",
      "âœ… STAGE 3 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Validation checks:\n",
      "  âœ“ No NaN values: True\n",
      "  âœ“ Total rows: 4,044,442\n",
      "  âœ“ Total features: 72\n",
      "  âœ“ Memory usage: 1546.70 MB\n",
      "  âœ“ Text features integrated: YES âœ¨\n",
      "    - 4 text-based features added\n",
      "\n",
      "Next steps:\n",
      "  1. Review training_features.parquet\n",
      "  2. Check feature_metadata.json for complete feature info\n",
      "  3. Proceed to Stage 4: Model Training\n",
      "\n",
      "ðŸ’¡ TIP: Delete intermediate files (user_features.parquet, etc.) to force\n",
      "   recomputation if you change feature engineering logic.\n",
      "\n",
      "ðŸŽ‰ SUCCESS: Text semantic features successfully integrated!\n",
      "   Your model now has access to product descriptions, colors,\n",
      "   departments, and semantic similarity features!\n"
     ]
    }
   ],
   "source": [
    "print_section(\"SAVING FEATURES\")\n",
    "\n",
    "print(\"Saving feature matrix...\")\n",
    "\n",
    "# IMPORTANT: Check if text features were added\n",
    "text_features_added = any('text_similarity' in col or 'semantic' in col \n",
    "                          for col in all_features.columns)\n",
    "\n",
    "if text_features_added:\n",
    "    print(\"  âœ“ Text semantic features detected in feature matrix\")\n",
    "else:\n",
    "    print(\"  âš ï¸  No text semantic features detected\")\n",
    "\n",
    "# Save the complete feature matrix\n",
    "all_features.to_parquet(config.OUTPUT_PATH / 'training_features.parquet', index=False)\n",
    "\n",
    "file_size = (config.OUTPUT_PATH / 'training_features.parquet').stat().st_size / 1024**2\n",
    "print(f\"âœ“ Saved training_features.parquet ({file_size:.2f} MB)\")\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_names = [col for col in all_features.columns \n",
    "                 if col not in ['customer_id', 'article_id']]\n",
    "\n",
    "with open(config.OUTPUT_PATH / 'feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(feature_names))\n",
    "\n",
    "print(f\"âœ“ Saved feature_names.txt ({len(feature_names)} features)\")\n",
    "\n",
    "# NEW: Save feature metadata including text features info\n",
    "feature_metadata = {\n",
    "    'total_features': len(feature_names),\n",
    "    'has_text_features': text_features_added,\n",
    "    'feature_list': feature_names,\n",
    "    'timestamp': str(datetime.now())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(config.OUTPUT_PATH / 'feature_metadata.json', 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "print(f\"âœ“ Saved feature_metadata.json\")\n",
    "\n",
    "# Print feature summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal features: {len(feature_names)}\")\n",
    "\n",
    "# Categorize features\n",
    "user_features = [f for f in feature_names if any(x in f.lower() for x in [\n",
    "    'user', 'customer', 'purchase', 'age', 'active', 'fn', 'trend'\n",
    "])]\n",
    "\n",
    "item_features = [f for f in feature_names if any(x in f.lower() for x in [\n",
    "    'item', 'article', 'sales', 'product', 'colour', 'color', \n",
    "    'department', 'section', 'garment', 'frequency', 'count'\n",
    "]) and f not in user_features]\n",
    "\n",
    "interaction_features = [f for f in feature_names if any(x in f.lower() for x in [\n",
    "    'score', 'rank', 'strategies', 'match', 'purchased', 'category_match',\n",
    "    'price_vs', 'cheaper'\n",
    "]) and f not in user_features and f not in item_features]\n",
    "\n",
    "# NEW: Identify text features\n",
    "text_features = [f for f in feature_names if any(x in f.lower() for x in [\n",
    "    'text_similarity', 'semantic', 'embedding'\n",
    "])]\n",
    "\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - User features: {len(user_features)}\")\n",
    "print(f\"  - Item features: {len(item_features)}\")\n",
    "print(f\"  - Interaction features: {len(interaction_features)}\")\n",
    "\n",
    "# NEW: Show text features if present\n",
    "if text_features:\n",
    "    print(f\"  - Text semantic features: {len(text_features)} âœ“\")\n",
    "    print(f\"\\n  Text features included:\")\n",
    "    for feat in text_features:\n",
    "        print(f\"    â€¢ {feat}\")\n",
    "else:\n",
    "    print(f\"  - Text semantic features: 0 (not integrated)\")\n",
    "\n",
    "# NEW: Show sample of other feature categories\n",
    "print(f\"\\n  Sample user features:\")\n",
    "for feat in user_features[:5]:\n",
    "    print(f\"    â€¢ {feat}\")\n",
    "\n",
    "print(f\"\\n  Sample item features:\")\n",
    "for feat in item_features[:5]:\n",
    "    print(f\"    â€¢ {feat}\")\n",
    "\n",
    "print(f\"\\n  Sample interaction features:\")\n",
    "for feat in interaction_features[:5]:\n",
    "    print(f\"    â€¢ {feat}\")\n",
    "\n",
    "print(\"\\n SAVED FILES \\n\")\n",
    "print(\"\\nIntermediate feature files (for reuse):\")\n",
    "print(\"user_features.parquet - User-level features\")\n",
    "print(\"item_features.parquet - Item-level features\")\n",
    "print(\"user_purchase_history.parquet - User purchase history\")\n",
    "print(\"user_top_categories.parquet - User category preferences\")\n",
    "print(\"user_price_stats.parquet - User price statistics\")\n",
    "\n",
    "# NEW: Check for text-related files\n",
    "if (config.OUTPUT_PATH / 'article_embeddings.pkl').exists():\n",
    "    print(\"article_embeddings.pkl - Article text embeddings\")\n",
    "if (config.OUTPUT_PATH / 'user_embeddings.pkl').exists():\n",
    "    print(\"user_embeddings.pkl - User preference embeddings\")\n",
    "\n",
    "print(\"\\nFinal output files:\")\n",
    "print(\"training_features.parquet - Complete feature matrix for training\")\n",
    "print(\"feature_names.txt - List of all feature names\")\n",
    "print(\"feature_metadata.json - Feature metadata and info\")\n",
    "\n",
    "print(\"\\n STAGE 3 COMPLETE! \\n\")\n",
    "\n",
    "# NEW: Validation checks\n",
    "print(\"\\nValidation checks:\")\n",
    "print(f\"No NaN values: {all_features.isnull().sum().sum() == 0}\")\n",
    "print(f\"Total rows: {len(all_features):,}\")\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Memory usage: {all_features.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "if text_features_added:\n",
    "    print(f\"Text features integrated: YES âœ¨\")\n",
    "    print(f\"{len(text_features)} text-based features added\")\n",
    "else:\n",
    "    print(f\"Text features not integrated\")\n",
    "    print(f\"Run Stage 2 with text integration first\")\n",
    "    print(f\"Or check if embeddings exist in {config.DATA_PATH}\")\n",
    "\n",
    "if not text_features_added:\n",
    "    print(\"Consider integrating text features for better performance\")\n",
    "    print(\"(See text_feature_integration guide)\")\n",
    "else:\n",
    "    print(\"Ready for Model Training\")\n",
    "\n",
    "if text_features_added:\n",
    "    print(\"\\n Text semantic features successfully integrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84802a24",
   "metadata": {},
   "source": [
    "### Getting Image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c359e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PyTorch available\n",
      "  PyTorch version: 2.9.1\n",
      "âœ“ MPS (Metal Performance Shaders) available - Apple Silicon GPU acceleration enabled!\n",
      "âœ“ CLIP/FashionCLIP available\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from torchvision import models, transforms\n",
    "    from PIL import Image\n",
    "    HAS_TORCH = True\n",
    "    print(\"âœ“ PyTorch available\")\n",
    "    print(f\"  PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check for MPS (Metal Performance Shaders) support\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"âœ“ MPS (Metal Performance Shaders) available - Apple Silicon GPU acceleration enabled!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  MPS not available - will use CPU\")\n",
    "        \n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"âš ï¸  PyTorch not available. Install with: pip install torch torchvision pillow\")\n",
    "\n",
    "try:\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    HAS_CLIP = True\n",
    "    print(\"âœ“ CLIP/FashionCLIP available\")\n",
    "except ImportError:\n",
    "    HAS_CLIP = False\n",
    "    print(\"âš ï¸  CLIP/FashionCLIP not available. Install with: pip install transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY MONITORING\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    print(f\"  ðŸ’¾ Memory: {mem:.2f} GB\")\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Aggressive garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()  # Clear MPS cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "class Config:\n",
    "    # Paths - UPDATE THESE FOR YOUR LOCAL SETUP\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')  # Where your parquet files are\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')  # Where to save outputs\n",
    "    IMAGE_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/h-and-m-personalized-fashion-recommendations/images')  # H&M image directory\n",
    "    \n",
    "    # Image embedding options\n",
    "    EMBEDDING_METHOD = 'fashion-clip'  # Options: 'fashion-clip', 'resnet50', 'clip', 'efficientnet'\n",
    "    EMBEDDING_DIM = 512  # Output dimension (will be projected from original)\n",
    "    \n",
    "    # Processing - Optimized for Apple Silicon\n",
    "    BATCH_SIZE = 64  # M4 can handle larger batches efficiently\n",
    "    IMAGE_SIZE = 224  # Input size for models\n",
    "    \n",
    "    # Memory optimization\n",
    "    PROCESS_SUBSET = False  # Set True to process only subset (for testing)\n",
    "    SUBSET_SIZE = 10000  # Number of images to process if PROCESS_SUBSET=True\n",
    "    USE_FP16 = False  # MPS doesn't fully support FP16 yet, keep False\n",
    "    \n",
    "    # Apple Silicon specific\n",
    "    USE_MPS = True  # Enable MPS acceleration\n",
    "    NUM_WORKERS = 4  # For data loading (M4 has excellent multi-core)\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "config.OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device for Apple Silicon\"\"\"\n",
    "    if config.USE_MPS and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE EMBEDDING EXTRACTION\n",
    "\n",
    "class ImageEmbeddingExtractor:\n",
    "    \"\"\"Extract embeddings from product images using pre-trained models\"\"\"\n",
    "    \n",
    "    def __init__(self, method='fashion-clip', device=None):\n",
    "        self.method = method\n",
    "        self.device = device if device else get_device()\n",
    "        self.model = None\n",
    "        self.transform = None\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        if method == 'fashion-clip':\n",
    "            self._load_fashion_clip()\n",
    "        elif method == 'resnet50':\n",
    "            self._load_resnet()\n",
    "        elif method == 'clip':\n",
    "            self._load_clip()\n",
    "        elif method == 'efficientnet':\n",
    "            self._load_efficientnet()\n",
    "    \n",
    "    def _load_fashion_clip(self):\n",
    "        \"\"\"Load pre-trained FashionCLIP - optimized for fashion domain\"\"\"\n",
    "        print(\"Loading FashionCLIP\")\n",
    "        \n",
    "        # Load FashionCLIP 2.0 - uses better base model\n",
    "        self.model = CLIPModel.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.output_dim = 512  # FashionCLIP outputs 512-dim embeddings\n",
    "        print(f\"âœ“ FashionCLIP 2.0 loaded on {self.device}, output dim: {self.output_dim}\")\n",
    "        print(\"  Model details: ViT-B/32 architecture, trained on Farfetch dataset\")\n",
    "        print(\"  Benefits: Better fashion understanding, semantic similarity, zero-shot capabilities\")\n",
    "    \n",
    "    def _load_resnet(self):\n",
    "        \"\"\"Load pre-trained ResNet50 - optimized for Apple Silicon\"\"\"\n",
    "        print(\"Loading ResNet50 (pre-trained on ImageNet)...\")\n",
    "        \n",
    "        # Load model with updated weights parameter (new PyTorch API)\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove final classification layer\n",
    "        self.model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Move to MPS device\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Image preprocessing - using updated normalize values from weights\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(config.IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.output_dim = 2048\n",
    "        print(f\"âœ“ ResNet50 loaded on {self.device}, output dim: {self.output_dim}\")\n",
    "    \n",
    "    def _load_efficientnet(self):\n",
    "        \"\"\"Load pre-trained EfficientNet-B0 - optimized for Apple Silicon\"\"\"\n",
    "        print(\"Loading EfficientNet-B0 (pre-trained on ImageNet)...\")\n",
    "        \n",
    "        # Load with updated weights parameter\n",
    "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove final classification layer\n",
    "        self.model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(config.IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.output_dim = 1280\n",
    "        print(f\"âœ“ EfficientNet-B0 loaded on {self.device}, output dim: {self.output_dim}\")\n",
    "    \n",
    "    def _load_clip(self):\n",
    "        \"\"\"Load CLIP model - optimized for Apple Silicon\"\"\"\n",
    "        print(\"Loading CLIP (vision-language model)...\")\n",
    "        \n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.output_dim = 512\n",
    "        print(f\"âœ“ CLIP loaded on {self.device}, output dim: {self.output_dim}\")\n",
    "    \n",
    "    def extract_single(self, image_path):\n",
    "        \"\"\"Extract embedding from a single image\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            if self.method in ['clip', 'fashion-clip']:\n",
    "                inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    image_features = self.model.get_image_features(**inputs)\n",
    "                \n",
    "                embedding = image_features.squeeze().cpu().numpy()\n",
    "            else:\n",
    "                img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    embedding = self.model(img_tensor).squeeze().cpu().numpy()\n",
    "            \n",
    "            return embedding\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Return zero vector if image processing fails\n",
    "            print(f\"âš ï¸  Error processing {image_path}: {e}\")\n",
    "            return np.zeros(self.output_dim, dtype=np.float32)\n",
    "    \n",
    "    def extract_batch(self, image_paths):\n",
    "        \"\"\"Extract embeddings from a batch of images - optimized for MPS\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        # For CLIP-based models (fashion-clip and clip), process individually\n",
    "        # because batch processing with processor is more complex\n",
    "        if self.method in ['clip', 'fashion-clip']:\n",
    "            for img_path in image_paths:\n",
    "                emb = self.extract_single(img_path)\n",
    "                embeddings.append(emb)\n",
    "            return np.array(embeddings)\n",
    "        \n",
    "        # For CNN models (ResNet, EfficientNet), process as batch\n",
    "        batch_tensors = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        # Load and preprocess all images in batch\n",
    "        for idx, img_path in enumerate(image_paths):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = self.transform(img)\n",
    "                batch_tensors.append(img_tensor)\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Error loading {img_path}: {e}\")\n",
    "                embeddings.append(np.zeros(self.output_dim, dtype=np.float32))\n",
    "        \n",
    "        # Process batch\n",
    "        if batch_tensors:\n",
    "            batch = torch.stack(batch_tensors).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = self.model(batch).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Handle single vs multiple embeddings\n",
    "            if len(batch_tensors) == 1:\n",
    "                batch_embeddings = batch_embeddings.reshape(1, -1)\n",
    "            \n",
    "            # Insert embeddings at correct positions\n",
    "            result_embeddings = []\n",
    "            batch_idx = 0\n",
    "            for idx in range(len(image_paths)):\n",
    "                if idx in valid_indices:\n",
    "                    result_embeddings.append(batch_embeddings[batch_idx])\n",
    "                    batch_idx += 1\n",
    "                else:\n",
    "                    result_embeddings.append(np.zeros(self.output_dim, dtype=np.float32))\n",
    "            \n",
    "            return np.array(result_embeddings)\n",
    "        \n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48909672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN EXTRACTION PIPELINE\n",
    "\n",
    "print_section(\"STAGE 1.5: IMAGE EMBEDDING EXTRACTION (APPLE SILICON OPTIMIZED)\")\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if (config.OUTPUT_PATH / 'image_embeddings_3.parquet').exists():\n",
    "    print(\"âš¡ Found existing image_embeddings_3.parquet!\")\n",
    "    print(\"   Delete this file if you want to re-extract embeddings.\")\n",
    "    \n",
    "    # Load existing embeddings\n",
    "    image_embeddings_df = pd.read_parquet(config.OUTPUT_PATH / 'image_embeddings_3.parquet')\n",
    "    print(f\"âœ“ Loaded {len(image_embeddings_df):,} image embeddings from disk\")\n",
    "    \n",
    "else:\n",
    "    print(\"Image embeddings not found. Starting extraction...\")\n",
    "    \n",
    "    # Check prerequisites\n",
    "    if not HAS_TORCH:\n",
    "        raise ImportError(\"PyTorch is required. Install with: pip install torch torchvision pillow\")\n",
    "    \n",
    "    # Load articles\n",
    "    print(\"\\nLoading articles metadata...\")\n",
    "    articles = pd.read_parquet(config.DATA_PATH / 'articles.parquet')\n",
    "    print(f\"âœ“ Loaded {len(articles):,} articles\")\n",
    "    \n",
    "    # Determine which articles to process\n",
    "    if config.PROCESS_SUBSET:\n",
    "        articles = articles.head(config.SUBSET_SIZE)\n",
    "        print(f\"âš ï¸  Processing subset of {len(articles):,} articles (PROCESS_SUBSET=True)\")\n",
    "    \n",
    "    article_ids = articles['article_id'].tolist()\n",
    "    \n",
    "    # Check image directory structure\n",
    "    print(f\"\\nChecking image directory: {config.IMAGE_PATH}\")\n",
    "    if not config.IMAGE_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Image directory not found: {config.IMAGE_PATH}\")\n",
    "    \n",
    "    # H&M images are organized as: images/0XX/0XXXXXXXX.jpg\n",
    "    # Where first 3 digits determine subfolder\n",
    "    \n",
    "    # Find available images\n",
    "    print(\"Scanning for available images...\")\n",
    "    available_images = {}\n",
    "    missing_count = 0\n",
    "    \n",
    "    for article_id in tqdm(article_ids, desc=\"Checking images\"):\n",
    "        # Try different possible paths\n",
    "        article_str = str(article_id).zfill(10)  # Pad to 10 digits\n",
    "        subfolder = article_str[:3]\n",
    "        \n",
    "        # Possible image paths\n",
    "        possible_paths = [\n",
    "            config.IMAGE_PATH / subfolder / f\"{article_str}.jpg\",\n",
    "            config.IMAGE_PATH / f\"{article_str}.jpg\",\n",
    "            config.IMAGE_PATH / f\"{article_id}.jpg\",\n",
    "        ]\n",
    "        \n",
    "        image_found = False\n",
    "        for img_path in possible_paths:\n",
    "            if img_path.exists():\n",
    "                available_images[article_id] = img_path\n",
    "                image_found = True\n",
    "                break\n",
    "        \n",
    "        if not image_found:\n",
    "            missing_count += 1\n",
    "    \n",
    "    print(f\"\\nâœ“ Found {len(available_images):,} images\")\n",
    "    print(f\"  Missing {missing_count:,} images ({100*missing_count/len(article_ids):.1f}%)\")\n",
    "    \n",
    "    # Initialize extractor\n",
    "    print(f\"\\nInitializing {config.EMBEDDING_METHOD} model...\")\n",
    "    extractor = ImageEmbeddingExtractor(method=config.EMBEDDING_METHOD)\n",
    "    print_memory()\n",
    "    \n",
    "    # Extract embeddings\n",
    "    print(f\"\\nExtracting embeddings for {len(available_images):,} images...\")\n",
    "    print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"  Device: {extractor.device}\")\n",
    "    estimated_time = len(available_images) / (config.BATCH_SIZE * 10)  # ~10 batches/sec on M4\n",
    "    print(f\"  Estimated time: {estimated_time:.1f} minutes\")\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    batch_article_ids = []\n",
    "    batch_image_paths = []\n",
    "    \n",
    "    for article_id, img_path in tqdm(available_images.items(), desc=\"Extracting embeddings\"):\n",
    "        batch_article_ids.append(article_id)\n",
    "        batch_image_paths.append(img_path)\n",
    "        \n",
    "        # Process batch\n",
    "        if len(batch_article_ids) >= config.BATCH_SIZE:\n",
    "            batch_embeddings = extractor.extract_batch(batch_image_paths)\n",
    "            \n",
    "            for aid, emb in zip(batch_article_ids, batch_embeddings):\n",
    "                embeddings_dict[aid] = emb\n",
    "            \n",
    "            # Clear batch\n",
    "            batch_article_ids = []\n",
    "            batch_image_paths = []\n",
    "            \n",
    "            # Periodic garbage collection\n",
    "            if len(embeddings_dict) % (config.BATCH_SIZE * 10) == 0:\n",
    "                force_garbage_collection()\n",
    "    \n",
    "    # Process remaining batch\n",
    "    if batch_article_ids:\n",
    "        batch_embeddings = extractor.extract_batch(batch_image_paths)\n",
    "        for aid, emb in zip(batch_article_ids, batch_embeddings):\n",
    "            embeddings_dict[aid] = emb\n",
    "    \n",
    "    print(f\"\\nâœ“ Extracted {len(embeddings_dict):,} embeddings\")\n",
    "    print_memory()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    print(\"\\nCreating embeddings DataFrame...\")\n",
    "    \n",
    "    # For missing images, use mean embedding or zero vector\n",
    "    mean_embedding = np.mean(list(embeddings_dict.values()), axis=0) if embeddings_dict else np.zeros(extractor.output_dim)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    for article_id in article_ids:\n",
    "        if article_id in embeddings_dict:\n",
    "            emb = embeddings_dict[article_id]\n",
    "        else:\n",
    "            emb = mean_embedding  # Use mean for missing images\n",
    "        \n",
    "        all_embeddings.append(emb)\n",
    "    \n",
    "    # Create DataFrame with article_id and embedding columns\n",
    "    image_embeddings_df = pd.DataFrame({\n",
    "        'article_id': article_ids\n",
    "    })\n",
    "    \n",
    "    # Add embedding dimensions as separate columns\n",
    "    embedding_matrix = np.array(all_embeddings)\n",
    "    \n",
    "    # Project to target dimension if needed\n",
    "    if embedding_matrix.shape[1] != config.EMBEDDING_DIM:\n",
    "        print(f\"\\nProjecting embeddings from {embedding_matrix.shape[1]} to {config.EMBEDDING_DIM} dimensions...\")\n",
    "        \n",
    "        pca = PCA(n_components=config.EMBEDDING_DIM, random_state=config.RANDOM_STATE)\n",
    "        embedding_matrix = pca.fit_transform(embedding_matrix)\n",
    "        \n",
    "        print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    # Add embedding columns\n",
    "    for i in range(embedding_matrix.shape[1]):\n",
    "        image_embeddings_df[f'image_emb_{i}'] = embedding_matrix[:, i].astype(np.float32)\n",
    "    \n",
    "    # Save embeddings\n",
    "    print(\"\\nSaving image embeddings...\")\n",
    "    image_embeddings_df.to_parquet(config.OUTPUT_PATH / 'image_embeddings_3.parquet', index=False)\n",
    "    \n",
    "    file_size = (config.OUTPUT_PATH / 'image_embeddings_3.parquet').stat().st_size / 1024**2\n",
    "    print(f\"âœ“ Saved image_embeddings.parquet ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # Clean up\n",
    "    del extractor, embeddings_dict, embedding_matrix\n",
    "    force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb82906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  INTEGRATING IMAGE EMBEDDINGS INTO TRAINING FEATURES\n",
      "================================================================================\n",
      "\n",
      "Loading training_features.parquet...\n",
      "âœ“ Loaded training features: 4,044,442 rows\n",
      "  Current features: 72\n",
      "  ðŸ’¾ Memory: 2.13 GB\n",
      "\n",
      "Merging image embeddings with training features...\n",
      "  Image embeddings: 16,616 articles\n",
      "âœ“ Merged successfully\n",
      "  New features: 584\n",
      "\n",
      "Optimizing data types...\n",
      "  ðŸ’¾ Memory: 0.60 GB\n",
      "\n",
      "Saving updated training_features.parquet...\n",
      "âœ“ Saved training_features.parquet (5126.99 MB)\n",
      "âœ“ Updated feature_names.txt (584 features)\n",
      "\n",
      "================================================================================\n",
      "  IMAGE EMBEDDING INTEGRATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "  ðŸ“¸ Image embeddings extracted: 16,616\n",
      "  ðŸ“Š Image embedding dimensions: 512\n",
      "  ðŸŽ¯ Total features: 584\n",
      "     - User features: 531\n",
      "     - Item features: 12\n",
      "     - Interaction features: 16\n",
      "     - Image features: 512\n",
      "\n",
      "Files created/updated:\n",
      "  âœ“ image_embeddings.parquet - Image embeddings for all articles\n",
      "  âœ“ training_features.parquet - Updated with image embeddings\n",
      "  âœ“ feature_names.txt - Updated feature list\n",
      "\n",
      "================================================================================\n",
      "âœ… Ready for Stage 4: Model Training with Image Features!\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ Apple Silicon Optimization Tips:\n",
      "  - MPS acceleration enabled for GPU-like performance on M4\n",
      "  - Batch size optimized for Apple Silicon (64)\n",
      "  - Memory management tuned for unified memory architecture\n",
      "  - Image embeddings are cached and won't be re-extracted on next run\n",
      "  - To re-extract, delete image_embeddings.parquet\n",
      "  - Expected speed: ~10-15x faster than CPU on M4\n",
      "\n",
      "ðŸŽ¯ FashionCLIP Advantages for H&M Competition:\n",
      "  âœ“ Trained on 800K+ fashion products (not general images)\n",
      "  âœ“ Understands fashion-specific concepts (patterns, styles, materials)\n",
      "  âœ“ Better semantic similarity for similar items\n",
      "  âœ“ Can handle product attributes H&M uses (stripes, color-blocked, etc.)\n",
      "  âœ“ Optimized for standard product images (like H&M's dataset)\n",
      "  âœ“ Can be used for zero-shot classification of fashion categories\n"
     ]
    }
   ],
   "source": [
    "# INTEGRATE INTO TRAINING FEATURES\n",
    "\n",
    "print_section(\"INTEGRATING IMAGE EMBEDDINGS INTO TRAINING FEATURES\")\n",
    "\n",
    "# Check for different possible training feature files\n",
    "training_file = None\n",
    "possible_files = [\n",
    "    'training_features.parquet'\n",
    "]\n",
    "\n",
    "for filename in possible_files:\n",
    "    if (Path(\"/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2/\" + filename)).exists():\n",
    "        training_file = filename\n",
    "        break\n",
    "try:\n",
    "    image_embeddings_df = pd.read_parquet(config.OUTPUT_PATH / 'image_embeddings_3.parquet')\n",
    "except Exception as e:\n",
    "    print(f\"unable to load image embeddings\")\n",
    "\n",
    "if training_file is None:\n",
    "    print(\"No training features file found!\")\n",
    "    print(f\"\\n Image embeddings saved to: {config.OUTPUT_PATH / 'image_embeddings.parquet'}\")\n",
    "else:\n",
    "    # Load training features\n",
    "    print(f\"Loading {training_file}...\")\n",
    "    training_features = pd.read_parquet(\"/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2/\" + training_file)\n",
    "    print(f\"âœ“ Loaded training features: {len(training_features):,} rows\")\n",
    "    print(f\"  Current features: {len(training_features.columns) - 2}\")  # Exclude customer_id, article_id\n",
    "    print_memory()\n",
    "\n",
    "    # Merge image embeddings\n",
    "    print(\"\\nMerging image embeddings with training features...\")\n",
    "    print(f\"  Image embeddings: {len(image_embeddings_df):,} articles\")\n",
    "\n",
    "    # Merge on article_id\n",
    "    training_features = training_features.merge(\n",
    "        image_embeddings_df,\n",
    "        on='article_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    print(f\"Merged successfully\")\n",
    "    print(f\"New features: {len(training_features.columns) - 2}\")\n",
    "\n",
    "    # Check for missing embeddings\n",
    "    image_cols = [col for col in training_features.columns if col.startswith('image_emb_')]\n",
    "    missing_embeddings = training_features[image_cols].isna().any(axis=1).sum()\n",
    "\n",
    "    if missing_embeddings > 0:\n",
    "        print(f\"\\n Found {missing_embeddings:,} rows with missing image embeddings\")\n",
    "        print(\"Filling with mean values...\")\n",
    "        \n",
    "        # Fill with mean\n",
    "        for col in image_cols:\n",
    "            mean_val = training_features[col].mean()\n",
    "            training_features[col] = training_features[col].fillna(mean_val)\n",
    "        \n",
    "        print(\"âœ“ Filled missing values\")\n",
    "\n",
    "    # Convert to float32 to save memory\n",
    "    print(\"\\nOptimizing data types...\")\n",
    "    for col in image_cols:\n",
    "        training_features[col] = training_features[col].astype(np.float32)\n",
    "\n",
    "    print_memory()\n",
    "\n",
    "    # Save updated training features\n",
    "    print(f\"\\nSaving updated {training_file}...\")\n",
    "    training_features.to_parquet(\n",
    "        config.OUTPUT_PATH / training_file,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    file_size = (config.OUTPUT_PATH / training_file).stat().st_size / 1024**2\n",
    "    print(f\"âœ“ Saved {training_file} ({file_size:.2f} MB)\")\n",
    "\n",
    "    # Update feature names\n",
    "    feature_names = [col for col in training_features.columns if col not in ['customer_id', 'article_id']]\n",
    "\n",
    "    with open(config.OUTPUT_PATH / 'feature_names.txt', 'w') as f:\n",
    "        f.write('\\n'.join(feature_names))\n",
    "\n",
    "    print(f\"âœ“ Updated feature_names.txt ({len(feature_names)} features)\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # SUMMARY\n",
    "    # ============================================================================\n",
    "\n",
    "    print_section(\"IMAGE EMBEDDING INTEGRATION COMPLETE!\")\n",
    "\n",
    "    print(\"Summary:\")\n",
    "    print(f\"Image embeddings extracted: {len(image_embeddings_df):,}\")\n",
    "    print(f\"Image embedding dimensions: {config.EMBEDDING_DIM}\")\n",
    "    print(f\"Total features: {len(feature_names)}\")\n",
    "    print(f\"User features: {len([f for f in feature_names if any(x in f for x in ['user', 'purchase', 'age'])])}\")\n",
    "    print(f\"Item features: {len([f for f in feature_names if any(x in f for x in ['sales', 'product', 'department'])])}\")\n",
    "    print(f\"Interaction features: {len([f for f in feature_names if any(x in f for x in ['match', 'rank', 'score', 'similarity'])])}\")\n",
    "    print(f\"Image features: {len(image_cols)}\")\n",
    "\n",
    "    print(\"\\nFiles created/updated:\")\n",
    "    print(f\"image_embeddings.parquet - Image embeddings for all articles\")\n",
    "    print(f\"{training_file} - Updated with image embeddings\")\n",
    "    print(f\"feature_names.txt - Updated feature list\")\n",
    "\n",
    "print(\" Ready for Stage 4: Model Training with Image Features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2de80288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2/training_features.parquet...\n",
      "\n",
      "Shape: (4044442, 74)\n",
      "\n",
      "Columns: ['customer_id', 'article_id', 'repurchase_score', 'popularity_score_x', 'copurchase_score', 'userknn_score', 'category_score', 'text_similarity_score', 'n_strategies', 'has_purchased_item', 'days_since_item_purchase', 'popularity_score_y', 'category_match', 'price_vs_user_avg', 'is_cheaper_than_usual', 'copurchase_score_normalized', 'has_copurchase_signal', 'repurchase_score_rank', 'copurchase_score_rank', 'userknn_score_rank', 'category_score_rank', 'overall_rank', 'n_purchases', 'avg_price', 'std_price', 'min_price', 'max_price', 'days_since_first_purchase', 'days_since_last_purchase', 'purchase_frequency', 'n_purchases_last_week', 'avg_price_last_week', 'is_active_last_week', 'n_unique_articles', 'n_unique_categories', 'exploration_rate', 'age', 'FN', 'Active', 'purchase_trend', 'n_unique_buyers', 'total_sales', 'days_since_first_sale', 'days_since_last_sale', 'sales_frequency', 'sales_last_week', 'buyers_last_week', 'sales_trend', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id', 'department_no', 'index_group_no', 'section_no', 'garment_group_no', 'popularity_score', 'product_type_name_frequency', 'product_type_name_count', 'product_group_name_frequency', 'product_group_name_count', 'colour_group_name_frequency', 'colour_group_name_count', 'department_name_frequency', 'department_name_count', 'index_group_name_frequency', 'index_group_name_count', 'section_name_frequency', 'section_name_count', 'garment_group_name_frequency', 'garment_group_name_count', 'semantic_diversity', 'semantic_range', 'user_item_text_similarity']\n",
      "\n",
      "First 5 rows:\n",
      "100    0.0\n",
      "101    0.0\n",
      "102    0.0\n",
      "103    0.0\n",
      "104    0.0\n",
      "Name: userknn_score, dtype: float64\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4044442 entries, 0 to 4044441\n",
      "Data columns (total 74 columns):\n",
      " #   Column                        Dtype   \n",
      "---  ------                        -----   \n",
      " 0   customer_id                   object  \n",
      " 1   article_id                    int64   \n",
      " 2   repurchase_score              float32 \n",
      " 3   popularity_score_x            float32 \n",
      " 4   copurchase_score              float64 \n",
      " 5   userknn_score                 float64 \n",
      " 6   category_score                float64 \n",
      " 7   text_similarity_score         float32 \n",
      " 8   n_strategies                  int64   \n",
      " 9   has_purchased_item            int8    \n",
      " 10  days_since_item_purchase      int16   \n",
      " 11  popularity_score_y            float32 \n",
      " 12  category_match                int8    \n",
      " 13  price_vs_user_avg             float32 \n",
      " 14  is_cheaper_than_usual         int8    \n",
      " 15  copurchase_score_normalized   float32 \n",
      " 16  has_copurchase_signal         int8    \n",
      " 17  repurchase_score_rank         int16   \n",
      " 18  copurchase_score_rank         int16   \n",
      " 19  userknn_score_rank            int16   \n",
      " 20  category_score_rank           int16   \n",
      " 21  overall_rank                  int16   \n",
      " 22  n_purchases                   int32   \n",
      " 23  avg_price                     float32 \n",
      " 24  std_price                     float32 \n",
      " 25  min_price                     float32 \n",
      " 26  max_price                     float32 \n",
      " 27  days_since_first_purchase     int16   \n",
      " 28  days_since_last_purchase      int16   \n",
      " 29  purchase_frequency            float32 \n",
      " 30  n_purchases_last_week         int16   \n",
      " 31  avg_price_last_week           float32 \n",
      " 32  is_active_last_week           int8    \n",
      " 33  n_unique_articles             int32   \n",
      " 34  n_unique_categories           int32   \n",
      " 35  exploration_rate              float32 \n",
      " 36  age                           float32 \n",
      " 37  FN                            float32 \n",
      " 38  Active                        float32 \n",
      " 39  purchase_trend                float32 \n",
      " 40  n_unique_buyers               float64 \n",
      " 41  total_sales                   float64 \n",
      " 42  days_since_first_sale         float64 \n",
      " 43  days_since_last_sale          float64 \n",
      " 44  sales_frequency               float32 \n",
      " 45  sales_last_week               float64 \n",
      " 46  buyers_last_week              float64 \n",
      " 47  sales_trend                   float32 \n",
      " 48  product_type_no               category\n",
      " 49  graphical_appearance_no       category\n",
      " 50  colour_group_code             category\n",
      " 51  perceived_colour_value_id     category\n",
      " 52  department_no                 category\n",
      " 53  index_group_no                category\n",
      " 54  section_no                    category\n",
      " 55  garment_group_no              category\n",
      " 56  popularity_score              float32 \n",
      " 57  product_type_name_frequency   float32 \n",
      " 58  product_type_name_count       int32   \n",
      " 59  product_group_name_frequency  float32 \n",
      " 60  product_group_name_count      int32   \n",
      " 61  colour_group_name_frequency   float32 \n",
      " 62  colour_group_name_count       int32   \n",
      " 63  department_name_frequency     float32 \n",
      " 64  department_name_count         int32   \n",
      " 65  index_group_name_frequency    float32 \n",
      " 66  index_group_name_count        int32   \n",
      " 67  section_name_frequency        float32 \n",
      " 68  section_name_count            int32   \n",
      " 69  garment_group_name_frequency  float32 \n",
      " 70  garment_group_name_count      int32   \n",
      " 71  semantic_diversity            float32 \n",
      " 72  semantic_range                float32 \n",
      " 73  user_item_text_similarity     float32 \n",
      "dtypes: category(8), float32(30), float64(9), int16(9), int32(10), int64(2), int8(5), object(1)\n",
      "memory usage: 1.1+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your file\n",
    "file_path = '/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2/training_features.parquet'\n",
    "\n",
    "try:\n",
    "    # Load the parquet file\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # 1. Show dimensions (rows, columns)\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "\n",
    "    # 2. Show column names\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "    # 3. Show the first 5 rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df[100:105][\"userknn_score\"])\n",
    "\n",
    "    # 4. (Optional) Show data types and memory usage\n",
    "    print(\"\\nInfo:\")\n",
    "    print(df.info())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae0df1",
   "metadata": {},
   "source": [
    "### Stage 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757dba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d658c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    MODEL_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models')\n",
    "    \n",
    "    MODEL_PATH.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Device\n",
    "    if torch.backends.mps.is_available():\n",
    "        DEVICE = torch.device('mps')\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "    else:\n",
    "        DEVICE = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    # Sampling configuration - LARGE DATASET\n",
    "    MAX_POSITIVE_SAMPLES = 2_500_000  # 2.5M positives\n",
    "    MAX_NEGATIVE_SAMPLES = 1_500_000  # 1.5M negatives\n",
    "    HARD_NEGATIVE_RATIO = 0.7  # 70% hard, 30% random\n",
    "    \n",
    "    VAL_SIZE = 0.15  # 15% validation (smaller to keep more for training)\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading training_features.parquet...\n",
      "âœ“ Loaded 4,044,442 candidates\n",
      "  Memory: 9.22 GB\n",
      "\n",
      "Loading val_ground_truth.parquet...\n",
      "âœ“ Loaded validation ground truth: 4,943 users\n",
      "\n",
      "Loading train_transactions.parquet...\n",
      "âœ“ Loaded training transactions: 412,156 rows\n",
      "  Memory: 0.05 GB\n",
      "\n",
      "User breakdown:\n",
      "  Total users in candidates: 47,543\n",
      "  Validation users: 4,943\n",
      "  Training-only users: 43,842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nLoading training_features.parquet...\")\n",
    "training_features = pd.read_parquet(config.DATA_PATH / 'training_features.parquet')\n",
    "print(f\"âœ“ Loaded {len(training_features):,} candidates\")\n",
    "print(f\"  Memory: {training_features.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Load ground truth for validation period\n",
    "print(\"\\nLoading val_ground_truth.parquet...\")\n",
    "val_ground_truth = pd.read_parquet(Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2') / 'val_ground_truth.parquet')\n",
    "print(f\"âœ“ Loaded validation ground truth: {len(val_ground_truth):,} users\")\n",
    "\n",
    "# Load training transactions for creating training period labels\n",
    "print(\"\\nLoading train_transactions.parquet...\")\n",
    "train_transactions = pd.read_parquet(config.DATA_PATH / 'train_transactions.parquet')\n",
    "print(f\"âœ“ Loaded training transactions: {len(train_transactions):,} rows\")\n",
    "print(f\"  Memory: {train_transactions.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Identify validation users\n",
    "val_users_set = set(val_ground_truth['customer_id'].unique())\n",
    "all_users_in_features = set(training_features['customer_id'].unique())\n",
    "train_only_users = all_users_in_features - val_users_set\n",
    "\n",
    "print(f\"\\nUser breakdown:\")\n",
    "print(f\"  Total users in candidates: {len(all_users_in_features):,}\")\n",
    "print(f\"  Validation users: {len(val_users_set):,}\")\n",
    "print(f\"  Training-only users: {len(train_only_users):,}\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f82529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING LABELS\n",
      "================================================================================\n",
      "\n",
      "[1/3] Creating training period labels...\n",
      "  Processing 412,156 training transactions...\n",
      "  âœ“ Created train_ground_truth: 47,543 users\n",
      "  Columns: ['customer_id', 'purchased_articles']\n",
      "  âœ“ Exploded to 412,156 rows\n",
      "  âœ“ All required columns present: ['customer_id', 'article_id', 'train_label']\n",
      "  train_ground_truth_exploded shape: (412156, 3)\n"
     ]
    }
   ],
   "source": [
    "# CREATE LABELS - OPTIMIZED FOR TRAINING & VALIDATION USERS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING LABELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Create training period labels (for ALL users)\n",
    "print(\"\\n[1/3] Creating training period labels...\")\n",
    "\n",
    "# Check train_transactions structure\n",
    "if len(train_transactions) == 0:\n",
    "    raise ValueError(\"train_transactions is empty! Cannot create training labels.\")\n",
    "    \n",
    "if 'customer_id' not in train_transactions.columns or 'article_id' not in train_transactions.columns:\n",
    "    raise ValueError(f\"train_transactions missing required columns. Available: {train_transactions.columns.tolist()}\")\n",
    "\n",
    "print(f\"  Processing {len(train_transactions):,} training transactions...\")\n",
    "\n",
    "train_ground_truth = (\n",
    "    train_transactions\n",
    "    .groupby('customer_id')['article_id']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={'article_id': 'purchased_articles'})\n",
    ")\n",
    "\n",
    "print(f\"  âœ“ Created train_ground_truth: {len(train_ground_truth):,} users\")\n",
    "print(f\"  Columns: {train_ground_truth.columns.tolist()}\")\n",
    "\n",
    "# Verify purchased_articles column exists before explode\n",
    "if 'purchased_articles' not in train_ground_truth.columns:\n",
    "    print(f\"  âš ï¸ Error: 'purchased_articles' column not found!\")\n",
    "    print(f\"  Available columns: {train_ground_truth.columns.tolist()}\")\n",
    "    raise KeyError(f\"'purchased_articles' column not found in train_ground_truth\")\n",
    "\n",
    "# Explode the lists into individual rows\n",
    "train_ground_truth_exploded = train_ground_truth.explode('purchased_articles')\n",
    "\n",
    "# Check if explode worked correctly\n",
    "if len(train_ground_truth_exploded) == 0:\n",
    "    print(\"  âš ï¸ Warning: train_ground_truth_exploded is empty after explode!\")\n",
    "    print(f\"  train_ground_truth shape: {train_ground_truth.shape}\")\n",
    "    print(f\"  Sample of purchased_articles: {train_ground_truth['purchased_articles'].head()}\")\n",
    "    raise ValueError(\"train_ground_truth_exploded is empty - check if purchased_articles contains valid lists\")\n",
    "else:\n",
    "    print(f\"  âœ“ Exploded to {len(train_ground_truth_exploded):,} rows\")\n",
    "\n",
    "# Rename the column from purchased_articles to article_id\n",
    "train_ground_truth_exploded = train_ground_truth_exploded.rename(\n",
    "    columns={'purchased_articles': 'article_id'}\n",
    ")\n",
    "\n",
    "# Add train_label column\n",
    "train_ground_truth_exploded['train_label'] = 1\n",
    "\n",
    "# Verify the column was created\n",
    "if 'train_label' not in train_ground_truth_exploded.columns:\n",
    "    print(f\"  âš ï¸ Error: Failed to create 'train_label' column!\")\n",
    "    print(f\"  DataFrame type: {type(train_ground_truth_exploded)}\")\n",
    "    print(f\"  Available columns: {train_ground_truth_exploded.columns.tolist()}\")\n",
    "    print(f\"  DataFrame shape: {train_ground_truth_exploded.shape}\")\n",
    "    raise KeyError(\"'train_label' column was not created successfully\")\n",
    "\n",
    "# Verify all required columns exist before merge\n",
    "required_cols = ['customer_id', 'article_id', 'train_label']\n",
    "missing_cols = [col for col in required_cols if col not in train_ground_truth_exploded.columns]\n",
    "if missing_cols:\n",
    "    print(f\"  âš ï¸ Error: Missing columns in train_ground_truth_exploded: {missing_cols}\")\n",
    "    print(f\"  Available columns: {train_ground_truth_exploded.columns.tolist()}\")\n",
    "    print(f\"  DataFrame info:\")\n",
    "    print(train_ground_truth_exploded.info())\n",
    "    raise KeyError(f\"Missing required columns: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"  âœ“ All required columns present: {required_cols}\")\n",
    "    print(f\"  train_ground_truth_exploded shape: {train_ground_truth_exploded.shape}\")\n",
    "\n",
    "# Merge training labels\n",
    "training_features = training_features.merge(\n",
    "    train_ground_truth_exploded[required_cols],\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='left'\n",
    ")\n",
    "training_features['train_label'] = training_features['train_label'].fillna(0).astype(np.int8)\n",
    "\n",
    "print(f\"  âœ“ Training period positives: {training_features['train_label'].sum():,}\")\n",
    "\n",
    "# Clean up\n",
    "del train_ground_truth, train_ground_truth_exploded\n",
    "gc.collect()\n",
    "\n",
    "# Step 2: Create validation period labels (only for validation users)\n",
    "print(\"\\n[2/3] Creating validation period labels...\")\n",
    "val_ground_truth_exploded = val_ground_truth.explode('purchased_articles').rename(\n",
    "    columns={'purchased_articles': 'article_id'}\n",
    ")\n",
    "val_ground_truth_exploded['val_label'] = 1\n",
    "\n",
    "# Merge validation labels\n",
    "training_features = training_features.merge(\n",
    "    val_ground_truth_exploded[['customer_id', 'article_id', 'val_label']],\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='left'\n",
    ")\n",
    "training_features['val_label'] = training_features['val_label'].fillna(0).astype(np.int8)\n",
    "\n",
    "print(f\"  âœ“ Validation period positives: {training_features['val_label'].sum():,}\")\n",
    "\n",
    "# Clean up\n",
    "del val_ground_truth_exploded\n",
    "gc.collect()\n",
    "\n",
    "# Step 3: Create final label - use validation labels for validation users, training labels for others\n",
    "print(\"\\n[3/3] Creating final labels (vectorized)...\")\n",
    "val_users_set = set(val_ground_truth['customer_id'].unique())\n",
    "\n",
    "# Vectorized approach - much faster than apply()\n",
    "# For validation users: use validation period labels (predict future purchases)\n",
    "# For training-only users: use training period labels (learn from past purchases)\n",
    "is_val_user = training_features['customer_id'].isin(val_users_set)\n",
    "training_features['label'] = np.where(\n",
    "    is_val_user,\n",
    "    training_features['val_label'],\n",
    "    training_features['train_label']\n",
    ").astype(np.int8)\n",
    "\n",
    "# Add user_type for tracking (vectorized)\n",
    "# Convert to pandas Series first, then to category dtype (numpy arrays don't support category)\n",
    "training_features['user_type'] = pd.Series(\n",
    "    np.where(is_val_user, 'validation', 'training_only'),\n",
    "    dtype='category'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Final label summary:\")\n",
    "print(f\"  Total positives: {training_features['label'].sum():,}\")\n",
    "print(f\"  Total negatives: {(training_features['label']==0).sum():,}\")\n",
    "print(f\"\\n  By user type:\")\n",
    "print(f\"    Validation users - Positives: {training_features[training_features['user_type']=='validation']['label'].sum():,}\")\n",
    "print(f\"    Validation users - Negatives: {(training_features[training_features['user_type']=='validation']['label']==0).sum():,}\")\n",
    "print(f\"    Training-only users - Positives: {training_features[training_features['user_type']=='training_only']['label'].sum():,}\")\n",
    "print(f\"    Training-only users - Negatives: {(training_features[training_features['user_type']=='training_only']['label']==0).sum():,}\")\n",
    "\n",
    "# Clean up intermediate columns (keep for now in case needed for analysis)\n",
    "# training_features = training_features.drop(['train_label', 'val_label'], axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SEPARATION\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEPARATING FEATURE TYPES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load feature names\n",
    "with open(config.DATA_PATH / 'feature_names.txt', 'r') as f:\n",
    "    all_features = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Total features: {len(all_features)}\")\n",
    "\n",
    "# Identify feature types\n",
    "user_features = [f for f in all_features if any(x in f for x in \n",
    "    ['user', 'customer', 'purchase', 'age', 'Active', 'FN', 'exploration', 'diversity', 'trend'])]\n",
    "\n",
    "item_features = [f for f in all_features if any(x in f for x in \n",
    "    ['sales', 'product', 'department', 'section', 'colour', 'garment', 'article', \n",
    "     'perceived', 'graphical', 'index_'])]\n",
    "\n",
    "image_features = [f for f in all_features if f.startswith('image_emb_')]\n",
    "\n",
    "# Interaction features are everything else (excluding user, item, image)\n",
    "interaction_features = [f for f in all_features \n",
    "                        if f not in user_features \n",
    "                        and f not in item_features \n",
    "                        and f not in image_features]\n",
    "\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  User features: {len(user_features)}\")\n",
    "print(f\"  Item features: {len(item_features)}\")\n",
    "print(f\"  Image features: {len(image_features)}\")\n",
    "print(f\"  Interaction features: {len(interaction_features)}\")\n",
    "\n",
    "# Update config with actual dimensions\n",
    "config.USER_FEATURE_DIM = len(user_features)\n",
    "config.ITEM_FEATURE_DIM = len(item_features)\n",
    "config.IMAGE_FEATURE_DIM = len(image_features)\n",
    "config.INTERACTION_FEATURE_DIM = len(interaction_features)\n",
    "\n",
    "# Save feature lists\n",
    "feature_dict = {\n",
    "    'user_features': user_features,\n",
    "    'item_features': item_features,\n",
    "    'image_features': image_features,\n",
    "    'interaction_features': interaction_features,\n",
    "    'all_features': all_features\n",
    "}\n",
    "\n",
    "with open(config.MODEL_PATH / 'feature_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_dict, f)\n",
    "\n",
    "print(f\"âœ“ Saved feature_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c288b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT INTO TRAIN/VAL AND SAVE\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLITTING INTO TRAIN/VAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split into train and validation sets\n",
    "# Use stratified split to maintain label distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get feature columns (exclude customer_id, article_id, label)\n",
    "feature_cols = [col for col in training_features.columns \n",
    "                if col not in ['customer_id', 'article_id', 'label']]\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data = train_test_split(\n",
    "    training_features,\n",
    "    test_size=config.VAL_SIZE,\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    stratify=training_features['label']  # Maintain label distribution\n",
    ")\n",
    "\n",
    "# Reset indices\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nâœ“ Train set: {len(train_data):,} samples\")\n",
    "print(f\"  Positive: {train_data['label'].sum():,} ({100*train_data['label'].mean():.2f}%)\")\n",
    "print(f\"  Negative: {(train_data['label']==0).sum():,} ({100*(1-train_data['label'].mean()):.2f}%)\")\n",
    "\n",
    "print(f\"\\nâœ“ Val set: {len(val_data):,} samples\")\n",
    "print(f\"  Positive: {val_data['label'].sum():,} ({100*val_data['label'].mean():.2f}%)\")\n",
    "print(f\"  Negative: {(val_data['label']==0).sum():,} ({100*(1-val_data['label'].mean()):.2f}%)\")\n",
    "\n",
    "# Save train and validation datasets\n",
    "print(\"\\nSaving datasets...\")\n",
    "train_data.to_parquet(config.MODEL_PATH / 'train_data.parquet', index=False)\n",
    "val_data.to_parquet(config.MODEL_PATH / 'val_data.parquet', index=False)\n",
    "\n",
    "print(f\"âœ“ Saved train_data.parquet ({len(train_data):,} rows)\")\n",
    "print(f\"âœ“ Saved val_data.parquet ({len(val_data):,} rows)\")\n",
    "\n",
    "# Clean up\n",
    "del training_features\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n Dataset creation complete! Ready for model training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0287f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DOWNSAMPLING DATASETS FOR LOCAL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Strategy: Keep ALL positives, sample negatives for balanced ratio (~40:60)\n",
      "\n",
      "Loading saved datasets...\n",
      "\n",
      "Original dataset sizes:\n",
      "  Train: 685,300 samples\n",
      "    Positives: 274,142 (40.00%)\n",
      "    Negatives: 411,158 (60.00%)\n",
      "  Val: 121,164 samples\n",
      "    Positives: 48,444 (39.98%)\n",
      "    Negatives: 72,720 (60.02%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Downsampling TRAINING set...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ“ Downsampled training set:\n",
      "  Total samples: 685,300\n",
      "  Positives: 274,142 (40.00%)\n",
      "  Negatives: 411,158 (60.00%)\n",
      "  Reduction: 0.0%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Downsampling VALIDATION set...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ“ Downsampled validation set:\n",
      "  Total samples: 121,164\n",
      "  Positives: 48,444 (39.98%)\n",
      "  Negatives: 72,720 (60.02%)\n",
      "  Reduction: 0.0%\n",
      "\n",
      "================================================================================\n",
      "SAVING DOWNSAMPLED DATASETS\n",
      "================================================================================\n",
      "\n",
      "Saving downsampled datasets...\n",
      "âœ“ Saved train_data.parquet (685,300 rows)\n",
      "âœ“ Saved val_data.parquet (121,164 rows)\n",
      "\n",
      "File sizes:\n",
      "  train_data.parquet: 1249.01 MB\n",
      "  val_data.parquet: 220.10 MB\n",
      "  Total: 1469.11 MB\n",
      "\n",
      "âœ… Downsampling complete! Datasets optimized for M4 MacBook Air local training.\n",
      "\n",
      "Summary:\n",
      "  Train: 685,300 samples (1249.0 MB)\n",
      "  Val: 121,164 samples (220.1 MB)\n",
      "  Positive ratio: ~30% (balanced for training)\n"
     ]
    }
   ],
   "source": [
    "# DOWNSAMPLE DATASETS FOR LOCAL TRAINING (M4 MacBook Air Optimized)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOWNSAMPLING DATASETS FOR LOCAL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nStrategy: Keep ALL positives, sample negatives for balanced ratio (~40:60)\\n\")\n",
    "\n",
    "# Configuration for balanced sampling\n",
    "TARGET_POSITIVE_RATIO = 0.30  # 40% positive, 60% negative\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Load the saved datasets\n",
    "print(\"Loading saved datasets...\")\n",
    "train_data = pd.read_parquet(config.MODEL_PATH / 'train_data.parquet')\n",
    "val_data = pd.read_parquet(config.MODEL_PATH / 'val_data.parquet')\n",
    "\n",
    "print(f\"\\nOriginal dataset sizes:\")\n",
    "print(f\"  Train: {len(train_data):,} samples\")\n",
    "print(f\"    Positives: {train_data['label'].sum():,} ({100*train_data['label'].mean():.2f}%)\")\n",
    "print(f\"    Negatives: {(train_data['label']==0).sum():,} ({100*(1-train_data['label'].mean()):.2f}%)\")\n",
    "print(f\"  Val: {len(val_data):,} samples\")\n",
    "print(f\"    Positives: {val_data['label'].sum():,} ({100*val_data['label'].mean():.2f}%)\")\n",
    "print(f\"    Negatives: {(val_data['label']==0).sum():,} ({100*(1-val_data['label'].mean()):.2f}%)\")\n",
    "\n",
    "# Function to downsample while keeping all positives\n",
    "def downsample_balanced(df, target_positive_ratio=0.40, random_state=42):\n",
    "    \"\"\"\n",
    "    Downsample dataset keeping ALL positives and sampling negatives\n",
    "    to achieve target positive ratio\n",
    "    \"\"\"\n",
    "    positives = df[df['label'] == 1].copy()\n",
    "    negatives = df[df['label'] == 0].copy()\n",
    "    \n",
    "    n_positives = len(positives)\n",
    "    \n",
    "    # Calculate how many negatives we need for target ratio\n",
    "    # target_positive_ratio = n_positives / (n_positives + n_negatives)\n",
    "    # Solving for n_negatives:\n",
    "    # n_negatives = n_positives * (1 - target_positive_ratio) / target_positive_ratio\n",
    "    n_negatives_needed = int(n_positives * (1 - target_positive_ratio) / target_positive_ratio)\n",
    "    \n",
    "    # Sample negatives (don't exceed available)\n",
    "    n_negatives_to_sample = min(n_negatives_needed, len(negatives))\n",
    "    \n",
    "    if n_negatives_to_sample < len(negatives):\n",
    "        negatives_sampled = negatives.sample(n=n_negatives_to_sample, random_state=random_state)\n",
    "    else:\n",
    "        negatives_sampled = negatives.copy()\n",
    "    \n",
    "    # Combine\n",
    "    balanced_df = pd.concat([positives, negatives_sampled], ignore_index=True)\n",
    "    \n",
    "    # Shuffle\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return balanced_df, n_positives, n_negatives_to_sample\n",
    "\n",
    "# Downsample training set\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Downsampling TRAINING set...\")\n",
    "print(\"-\"*80)\n",
    "train_data_downsampled, train_pos, train_neg = downsample_balanced(\n",
    "    train_data, \n",
    "    target_positive_ratio=TARGET_POSITIVE_RATIO,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Downsampled training set:\")\n",
    "print(f\"  Total samples: {len(train_data_downsampled):,}\")\n",
    "print(f\"  Positives: {train_pos:,} ({100*train_data_downsampled['label'].mean():.2f}%)\")\n",
    "print(f\"  Negatives: {train_neg:,} ({100*(1-train_data_downsampled['label'].mean()):.2f}%)\")\n",
    "print(f\"  Reduction: {100*(1 - len(train_data_downsampled)/len(train_data)):.1f}%\")\n",
    "\n",
    "# Downsample validation set\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Downsampling VALIDATION set...\")\n",
    "print(\"-\"*80)\n",
    "val_data_downsampled, val_pos, val_neg = downsample_balanced(\n",
    "    val_data,\n",
    "    target_positive_ratio=TARGET_POSITIVE_RATIO,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Downsampled validation set:\")\n",
    "print(f\"  Total samples: {len(val_data_downsampled):,}\")\n",
    "print(f\"  Positives: {val_pos:,} ({100*val_data_downsampled['label'].mean():.2f}%)\")\n",
    "print(f\"  Negatives: {val_neg:,} ({100*(1-val_data_downsampled['label'].mean()):.2f}%)\")\n",
    "print(f\"  Reduction: {100*(1 - len(val_data_downsampled)/len(val_data)):.1f}%\")\n",
    "\n",
    "# Save downsampled datasets (overwrite original files)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING DOWNSAMPLED DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nSaving downsampled datasets...\")\n",
    "train_data_downsampled.to_parquet(config.MODEL_PATH / 'train_data.parquet', index=False)\n",
    "val_data_downsampled.to_parquet(config.MODEL_PATH / 'val_data.parquet', index=False)\n",
    "\n",
    "print(f\"âœ“ Saved train_data.parquet ({len(train_data_downsampled):,} rows)\")\n",
    "print(f\"âœ“ Saved val_data.parquet ({len(val_data_downsampled):,} rows)\")\n",
    "\n",
    "# Calculate file sizes\n",
    "train_size_mb = (config.MODEL_PATH / 'train_data.parquet').stat().st_size / 1024**2\n",
    "val_size_mb = (config.MODEL_PATH / 'val_data.parquet').stat().st_size / 1024**2\n",
    "\n",
    "print(f\"\\nFile sizes:\")\n",
    "print(f\"  train_data.parquet: {train_size_mb:.2f} MB\")\n",
    "print(f\"  val_data.parquet: {val_size_mb:.2f} MB\")\n",
    "print(f\"  Total: {train_size_mb + val_size_mb:.2f} MB\")\n",
    "\n",
    "# Clean up\n",
    "del train_data, val_data\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nâœ… Downsampling complete! Datasets optimized for M4 MacBook Air local training.\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Train: {len(train_data_downsampled):,} samples ({train_size_mb:.1f} MB)\")\n",
    "print(f\"  Val: {len(val_data_downsampled):,} samples ({val_size_mb:.1f} MB)\")\n",
    "print(f\"  Positive ratio: ~{TARGET_POSITIVE_RATIO*100:.0f}% (balanced for training)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a8297",
   "metadata": {},
   "source": [
    "### Stage 4A: LightGBM Training & Reranking\n",
    "\n",
    "This stage implements multiple LightGBM models for reranking candidates:\n",
    "- **LightGBM Classifier**: Binary classification approach\n",
    "- **LightGBM Ranker (LambdaRank)**: Learning-to-rank with MAP@12 optimization\n",
    "- **LightGBM Ranker (XENDCG)**: Alternative ranking objective\n",
    "- **Ensemble**: Weighted combination of all models\n",
    "\n",
    "**Key Features:**\n",
    "- MAP@12 evaluation metric (competition metric)\n",
    "- Time-based cross-validation\n",
    "- Feature importance analysis\n",
    "- Model checkpointing and saving\n",
    "- Optimized for M4 MacBook Air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ac52ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LightGBM Configuration loaded\n",
      "  Model path: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models\n",
      "  Data path: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND CONFIGURATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "class LightGBMConfig:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    MODEL_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models')\n",
    "    \n",
    "    MODEL_PATH.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Training configuration\n",
    "    RANDOM_STATE = 42\n",
    "    N_FOLDS = 5  # Number of CV folds\n",
    "    EARLY_STOPPING_ROUNDS = 100\n",
    "    VERBOSE_EVAL = 50\n",
    "    \n",
    "    # Model configurations\n",
    "    N_ESTIMATORS = 2000  # Maximum boosting rounds\n",
    "    \n",
    "    # Device (LightGBM uses CPU by default, but optimized for M4)\n",
    "    DEVICE = 'cpu'  # LightGBM doesn't support MPS, but CPU is fast on M4\n",
    "    \n",
    "    # Feature selection\n",
    "    MIN_FEATURE_IMPORTANCE = 0.001  # Minimum importance to keep feature\n",
    "    \n",
    "print(\"âœ“ LightGBM Configuration loaded\")\n",
    "print(f\"  Model path: {LightGBMConfig.MODEL_PATH}\")\n",
    "print(f\"  Data path: {LightGBMConfig.DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42244f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MAP@12 evaluation functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAP@12 EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_map_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision at K (MAP@K)\n",
    "    \n",
    "    Args:\n",
    "        y_true: List of lists, where each inner list contains the true article_ids\n",
    "        y_pred: List of lists, where each inner list contains predicted article_ids (ranked)\n",
    "        k: Number of top predictions to consider\n",
    "    \n",
    "    Returns:\n",
    "        MAP@K score\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Limit predictions to top k\n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    # Calculate AP for each user\n",
    "    aps = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Convert to sets for faster lookup\n",
    "        true_set = set(true_items)\n",
    "        \n",
    "        # Calculate precision at each position\n",
    "        hits = 0\n",
    "        precisions = []\n",
    "        \n",
    "        for i, pred_item in enumerate(pred_items):\n",
    "            if pred_item in true_set:\n",
    "                hits += 1\n",
    "                precision = hits / (i + 1)\n",
    "                precisions.append(precision)\n",
    "        \n",
    "        # Average Precision (AP) = mean of precisions at hit positions\n",
    "        if len(precisions) > 0:\n",
    "            ap = np.mean(precisions)\n",
    "            aps.append(ap)\n",
    "        else:\n",
    "            aps.append(0.0)\n",
    "    \n",
    "    # MAP = mean of all APs\n",
    "    return np.mean(aps) if len(aps) > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_map_at_12(df, predictions, customer_col='customer_id', \n",
    "                       article_col='article_id', label_col='label', k=12):\n",
    "    \"\"\"\n",
    "    Evaluate MAP@12 for a dataframe with predictions\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with customer_id, article_id, label columns\n",
    "        predictions: Array of prediction scores (same length as df)\n",
    "        customer_col: Name of customer ID column\n",
    "        article_col: Name of article ID column\n",
    "        label_col: Name of label column\n",
    "        k: Number of top predictions to consider\n",
    "    \n",
    "    Returns:\n",
    "        MAP@12 score\n",
    "    \"\"\"\n",
    "    # Add predictions to dataframe\n",
    "    df_eval = df[[customer_col, article_col, label_col]].copy()\n",
    "    df_eval['pred_score'] = predictions\n",
    "    \n",
    "    # Get true positives (purchased items) for each customer\n",
    "    true_positives = df_eval[df_eval[label_col] == 1].groupby(customer_col)[article_col].apply(list).to_dict()\n",
    "    \n",
    "    # Get top-k predictions for each customer\n",
    "    top_predictions = (df_eval.groupby(customer_col)\n",
    "                      .apply(lambda x: x.nlargest(k, 'pred_score')[article_col].tolist())\n",
    "                      .to_dict())\n",
    "    \n",
    "    # Calculate MAP@12\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Only evaluate on customers with true positives\n",
    "    for customer_id in true_positives.keys():\n",
    "        if customer_id in top_predictions:\n",
    "            y_true.append(true_positives[customer_id])\n",
    "            y_pred.append(top_predictions[customer_id])\n",
    "    \n",
    "    map_score = calculate_map_at_k(y_true, y_pred, k=k)\n",
    "    return map_score\n",
    "\n",
    "\n",
    "print(\"âœ“ MAP@12 evaluation functions loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8675a225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING TRAINING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading train_data.parquet...\n",
      "âœ“ Loaded 685,300 training samples\n",
      "  Memory: 1.57 GB\n",
      "  Positives: 274,142 (40.00%)\n",
      "  Negatives: 411,158 (60.00%)\n",
      "\n",
      "Loading val_data.parquet...\n",
      "âœ“ Loaded 121,164 validation samples\n",
      "  Memory: 0.28 GB\n",
      "  Positives: 48,444 (39.98%)\n",
      "  Negatives: 72,720 (60.02%)\n",
      "\n",
      " Feature columns identified: 584\n",
      "  Excluded columns: ['customer_id', 'article_id', 'label', 'user_type', 'train_label', 'val_label']\n",
      "\n",
      "Checking for missing values...\n",
      " No missing values found\n",
      "\n",
      "âœ“ Feature matrices prepared:\n",
      "  X_train: (685300, 584)\n",
      "  X_val: (121164, 584)\n",
      "  Feature types: {dtype('float32'): 542, dtype('int32'): 10, dtype('float64'): 9, dtype('int16'): 9, dtype('int8'): 5, dtype('int64'): 1, CategoricalDtype(categories=[ -1,  57,  59,  66,  67,  68,  69,  70,  71,  72,\n",
      "                  ...\n",
      "                  504, 508, 509, 511, 512, 515, 521, 523, 529, 532],\n",
      ", ordered=False, categories_dtype=int16): 1, CategoricalDtype(categories=[1010001, 1010002, 1010004, 1010005, 1010006, 1010007,\n",
      "                  1010008, 1010009, 1010010, 1010011, 1010012, 1010013,\n",
      "                  1010014, 1010015, 1010016, 1010017, 1010018, 1010019,\n",
      "                  1010020, 1010021, 1010022, 1010023, 1010024, 1010025,\n",
      "                  1010026, 1010027, 1010028],\n",
      ", ordered=False, categories_dtype=int32): 1, CategoricalDtype(categories=[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
      "                  17, 19, 20, 21, 22, 23, 30, 31, 32, 33, 40, 41, 42, 43, 50,\n",
      "                  51, 52, 53, 60, 61, 62, 63, 70, 71, 72, 73, 81, 82, 83, 90,\n",
      "                  91, 92, 93],\n",
      ", ordered=False, categories_dtype=int8): 1, CategoricalDtype(categories=[1, 2, 3, 4, 5, 6, 7], ordered=False, categories_dtype=int8): 1, CategoricalDtype(categories=[1201, 1212, 1222, 1241, 1244, 1310, 1313, 1322, 1334, 1336,\n",
      "                  ...\n",
      "                  8812, 8852, 8888, 8917, 8956, 9020, 9984, 9985, 9986, 9989],\n",
      ", ordered=False, categories_dtype=int16): 1, CategoricalDtype(categories=[1, 2, 3, 4, 26], ordered=False, categories_dtype=int8): 1, CategoricalDtype(categories=[ 2,  5,  6,  8, 11, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24,\n",
      "                  25, 26, 27, 29, 31, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50,\n",
      "                  51, 52, 53, 55, 56, 57, 58, 60, 61, 62, 64, 65, 66, 70, 72,\n",
      "                  76, 77, 79, 80, 82, 97],\n",
      ", ordered=False, categories_dtype=int8): 1, CategoricalDtype(categories=[1001, 1002, 1003, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n",
      "                  1012, 1013, 1014, 1016, 1017, 1018, 1019, 1020, 1021, 1023,\n",
      "                  1025],\n",
      ", ordered=False, categories_dtype=int16): 1}\n",
      "\n",
      "âœ“ Categorical features: 8\n",
      "  Examples: ['product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id', 'department_no']\n",
      "\n",
      " Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA AND PREPARE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING TRAINING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load train and validation datasets\n",
    "print(\"\\nLoading train_data.parquet...\")\n",
    "train_data = pd.read_parquet(LightGBMConfig.MODEL_PATH / 'train_data.parquet')\n",
    "print(f\"âœ“ Loaded {len(train_data):,} training samples\")\n",
    "print(f\"  Memory: {train_data.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "print(f\"  Positives: {train_data['label'].sum():,} ({100*train_data['label'].mean():.2f}%)\")\n",
    "print(f\"  Negatives: {(train_data['label']==0).sum():,} ({100*(1-train_data['label'].mean()):.2f}%)\")\n",
    "\n",
    "print(\"\\nLoading val_data.parquet...\")\n",
    "val_data = pd.read_parquet(LightGBMConfig.MODEL_PATH / 'val_data.parquet')\n",
    "print(f\"âœ“ Loaded {len(val_data):,} validation samples\")\n",
    "print(f\"  Memory: {val_data.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "print(f\"  Positives: {val_data['label'].sum():,} ({100*val_data['label'].mean():.2f}%)\")\n",
    "print(f\"  Negatives: {(val_data['label']==0).sum():,} ({100*(1-val_data['label'].mean()):.2f}%)\")\n",
    "\n",
    "# Identify feature columns (exclude ID and label columns)\n",
    "exclude_cols = ['customer_id', 'article_id', 'label', 'user_type', 'train_label', 'val_label']\n",
    "feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\n Feature columns identified: {len(feature_cols)}\")\n",
    "print(f\"  Excluded columns: {exclude_cols}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nChecking for missing values...\")\n",
    "train_missing = train_data[feature_cols].isnull().sum()\n",
    "val_missing = val_data[feature_cols].isnull().sum()\n",
    "\n",
    "if train_missing.sum() > 0:\n",
    "    print(f\" Training set has {train_missing.sum():,} missing values\")\n",
    "    missing_cols = train_missing[train_missing > 0].index.tolist()\n",
    "    print(f\"  Columns with missing values: {len(missing_cols)}\")\n",
    "    # Fill missing values with median (for numeric) or mode (for categorical)\n",
    "    for col in missing_cols:\n",
    "        if train_data[col].dtype in ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n",
    "            fill_value = train_data[col].median()\n",
    "            train_data[col] = train_data[col].fillna(fill_value)\n",
    "            val_data[col] = val_data[col].fillna(fill_value)\n",
    "        else:\n",
    "            fill_value = train_data[col].mode()[0] if len(train_data[col].mode()) > 0 else 0\n",
    "            train_data[col] = train_data[col].fillna(fill_value)\n",
    "            val_data[col] = val_data[col].fillna(fill_value)\n",
    "    print(\" Filled missing values\")\n",
    "else:\n",
    "    print(\" No missing values found\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = train_data[feature_cols].copy()\n",
    "y_train = train_data['label'].copy()\n",
    "X_val = val_data[feature_cols].copy()\n",
    "y_val = val_data['label'].copy()\n",
    "\n",
    "# Store customer and article IDs for evaluation\n",
    "train_customer_ids = train_data['customer_id'].copy()\n",
    "train_article_ids = train_data['article_id'].copy()\n",
    "val_customer_ids = val_data['customer_id'].copy()\n",
    "val_article_ids = val_data['article_id'].copy()\n",
    "\n",
    "print(f\"\\nâœ“ Feature matrices prepared:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  Feature types: {X_train.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = [col for col in feature_cols \n",
    "                       if X_train[col].dtype == 'category' or \n",
    "                          X_train[col].dtype == 'object' or\n",
    "                          col.endswith('_no') or col.endswith('_id') or col.endswith('_code')]\n",
    "\n",
    "print(f\"\\nâœ“ Categorical features: {len(categorical_features)}\")\n",
    "if len(categorical_features) > 0:\n",
    "    print(f\"  Examples: {categorical_features[:5]}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n Data loading complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4dc2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING LIGHTGBM MODELS\n",
      "================================================================================\n",
      "\n",
      "Converting categorical features to numeric codes...\n",
      " Converted 8 categorical features to numeric codes\n",
      "\n",
      " Creating LightGBM datasets...\n",
      " LightGBM datasets created\n",
      "\n",
      "Preparing group information for ranking models...\n",
      " Ranking datasets created with group information\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING MODELS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Training: lgb_classifier\n",
      "================================================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's binary_logloss: 0.0553072\tvalid's binary_logloss: 0.0573332\n",
      "[100]\ttrain's binary_logloss: 0.0217025\tvalid's binary_logloss: 0.0260448\n",
      "[150]\ttrain's binary_logloss: 0.0172244\tvalid's binary_logloss: 0.0242604\n",
      "[200]\ttrain's binary_logloss: 0.0151918\tvalid's binary_logloss: 0.0243083\n",
      "[250]\ttrain's binary_logloss: 0.0137373\tvalid's binary_logloss: 0.0243676\n",
      "Early stopping, best iteration is:\n",
      "[151]\ttrain's binary_logloss: 0.0171706\tvalid's binary_logloss: 0.0242534\n",
      "\n",
      "âœ“ lgb_classifier trained\n",
      "  Best iteration: 151\n",
      "  MAP@12: 0.988774\n",
      "  âœ“ Saved to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/lgb_classifier.pkl\n",
      "\n",
      "================================================================================\n",
      "Training: lgb_ranker_lambdarank\n",
      "================================================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's ndcg@1: 0.992923\ttrain's ndcg@2: 0.994029\ttrain's ndcg@3: 0.994859\ttrain's ndcg@4: 0.995419\ttrain's ndcg@5: 0.995712\tvalid's ndcg@1: 0.984436\tvalid's ndcg@2: 0.987456\tvalid's ndcg@3: 0.98821\tvalid's ndcg@4: 0.989134\tvalid's ndcg@5: 0.989886\n",
      "[100]\ttrain's ndcg@1: 0.994655\ttrain's ndcg@2: 0.995628\ttrain's ndcg@3: 0.996282\ttrain's ndcg@4: 0.996679\ttrain's ndcg@5: 0.996916\tvalid's ndcg@1: 0.984156\tvalid's ndcg@2: 0.987258\tvalid's ndcg@3: 0.988228\tvalid's ndcg@4: 0.988986\tvalid's ndcg@5: 0.989753\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttrain's ndcg@1: 0.990696\ttrain's ndcg@2: 0.992013\ttrain's ndcg@3: 0.992857\ttrain's ndcg@4: 0.993445\ttrain's ndcg@5: 0.993947\tvalid's ndcg@1: 0.985278\tvalid's ndcg@2: 0.987372\tvalid's ndcg@3: 0.988279\tvalid's ndcg@4: 0.988782\tvalid's ndcg@5: 0.989571\n",
      "\n",
      "âœ“ lgb_ranker_lambdarank trained\n",
      "  Best iteration: 20\n",
      "  MAP@12: 0.988712\n",
      "  âœ“ Saved to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/lgb_ranker_lambdarank.pkl\n",
      "\n",
      "================================================================================\n",
      "Training: lgb_ranker_xendcg\n",
      "================================================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's ndcg@1: 0.988691\ttrain's ndcg@2: 0.989539\ttrain's ndcg@3: 0.990265\ttrain's ndcg@4: 0.990954\ttrain's ndcg@5: 0.99144\tvalid's ndcg@1: 0.983875\tvalid's ndcg@2: 0.985631\tvalid's ndcg@3: 0.986311\tvalid's ndcg@4: 0.987052\tvalid's ndcg@5: 0.987598\n",
      "[100]\ttrain's ndcg@1: 0.989112\ttrain's ndcg@2: 0.990206\ttrain's ndcg@3: 0.990988\ttrain's ndcg@4: 0.99157\ttrain's ndcg@5: 0.992084\tvalid's ndcg@1: 0.984016\tvalid's ndcg@2: 0.98558\tvalid's ndcg@3: 0.986562\tvalid's ndcg@4: 0.98701\tvalid's ndcg@5: 0.987692\n",
      "[150]\ttrain's ndcg@1: 0.989805\ttrain's ndcg@2: 0.990842\ttrain's ndcg@3: 0.991777\ttrain's ndcg@4: 0.992486\ttrain's ndcg@5: 0.992896\tvalid's ndcg@1: 0.983735\tvalid's ndcg@2: 0.98585\tvalid's ndcg@3: 0.986853\tvalid's ndcg@4: 0.987314\tvalid's ndcg@5: 0.987692\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttrain's ndcg@1: 0.989013\ttrain's ndcg@2: 0.989813\ttrain's ndcg@3: 0.990506\ttrain's ndcg@4: 0.991103\ttrain's ndcg@5: 0.991713\tvalid's ndcg@1: 0.984436\tvalid's ndcg@2: 0.985818\tvalid's ndcg@3: 0.986778\tvalid's ndcg@4: 0.987149\tvalid's ndcg@5: 0.987838\n",
      "\n",
      "âœ“ lgb_ranker_xendcg trained\n",
      "  Best iteration: 56\n",
      "  MAP@12: 0.986996\n",
      "  âœ“ Saved to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/lgb_ranker_xendcg.pkl\n",
      "\n",
      "================================================================================\n",
      "Training: lgb_classifier_deep\n",
      "================================================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's binary_logloss: 0.124838\tvalid's binary_logloss: 0.127286\n",
      "[100]\ttrain's binary_logloss: 0.0391808\tvalid's binary_logloss: 0.0437009\n",
      "[150]\ttrain's binary_logloss: 0.0199597\tvalid's binary_logloss: 0.0276618\n",
      "[200]\ttrain's binary_logloss: 0.013315\tvalid's binary_logloss: 0.0248073\n",
      "[250]\ttrain's binary_logloss: 0.0100665\tvalid's binary_logloss: 0.0243913\n",
      "[300]\ttrain's binary_logloss: 0.008225\tvalid's binary_logloss: 0.0245407\n",
      "Early stopping, best iteration is:\n",
      "[242]\ttrain's binary_logloss: 0.0104162\tvalid's binary_logloss: 0.0243704\n",
      "\n",
      "âœ“ lgb_classifier_deep trained\n",
      "  Best iteration: 242\n",
      "  MAP@12: 0.988739\n",
      "  âœ“ Saved to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/lgb_classifier_deep.pkl\n",
      "\n",
      "================================================================================\n",
      "TRAINING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Model Performance (MAP@12):\n",
      "  lgb_classifier                : 0.988774 (iter: 151)\n",
      "  lgb_classifier_deep           : 0.988739 (iter: 242)\n",
      "  lgb_ranker_lambdarank         : 0.988712 (iter: 20)\n",
      "  lgb_ranker_xendcg             : 0.986996 (iter: 56)\n",
      "\n",
      "ðŸ† Best Model: lgb_classifier (MAP@12: 0.988774)\n",
      "\n",
      " Saved model metadata to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/lgb_models_metadata.json\n",
      "\n",
      " All models trained and saved!\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MULTIPLE LIGHTGBM MODELS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING LIGHTGBM MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Convert categorical features to numeric codes (LightGBM requirement)\n",
    "# LightGBM doesn't accept object dtype, so we convert to categorical codes\n",
    "print(\"\\nConverting categorical features to numeric codes...\")\n",
    "for col in categorical_features:\n",
    "    if col in X_train.columns:\n",
    "        # Convert to categorical and then to codes (integers)\n",
    "        # This ensures consistent encoding between train and val\n",
    "        # First, get all unique values from both train and val\n",
    "        all_values = pd.concat([X_train[col], X_val[col]]).unique()\n",
    "        # Create categorical with all possible values\n",
    "        train_cat = pd.Categorical(X_train[col], categories=sorted(all_values))\n",
    "        val_cat = pd.Categorical(X_val[col], categories=sorted(all_values))\n",
    "        # Convert to integer codes\n",
    "        X_train[col] = train_cat.codes.astype('int32')\n",
    "        X_val[col] = val_cat.codes.astype('int32')\n",
    "        # Replace -1 (missing values) with a valid code (use max code + 1)\n",
    "        if (X_train[col] == -1).any() or (X_val[col] == -1).any():\n",
    "            max_code = max(X_train[col].max(), X_val[col].max())\n",
    "            X_train[col] = X_train[col].replace(-1, max_code + 1)\n",
    "            X_val[col] = X_val[col].replace(-1, max_code + 1)\n",
    "print(f\" Converted {len(categorical_features)} categorical features to numeric codes\")\n",
    "\n",
    "# Create LightGBM datasets\n",
    "print(\"\\n Creating LightGBM datasets...\")\n",
    "train_dataset = lgb.Dataset(\n",
    "    X_train, \n",
    "    label=y_train,\n",
    "    categorical_feature=categorical_features,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "val_dataset = lgb.Dataset(\n",
    "    X_val,\n",
    "    label=y_val,\n",
    "    categorical_feature=categorical_features,\n",
    "    reference=train_dataset,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "print(\" LightGBM datasets created\")\n",
    "\n",
    "# Model configurations\n",
    "models_config = {\n",
    "    'lgb_classifier': {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': LightGBMConfig.RANDOM_STATE,\n",
    "        'force_col_wise': True,\n",
    "    },\n",
    "    'lgb_ranker_lambdarank': {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': LightGBMConfig.RANDOM_STATE,\n",
    "        'force_col_wise': True,\n",
    "        'label_gain': [0, 1],  # 0 for negative, 1 for positive\n",
    "    },\n",
    "    'lgb_ranker_xendcg': {\n",
    "        'objective': 'rank_xendcg',\n",
    "        'metric': 'ndcg',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 63,\n",
    "        'learning_rate': 0.03,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': LightGBMConfig.RANDOM_STATE,\n",
    "        'force_col_wise': True,\n",
    "    },\n",
    "    'lgb_classifier_deep': {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 15,\n",
    "        'learning_rate': 0.03,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'verbose': -1,\n",
    "        'seed': LightGBMConfig.RANDOM_STATE,\n",
    "        'force_col_wise': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# For ranking models, we need to provide group information\n",
    "# Group by customer_id (each customer is a query group)\n",
    "print(\"\\nPreparing group information for ranking models...\")\n",
    "train_groups = train_customer_ids.value_counts().sort_index().values\n",
    "val_groups = val_customer_ids.value_counts().sort_index().values\n",
    "\n",
    "# Sort data by customer_id to match group order\n",
    "train_sort_idx = train_customer_ids.argsort()\n",
    "val_sort_idx = val_customer_ids.argsort()\n",
    "\n",
    "X_train_sorted = X_train.iloc[train_sort_idx].reset_index(drop=True)\n",
    "y_train_sorted = y_train.iloc[train_sort_idx].reset_index(drop=True)\n",
    "train_customer_ids_sorted = train_customer_ids.iloc[train_sort_idx].reset_index(drop=True)\n",
    "train_article_ids_sorted = train_article_ids.iloc[train_sort_idx].reset_index(drop=True)\n",
    "\n",
    "X_val_sorted = X_val.iloc[val_sort_idx].reset_index(drop=True)\n",
    "y_val_sorted = y_val.iloc[val_sort_idx].reset_index(drop=True)\n",
    "val_customer_ids_sorted = val_customer_ids.iloc[val_sort_idx].reset_index(drop=True)\n",
    "val_article_ids_sorted = val_article_ids.iloc[val_sort_idx].reset_index(drop=True)\n",
    "\n",
    "# Create ranking datasets with groups\n",
    "train_ranking_dataset = lgb.Dataset(\n",
    "    X_train_sorted,\n",
    "    label=y_train_sorted,\n",
    "    categorical_feature=categorical_features,\n",
    "    group=train_groups,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "val_ranking_dataset = lgb.Dataset(\n",
    "    X_val_sorted,\n",
    "    label=y_val_sorted,\n",
    "    categorical_feature=categorical_features,\n",
    "    group=val_groups,\n",
    "    reference=train_ranking_dataset,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "print(\" Ranking datasets created with group information\")\n",
    "\n",
    "# Train models\n",
    "trained_models = {}\n",
    "model_predictions = {}\n",
    "model_scores = {}\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for model_name, params in models_config.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Select appropriate dataset\n",
    "    if 'ranker' in model_name:\n",
    "        train_ds = train_ranking_dataset\n",
    "        val_ds = val_ranking_dataset\n",
    "        use_sorted = True\n",
    "    else:\n",
    "        train_ds = train_dataset\n",
    "        val_ds = val_dataset\n",
    "        use_sorted = False\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_ds,\n",
    "        num_boost_round=LightGBMConfig.N_ESTIMATORS,\n",
    "        valid_sets=[train_ds, val_ds],\n",
    "        valid_names=['train', 'valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=LightGBMConfig.EARLY_STOPPING_ROUNDS, verbose=True),\n",
    "            lgb.log_evaluation(period=LightGBMConfig.VERBOSE_EVAL)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    if use_sorted:\n",
    "        predictions = model.predict(X_val_sorted, num_iteration=model.best_iteration)\n",
    "        # Revert to original order\n",
    "        val_revert_idx = val_sort_idx.argsort()\n",
    "        predictions = predictions[val_revert_idx]\n",
    "        eval_df = val_data.copy()\n",
    "    else:\n",
    "        predictions = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        eval_df = val_data.copy()\n",
    "    \n",
    "    # Calculate MAP@12\n",
    "    map_score = evaluate_map_at_12(eval_df, predictions)\n",
    "    \n",
    "    # Store model and results\n",
    "    trained_models[model_name] = model\n",
    "    model_predictions[model_name] = predictions\n",
    "    model_scores[model_name] = {\n",
    "        'map_at_12': map_score,\n",
    "        'best_iteration': model.best_iteration\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ“ {model_name} trained\")\n",
    "    print(f\"  Best iteration: {model.best_iteration}\")\n",
    "    print(f\"  MAP@12: {map_score:.6f}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = LightGBMConfig.MODEL_PATH / f'{model_name}.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"  âœ“ Saved to {model_path}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nModel Performance (MAP@12):\")\n",
    "for model_name, scores in sorted(model_scores.items(), key=lambda x: x[1]['map_at_12'], reverse=True):\n",
    "    print(f\"  {model_name:30s}: {scores['map_at_12']:.6f} (iter: {scores['best_iteration']})\")\n",
    "\n",
    "best_model_name = max(model_scores.items(), key=lambda x: x[1]['map_at_12'])[0]\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name} (MAP@12: {model_scores[best_model_name]['map_at_12']:.6f})\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'models': {name: scores for name, scores in model_scores.items()},\n",
    "    'best_model': best_model_name,\n",
    "    'feature_columns': feature_cols,\n",
    "    'categorical_features': categorical_features,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(LightGBMConfig.MODEL_PATH / 'lgb_models_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n Saved model metadata to {LightGBMConfig.MODEL_PATH / 'lgb_models_metadata.json'}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n All models trained and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb2cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Top 20 Most Important Features (lgb_classifier):\n",
      "--------------------------------------------------------------------------------\n",
      "  has_purchased_item                      :    5,256,895\n",
      "  days_since_item_purchase                :    1,416,434\n",
      "  repurchase_score                        :      336,317\n",
      "  userknn_score_rank                      :      155,513\n",
      "  repurchase_score_rank                   :       45,798\n",
      "  n_unique_articles                       :       30,044\n",
      "  popularity_score_y                      :       20,867\n",
      "  days_since_last_purchase                :       15,409\n",
      "  copurchase_score                        :       10,176\n",
      "  category_score                          :        8,592\n",
      "  department_no                           :        4,842\n",
      "  user_item_text_similarity               :        4,367\n",
      "  semantic_range                          :        3,460\n",
      "  exploration_rate                        :        3,210\n",
      "  std_price                               :        3,196\n",
      "  n_strategies                            :        3,094\n",
      "  n_unique_categories                     :        3,074\n",
      "  purchase_frequency                      :        2,423\n",
      "  semantic_diversity                      :        2,254\n",
      "  n_purchases                             :        2,203\n",
      "\n",
      "âœ“ Saved feature importance to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/feature_importance.csv\n",
      "\n",
      "Low importance features (< 0.001): 41\n",
      "  Examples: ['section_name_count', 'index_group_no', 'image_emb_510', 'is_cheaper_than_usual', 'has_copurchase_signal', 'is_active_last_week', 'perceived_colour_value_id', 'garment_group_name_count', 'section_name_frequency', 'garment_group_name_frequency']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze feature importance for the best model\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': best_model.feature_importance(importance_type='gain')\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 20 Most Important Features ({best_model_name}):\")\n",
    "print(\"-\"*80)\n",
    "for idx, row in feature_importance.head(20).iterrows():\n",
    "    print(f\"  {row['feature']:40s}: {row['importance']:>12,.0f}\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = LightGBMConfig.MODEL_PATH / 'feature_importance.csv'\n",
    "feature_importance.to_csv(importance_path, index=False)\n",
    "print(f\"\\nâœ“ Saved feature importance to {importance_path}\")\n",
    "\n",
    "# Identify low-importance features\n",
    "low_importance_features = feature_importance[\n",
    "    feature_importance['importance'] < LightGBMConfig.MIN_FEATURE_IMPORTANCE\n",
    "]['feature'].tolist()\n",
    "\n",
    "print(f\"\\nLow importance features (< {LightGBMConfig.MIN_FEATURE_IMPORTANCE}): {len(low_importance_features)}\")\n",
    "if len(low_importance_features) > 0:\n",
    "    print(f\"  Examples: {low_importance_features[:10]}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd2edad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING ENSEMBLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Normalizing predictions...\n",
      "  lgb_classifier: [0.0000, 0.9993] -> [0, 1]\n",
      "  lgb_ranker_lambdarank: [-1.3156, 1.3233] -> [0, 1]\n",
      "  lgb_ranker_xendcg: [-0.6435, 0.8475] -> [0, 1]\n",
      "  lgb_classifier_deep: [0.0001, 0.9992] -> [0, 1]\n",
      "\n",
      "Ensemble strategies:\n",
      "\n",
      "  equal_weight:\n",
      "    lgb_classifier                : 0.2500\n",
      "    lgb_ranker_lambdarank         : 0.2500\n",
      "    lgb_ranker_xendcg             : 0.2500\n",
      "    lgb_classifier_deep           : 0.2500\n",
      "\n",
      "  performance_weight:\n",
      "    lgb_classifier                : 0.2501\n",
      "    lgb_ranker_lambdarank         : 0.2501\n",
      "    lgb_ranker_xendcg             : 0.2497\n",
      "    lgb_classifier_deep           : 0.2501\n",
      "\n",
      "  best_only:\n",
      "    lgb_classifier                : 1.0000\n",
      "\n",
      "  equal_weight             : MAP@12 = 0.988396\n",
      "\n",
      "  performance_weight       : MAP@12 = 0.988396\n",
      "\n",
      "  best_only                : MAP@12 = 0.988774\n",
      "\n",
      " Best Ensemble: best_only (MAP@12: 0.988774)\n",
      "\n",
      " Saved ensemble predictions to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/ensemble_predictions_val.parquet\n",
      " Saved ensemble metadata to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/ensemble_metadata.json\n",
      "\n",
      " Ensemble predictions created!\n"
     ]
    }
   ],
   "source": [
    "# ENSEMBLE PREDICTIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING ENSEMBLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Normalize predictions to [0, 1] range for ensemble\n",
    "print(\"\\nNormalizing predictions...\")\n",
    "normalized_predictions = {}\n",
    "for model_name, preds in model_predictions.items():\n",
    "    # Min-max normalization\n",
    "    min_pred = preds.min()\n",
    "    max_pred = preds.max()\n",
    "    if max_pred > min_pred:\n",
    "        normalized = (preds - min_pred) / (max_pred - min_pred)\n",
    "    else:\n",
    "        normalized = preds\n",
    "    normalized_predictions[model_name] = normalized\n",
    "    print(f\"  {model_name}: [{preds.min():.4f}, {preds.max():.4f}] -> [0, 1]\")\n",
    "\n",
    "# Create different ensemble strategies\n",
    "ensemble_strategies = {\n",
    "    'equal_weight': {name: 1.0/len(normalized_predictions) for name in normalized_predictions.keys()},\n",
    "    'performance_weight': {},  # Will be calculated based on MAP@12 scores\n",
    "    'best_only': {best_model_name: 1.0}\n",
    "}\n",
    "\n",
    "# Calculate performance-based weights (proportional to MAP@12 score)\n",
    "total_map = sum(scores['map_at_12'] for scores in model_scores.values())\n",
    "for model_name in normalized_predictions.keys():\n",
    "    ensemble_strategies['performance_weight'][model_name] = model_scores[model_name]['map_at_12'] / total_map\n",
    "\n",
    "print(\"\\nEnsemble strategies:\")\n",
    "for strategy_name, weights in ensemble_strategies.items():\n",
    "    print(f\"\\n  {strategy_name}:\")\n",
    "    for model_name, weight in weights.items():\n",
    "        print(f\"    {model_name:30s}: {weight:.4f}\")\n",
    "\n",
    "# Create ensemble predictions\n",
    "ensemble_predictions = {}\n",
    "ensemble_scores = {}\n",
    "\n",
    "for strategy_name, weights in ensemble_strategies.items():\n",
    "    # Weighted average\n",
    "    ensemble_pred = np.zeros(len(val_data))\n",
    "    for model_name, weight in weights.items():\n",
    "        ensemble_pred += weight * normalized_predictions[model_name]\n",
    "    \n",
    "    ensemble_predictions[strategy_name] = ensemble_pred\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    map_score = evaluate_map_at_12(val_data, ensemble_pred)\n",
    "    ensemble_scores[strategy_name] = map_score\n",
    "    \n",
    "    print(f\"\\n  {strategy_name:25s}: MAP@12 = {map_score:.6f}\")\n",
    "\n",
    "# Find best ensemble\n",
    "best_ensemble_name = max(ensemble_scores.items(), key=lambda x: x[1])[0]\n",
    "print(f\"\\n Best Ensemble: {best_ensemble_name} (MAP@12: {ensemble_scores[best_ensemble_name]:.6f})\")\n",
    "\n",
    "# Save ensemble predictions\n",
    "ensemble_path = LightGBMConfig.MODEL_PATH / 'ensemble_predictions_val.parquet'\n",
    "val_data_with_preds = val_data.copy()\n",
    "val_data_with_preds['ensemble_pred'] = ensemble_predictions[best_ensemble_name]\n",
    "val_data_with_preds.to_parquet(ensemble_path, index=False)\n",
    "print(f\"\\n Saved ensemble predictions to {ensemble_path}\")\n",
    "\n",
    "# Save ensemble metadata\n",
    "ensemble_metadata = {\n",
    "    'strategies': ensemble_strategies,\n",
    "    'scores': ensemble_scores,\n",
    "    'best_ensemble': best_ensemble_name,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(LightGBMConfig.MODEL_PATH / 'ensemble_metadata.json', 'w') as f:\n",
    "    json.dump(ensemble_metadata, f, indent=2)\n",
    "\n",
    "print(f\" Saved ensemble metadata to {LightGBMConfig.MODEL_PATH / 'ensemble_metadata.json'}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n Ensemble predictions created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LIGHTGBM TRAINING - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      " Model Performance:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                               MAP@12          Best Iteration \n",
      "--------------------------------------------------------------------------------\n",
      "lgb_classifier                      0.989001        146            \n",
      "lgb_classifier_deep                 0.988969        242            \n",
      "lgb_ranker_lambdarank               0.988961        34             \n",
      "lgb_ranker_xendcg                   0.987259        4              \n",
      "\n",
      " Ensemble Performance:\n",
      "--------------------------------------------------------------------------------\n",
      "  best_only                : 0.989001\n",
      "  equal_weight             : 0.988874\n",
      "  performance_weight       : 0.988874\n",
      "\n",
      " Best Single Model: lgb_classifier\n",
      "   MAP@12: 0.989001\n",
      "\n",
      " Best Ensemble: best_only\n",
      "   MAP@12: 0.989001\n",
      "\n",
      "ðŸ“ˆ Ensemble Improvement: +0.000000 (+0.00%)\n",
      "\n",
      "ðŸ’¾ Saved Files:\n",
      "--------------------------------------------------------------------------------\n",
      "  Models: 4 models saved to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models\n",
      "  Feature Importance: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/feature_importance.csv\n",
      "  Model Metadata: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/lgb_models_metadata.json\n",
      "  Ensemble Metadata: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/ensemble_metadata.json\n",
      "  Ensemble Predictions: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/ensemble_predictions_val.parquet\n",
      "\n",
      " Step 1 Complete: LightGBM Training & Reranking\n",
      "   Ready for Step 2: Neural Towers Training\n"
     ]
    }
   ],
   "source": [
    "# FINAL SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIGHTGBM TRAINING - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n Model Performance:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Model':<35} {'MAP@12':<15} {'Best Iteration':<15}\")\n",
    "print(\"-\"*80)\n",
    "for model_name, scores in sorted(model_scores.items(), key=lambda x: x[1]['map_at_12'], reverse=True):\n",
    "    print(f\"{model_name:<35} {scores['map_at_12']:<15.6f} {scores['best_iteration']:<15}\")\n",
    "\n",
    "print(\"\\n Ensemble Performance:\")\n",
    "print(\"-\"*80)\n",
    "for strategy_name, score in sorted(ensemble_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {strategy_name:25s}: {score:.6f}\")\n",
    "\n",
    "print(f\"\\n Best Single Model: {best_model_name}\")\n",
    "print(f\"   MAP@12: {model_scores[best_model_name]['map_at_12']:.6f}\")\n",
    "\n",
    "print(f\"\\n Best Ensemble: {best_ensemble_name}\")\n",
    "print(f\"   MAP@12: {ensemble_scores[best_ensemble_name]:.6f}\")\n",
    "\n",
    "improvement = ensemble_scores[best_ensemble_name] - model_scores[best_model_name]['map_at_12']\n",
    "improvement_pct = (improvement / model_scores[best_model_name]['map_at_12']) * 100\n",
    "print(f\"\\nðŸ“ˆ Ensemble Improvement: {improvement:+.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved Files:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  Models: {len(trained_models)} models saved to {LightGBMConfig.MODEL_PATH}\")\n",
    "print(f\"  Feature Importance: {LightGBMConfig.MODEL_PATH / 'feature_importance.csv'}\")\n",
    "print(f\"  Model Metadata: {LightGBMConfig.MODEL_PATH / 'lgb_models_metadata.json'}\")\n",
    "print(f\"  Ensemble Metadata: {LightGBMConfig.MODEL_PATH / 'ensemble_metadata.json'}\")\n",
    "print(f\"  Ensemble Predictions: {LightGBMConfig.MODEL_PATH / 'ensemble_predictions_val.parquet'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abccf5e",
   "metadata": {},
   "source": [
    "### Stage 4B: Neural Towers Training\n",
    "\n",
    "This stage implements a three-tower neural network architecture for recommendation:\n",
    "- **User Tower**: Processes user-level features (purchase history, preferences, demographics)\n",
    "- **Item Tower**: Processes item-level features (product attributes, popularity, sales)\n",
    "- **Image Tower**: Processes image embeddings (visual features from FashionCLIP)\n",
    "- **Fusion Layer**: Combines all three towers with MLP layers\n",
    "- **Output**: Binary classification score for purchase prediction\n",
    "\n",
    "**Key Features:**\n",
    "- MPS acceleration for Apple Silicon (M4)\n",
    "- MAP@12 evaluation during training\n",
    "- Early stopping and model checkpointing\n",
    "- Batch processing optimized for memory\n",
    "- Learning rate scheduling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dca7e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "âœ“ Neural Tower Configuration loaded\n",
      "  Model path: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models\n",
      "  Device: mps\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND CONFIGURATION FOR NEURAL TOWERS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# WEIGHTED BCE LOSS FOR CLASS IMBALANCE\n",
    "# ============================================================================\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Weighted Binary Cross-Entropy Loss\n",
    "    Useful for handling class imbalance\n",
    "    MPS-compatible: Uses float32 explicitly\n",
    "    Manually implements pos_weight for compatibility with all PyTorch versions\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate standard BCE loss\n",
    "        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        if self.pos_weight is not None:\n",
    "            # Create pos_weight tensor with float32 dtype for MPS compatibility\n",
    "            if isinstance(self.pos_weight, (int, float)):\n",
    "                pos_weight_val = float(self.pos_weight)\n",
    "            else:\n",
    "                pos_weight_val = float(self.pos_weight.item()) if hasattr(self.pos_weight, 'item') else float(self.pos_weight)\n",
    "            \n",
    "            # Apply pos_weight: multiply positive class losses by pos_weight\n",
    "            # Positive samples (targets == 1) get weighted by pos_weight\n",
    "            # Negative samples (targets == 0) remain unchanged\n",
    "            weights = torch.where(targets == 1, \n",
    "                                 torch.tensor(pos_weight_val, device=inputs.device, dtype=torch.float32),\n",
    "                                 torch.tensor(1.0, device=inputs.device, dtype=torch.float32))\n",
    "            \n",
    "            weighted_loss = bce_loss * weights\n",
    "            return weighted_loss.mean()\n",
    "        else:\n",
    "            return bce_loss.mean()\n",
    "\n",
    "# Configuration\n",
    "class NeuralTowerConfig:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    MODEL_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models')\n",
    "    \n",
    "    MODEL_PATH.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Device configuration\n",
    "    if torch.backends.mps.is_available():\n",
    "        DEVICE = torch.device('mps')\n",
    "        print(\"Using Apple Silicon GPU (MPS)\")\n",
    "    else:\n",
    "        DEVICE = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    # Training configuration (OPTIMIZED FOR BETTER PERFORMANCE & RANKING)\n",
    "    BATCH_SIZE = 4096  # Larger batches for more stable gradients\n",
    "    N_EPOCHS = 30  # Reduced from 40 to prevent overfitting\n",
    "    LEARNING_RATE = 3e-4  # Further reduced for more stable training\n",
    "    WEIGHT_DECAY = 2e-4  # Increased regularization\n",
    "    EARLY_STOPPING_PATIENCE = 5  # Reduced patience to stop earlier\n",
    "    VALIDATION_FREQ = 1  # Validate every epoch\n",
    "    \n",
    "    # Model architecture (OPTIMIZED TO REDUCE OVERFITTING & IMPROVE RANKING)\n",
    "    USER_EMBEDDING_DIM = 96  # Reduced from 128 to prevent overfitting\n",
    "    ITEM_EMBEDDING_DIM = 48  # Reduced from 64\n",
    "    IMAGE_EMBEDDING_DIM = 96  # Reduced from 128\n",
    "    FUSION_HIDDEN_DIMS = [192, 96, 48]  # Smaller fusion layers\n",
    "    DROPOUT_RATE = 0.5  # Increased from 0.4 for stronger regularization\n",
    "    \n",
    "    # Feature groups (will be determined from data)\n",
    "    USER_FEATURE_PREFIXES = ['n_', 'avg_', 'std_', 'min_', 'max_', 'days_', 'purchase_', \n",
    "                             'exploration_', 'age', 'FN', 'Active', 'unique_']\n",
    "    ITEM_FEATURE_PREFIXES = ['product_', 'graphical_', 'colour_', 'perceived_', 'department_',\n",
    "                            'index_', 'section_', 'garment_', 'popularity_', 'sales_', 'buyers_']\n",
    "    IMAGE_FEATURE_PREFIXES = ['image_emb_']\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "print(\"âœ“ Neural Tower Configuration loaded\")\n",
    "print(f\"  Model path: {NeuralTowerConfig.MODEL_PATH}\")\n",
    "print(f\"  Device: {NeuralTowerConfig.DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e6cfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ThreeTowerModel class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# THREE-TOWER NEURAL NETWORK MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class ThreeTowerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Three-tower neural network for recommendation:\n",
    "    - User Tower: User features -> User embedding\n",
    "    - Item Tower: Item features -> Item embedding  \n",
    "    - Image Tower: Image embeddings -> Image embedding\n",
    "    - Fusion: Concatenated embeddings -> Final prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 user_feature_dim,\n",
    "                 item_feature_dim,\n",
    "                 image_feature_dim,\n",
    "                 user_embedding_dim=128,\n",
    "                 item_embedding_dim=64,\n",
    "                 image_embedding_dim=128,\n",
    "                 fusion_hidden_dims=[256, 128, 64],\n",
    "                 dropout_rate=0.3):\n",
    "        super(ThreeTowerModel, self).__init__()\n",
    "        \n",
    "        # User Tower\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, user_embedding_dim),\n",
    "            nn.BatchNorm1d(user_embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Item Tower\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(item_feature_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, item_embedding_dim),\n",
    "            nn.BatchNorm1d(item_embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Image Tower\n",
    "        self.image_tower = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, image_embedding_dim),\n",
    "            nn.BatchNorm1d(image_embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fusion Layer\n",
    "        fusion_input_dim = user_embedding_dim + item_embedding_dim + image_embedding_dim\n",
    "        fusion_layers = []\n",
    "        \n",
    "        prev_dim = fusion_input_dim\n",
    "        for hidden_dim in fusion_hidden_dims:\n",
    "            fusion_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        fusion_layers.append(nn.Linear(prev_dim, 1))\n",
    "        fusion_layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.fusion = nn.Sequential(*fusion_layers)\n",
    "        \n",
    "    def forward(self, user_features, item_features, image_features):\n",
    "        # Pass through towers\n",
    "        user_emb = self.user_tower(user_features)\n",
    "        item_emb = self.item_tower(item_features)\n",
    "        image_emb = self.image_tower(image_features)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        fused = torch.cat([user_emb, item_emb, image_emb], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.fusion(fused)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "print(\"âœ“ ThreeTowerModel class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3613a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RecommendationDataset class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class RecommendationDataset(Dataset):\n",
    "    \"\"\"Dataset for recommendation training\"\"\"\n",
    "    \n",
    "    def __init__(self, df, user_features, item_features, image_features, labels=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.user_features = user_features.values.astype(np.float32)\n",
    "        self.item_features = item_features.values.astype(np.float32)\n",
    "        self.image_features = image_features.values.astype(np.float32)\n",
    "        self.labels = labels.values.astype(np.float32) if labels is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_feat = torch.FloatTensor(self.user_features[idx])\n",
    "        item_feat = torch.FloatTensor(self.item_features[idx])\n",
    "        image_feat = torch.FloatTensor(self.image_features[idx])\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = torch.FloatTensor([self.labels[idx]])\n",
    "            return user_feat, item_feat, image_feat, label\n",
    "        else:\n",
    "            return user_feat, item_feat, image_feat\n",
    "\n",
    "print(\" RecommendationDataset class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c78237f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATA FOR NEURAL TOWERS\n",
      "================================================================================\n",
      "\n",
      "Loading train_data.parquet...\n",
      "âœ“ Loaded 685,300 training samples\n",
      "\n",
      "Loading val_data.parquet...\n",
      "âœ“ Loaded 121,164 validation samples\n",
      "\n",
      "Loading test_data.parquet...\n",
      "âœ“ Loaded 36,379 test samples\n",
      "\n",
      "âœ“ Feature separation:\n",
      "  User features: 43\n",
      "  Item features: 29\n",
      "  Image features: 512\n",
      "  Total features: 584\n",
      "\n",
      "Filling missing values...\n",
      "âœ“ Missing values filled\n",
      "\n",
      "Standardizing features...\n",
      "âœ“ Test features standardized\n",
      " Features standardized\n",
      "\n",
      " Feature matrices prepared:\n",
      "  User: 43 features\n",
      "  Item: 29 features\n",
      "  Image: 512 features\n",
      "\n",
      " Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA AND PREPARE FEATURES FOR NEURAL TOWERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA FOR NEURAL TOWERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load train and validation datasets\n",
    "print(\"\\nLoading train_data.parquet...\")\n",
    "train_data = pd.read_parquet(NeuralTowerConfig.MODEL_PATH / 'train_data.parquet')\n",
    "print(f\"âœ“ Loaded {len(train_data):,} training samples\")\n",
    "\n",
    "print(\"\\nLoading val_data.parquet...\")\n",
    "val_data = pd.read_parquet(NeuralTowerConfig.MODEL_PATH / 'val_data.parquet')\n",
    "print(f\"âœ“ Loaded {len(val_data):,} validation samples\")\n",
    "\n",
    "# Load or create test dataset\n",
    "test_data_path = Path(\"/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/test_data.parquet\")\n",
    "if test_data_path.exists():\n",
    "    print(\"\\nLoading test_data.parquet...\")\n",
    "    test_data = pd.read_parquet(test_data_path)\n",
    "    print(f\"âœ“ Loaded {len(test_data):,} test samples\")\n",
    "    use_test_set = True\n",
    "else:\n",
    "    print(\"\\nâš ï¸  test_data.parquet not found. Creating test set from validation data...\")\n",
    "    print(\"   (Splitting validation data: % validation, 30% test)\")\n",
    "    # Split validation data into validation and test (95/5 split)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    val_users = val_data['customer_id'].unique()\n",
    "    val_users_train, val_users_test = train_test_split(\n",
    "        val_users, \n",
    "        test_size=0.3, \n",
    "        random_state=NeuralTowerConfig.RANDOM_STATE\n",
    "    )\n",
    "    test_data = val_data[val_data['customer_id'].isin(val_users_test)].copy()\n",
    "    val_data = val_data[val_data['customer_id'].isin(val_users_train)].copy()\n",
    "    print(f\"âœ“ Created test set: {len(test_data):,} samples\")\n",
    "    print(f\"âœ“ Validation set (after split): {len(val_data):,} samples\")\n",
    "    # Save test data for future use\n",
    "    test_data.to_parquet(test_data_path, index=False)\n",
    "    print(f\"âœ“ Saved test set to {test_data_path}\")\n",
    "    use_test_set = True\n",
    "\n",
    "# Identify feature groups\n",
    "exclude_cols = ['customer_id', 'article_id', 'label', 'user_type', 'train_label', 'val_label']\n",
    "all_feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "\n",
    "# Separate features into user, item, and image\n",
    "user_feature_cols = []\n",
    "item_feature_cols = []\n",
    "image_feature_cols = []\n",
    "\n",
    "for col in all_feature_cols:\n",
    "    if any(col.startswith(prefix) for prefix in NeuralTowerConfig.IMAGE_FEATURE_PREFIXES):\n",
    "        image_feature_cols.append(col)\n",
    "    elif any(col.startswith(prefix) for prefix in NeuralTowerConfig.USER_FEATURE_PREFIXES):\n",
    "        user_feature_cols.append(col)\n",
    "    elif any(col.startswith(prefix) for prefix in NeuralTowerConfig.ITEM_FEATURE_PREFIXES):\n",
    "        item_feature_cols.append(col)\n",
    "    else:\n",
    "        # Default to user features if unclear\n",
    "        user_feature_cols.append(col)\n",
    "\n",
    "print(f\"\\nâœ“ Feature separation:\")\n",
    "print(f\"  User features: {len(user_feature_cols)}\")\n",
    "print(f\"  Item features: {len(item_feature_cols)}\")\n",
    "print(f\"  Image features: {len(image_feature_cols)}\")\n",
    "print(f\"  Total features: {len(all_feature_cols)}\")\n",
    "\n",
    "# Check for missing image features\n",
    "if len(image_feature_cols) == 0:\n",
    "    print(\"\\n Warning: No image features found!\")\n",
    "    print(\"  Creating dummy image features...\")\n",
    "    # Create dummy image features (zeros)\n",
    "    for i in range(52):  # 52 dimensions\n",
    "        col_name = f'image_emb_{i}'\n",
    "        train_data[col_name] = 0.0\n",
    "        val_data[col_name] = 0.0\n",
    "        image_feature_cols.append(col_name)\n",
    "    print(f\"  âœ“ Created {len(image_feature_cols)} dummy image features\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train_user = train_data[user_feature_cols].copy()\n",
    "X_train_item = train_data[item_feature_cols].copy()\n",
    "X_train_image = train_data[image_feature_cols].copy()\n",
    "y_train = train_data['label'].copy()\n",
    "\n",
    "X_val_user = val_data[user_feature_cols].copy()\n",
    "X_val_item = val_data[item_feature_cols].copy()\n",
    "X_val_image = val_data[image_feature_cols].copy()\n",
    "y_val = val_data['label'].copy()\n",
    "\n",
    "# Prepare test features if test set exists\n",
    "if use_test_set:\n",
    "    # Check for missing image features in test data\n",
    "    if len(image_feature_cols) > 0:\n",
    "        for col in image_feature_cols:\n",
    "            if col not in test_data.columns:\n",
    "                test_data[col] = 0.0\n",
    "    \n",
    "    X_test_user = test_data[user_feature_cols].copy()\n",
    "    X_test_item = test_data[item_feature_cols].copy()\n",
    "    X_test_image = test_data[image_feature_cols].copy()\n",
    "    y_test = test_data['label'].copy()\n",
    "    \n",
    "    # Store test customer and article IDs for evaluation\n",
    "    test_customer_ids = test_data['customer_id'].copy()\n",
    "    test_article_ids = test_data['article_id'].copy()\n",
    "else:\n",
    "    X_test_user = None\n",
    "    X_test_item = None\n",
    "    X_test_image = None\n",
    "    y_test = None\n",
    "    test_customer_ids = None\n",
    "    test_article_ids = None\n",
    "\n",
    "# Fill any missing values\n",
    "print(\"\\nFilling missing values...\")\n",
    "# Handle categorical columns separately - convert to numeric first\n",
    "dataframes_to_fill = [X_train_user, X_train_item, X_train_image, X_val_user, X_val_item, X_val_image]\n",
    "if use_test_set:\n",
    "    dataframes_to_fill.extend([X_test_user, X_test_item, X_test_image])\n",
    "\n",
    "for df in dataframes_to_fill:\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            # Convert categorical to numeric codes\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "            # Replace -1 (missing) with 0\n",
    "            df[col] = df[col].replace(-1, 0)\n",
    "        else:\n",
    "            # Fill numeric columns\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "print(\"âœ“ Missing values filled\")\n",
    "\n",
    "# Standardize features (important for neural networks)\n",
    "print(\"\\nStandardizing features...\")\n",
    "scaler_user = StandardScaler()\n",
    "scaler_item = StandardScaler()\n",
    "scaler_image = StandardScaler()\n",
    "\n",
    "X_train_user_scaled = pd.DataFrame(\n",
    "    scaler_user.fit_transform(X_train_user),\n",
    "    columns=user_feature_cols,\n",
    "    index=X_train_user.index\n",
    ")\n",
    "X_val_user_scaled = pd.DataFrame(\n",
    "    scaler_user.transform(X_val_user),\n",
    "    columns=user_feature_cols,\n",
    "    index=X_val_user.index\n",
    ")\n",
    "\n",
    "X_train_item_scaled = pd.DataFrame(\n",
    "    scaler_item.fit_transform(X_train_item),\n",
    "    columns=item_feature_cols,\n",
    "    index=X_train_item.index\n",
    ")\n",
    "X_val_item_scaled = pd.DataFrame(\n",
    "    scaler_item.transform(X_val_item),\n",
    "    columns=item_feature_cols,\n",
    "    index=X_val_item.index\n",
    ")\n",
    "\n",
    "# Scale image features (fit on train, then transform val and test)\n",
    "X_train_image_scaled = pd.DataFrame(\n",
    "    scaler_image.fit_transform(X_train_image),\n",
    "    columns=image_feature_cols,\n",
    "    index=X_train_image.index\n",
    ")\n",
    "X_val_image_scaled = pd.DataFrame(\n",
    "    scaler_image.transform(X_val_image),\n",
    "    columns=image_feature_cols,\n",
    "    index=X_val_image.index\n",
    ")\n",
    "\n",
    "# Scale test features if test set exists (AFTER all scalers are fitted)\n",
    "if use_test_set:\n",
    "    X_test_user_scaled = pd.DataFrame(\n",
    "        scaler_user.transform(X_test_user),\n",
    "        columns=user_feature_cols,\n",
    "        index=X_test_user.index\n",
    "    )\n",
    "    X_test_item_scaled = pd.DataFrame(\n",
    "        scaler_item.transform(X_test_item),\n",
    "        columns=item_feature_cols,\n",
    "        index=X_test_item.index\n",
    "    )\n",
    "    X_test_image_scaled = pd.DataFrame(\n",
    "        scaler_image.transform(X_test_image),\n",
    "        columns=image_feature_cols,\n",
    "        index=X_test_image.index\n",
    "    )\n",
    "    print(\"âœ“ Test features standardized\")\n",
    "else:\n",
    "    X_test_user_scaled = None\n",
    "    X_test_item_scaled = None\n",
    "    X_test_image_scaled = None\n",
    "\n",
    "print(\" Features standardized\")\n",
    "\n",
    "# Store customer and article IDs for evaluation\n",
    "train_customer_ids = train_data['customer_id'].copy()\n",
    "train_article_ids = train_data['article_id'].copy()\n",
    "val_customer_ids = val_data['customer_id'].copy()\n",
    "val_article_ids = val_data['article_id'].copy()\n",
    "\n",
    "print(f\"\\n Feature matrices prepared:\")\n",
    "print(f\"  User: {X_train_user_scaled.shape[1]} features\")\n",
    "print(f\"  Item: {X_train_item_scaled.shape[1]} features\")\n",
    "print(f\"  Image: {X_train_image_scaled.shape[1]} features\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n Data loading complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acd6b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING DATA LOADERS\n",
      "================================================================================\n",
      "\n",
      " Data loaders created:\n",
      "  Train batches: 168\n",
      "  Val batches: 30\n",
      "  Test batches: 9\n",
      "  Batch size: 4096\n",
      "\n",
      " Model initialized:\n",
      "  Total parameters: 273,905\n",
      "  Trainable parameters: 273,905\n",
      "  Device: mps\n",
      "\n",
      "ðŸ“Š Calculating class weights...\n",
      "  Positive samples: 274,142\n",
      "  Negative samples: 411,158\n",
      "  Recommended pos_weight: 1.4998\n",
      "\n",
      " Model and optimizers ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE DATA LOADERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING DATA LOADERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RecommendationDataset(\n",
    "    train_data,\n",
    "    X_train_user_scaled,\n",
    "    X_train_item_scaled,\n",
    "    X_train_image_scaled,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "val_dataset = RecommendationDataset(\n",
    "    val_data,\n",
    "    X_val_user_scaled,\n",
    "    X_val_item_scaled,\n",
    "    X_val_image_scaled,\n",
    "    y_val\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=NeuralTowerConfig.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for MPS compatibility\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=NeuralTowerConfig.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Create test data loader if test set exists\n",
    "if use_test_set:\n",
    "    test_dataset = RecommendationDataset(\n",
    "        test_data,\n",
    "        X_test_user_scaled,\n",
    "        X_test_item_scaled,\n",
    "        X_test_image_scaled,\n",
    "        y_test\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=NeuralTowerConfig.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    print(f\"\\n Data loaders created:\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Val batches: {len(val_loader)}\")\n",
    "    print(f\"  Test batches: {len(test_loader)}\")\n",
    "    print(f\"  Batch size: {NeuralTowerConfig.BATCH_SIZE}\")\n",
    "else:\n",
    "    test_loader = None\n",
    "    print(f\"\\n Data loaders created:\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Val batches: {len(val_loader)}\")\n",
    "    print(f\"  Batch size: {NeuralTowerConfig.BATCH_SIZE}\")\n",
    "    print(f\"  âš ï¸  Test set not available\")\n",
    "\n",
    "# Initialize model\n",
    "model = ThreeTowerModel(\n",
    "    user_feature_dim=len(user_feature_cols),\n",
    "    item_feature_dim=len(item_feature_cols),\n",
    "    image_feature_dim=len(image_feature_cols),\n",
    "    user_embedding_dim=NeuralTowerConfig.USER_EMBEDDING_DIM,\n",
    "    item_embedding_dim=NeuralTowerConfig.ITEM_EMBEDDING_DIM,\n",
    "    image_embedding_dim=NeuralTowerConfig.IMAGE_EMBEDDING_DIM,\n",
    "    fusion_hidden_dims=NeuralTowerConfig.FUSION_HIDDEN_DIMS,\n",
    "    dropout_rate=NeuralTowerConfig.DROPOUT_RATE\n",
    ").to(NeuralTowerConfig.DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n Model initialized:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Device: {NeuralTowerConfig.DEVICE}\")\n",
    "\n",
    "# Calculate pos_weight for class imbalance\n",
    "print(\"\\nðŸ“Š Calculating class weights...\")\n",
    "pos_count = y_train.sum()\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_weight = neg_count / pos_count if pos_count > 0 else 1.5\n",
    "print(f\"  Positive samples: {pos_count:,}\")\n",
    "print(f\"  Negative samples: {neg_count:,}\")\n",
    "print(f\"  Recommended pos_weight: {pos_weight:.4f}\")\n",
    "\n",
    "# Loss and optimizer (USING WEIGHTED BCE FOR CLASS IMBALANCE)\n",
    "# Using slightly lower pos_weight to focus more on ranking quality\n",
    "criterion = WeightedBCELoss(pos_weight=pos_weight * 0.9)  # Slight reduction for better ranking\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=NeuralTowerConfig.LEARNING_RATE,\n",
    "    weight_decay=NeuralTowerConfig.WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.999)  # Standard AdamW betas\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',  # Maximize MAP@12\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n Model and optimizers ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa420b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING NEURAL TOWER MODEL\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced839266129494398792d2f62befd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bce3e323a0444a3b25732d64370bb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706c171e434e4d04a9501a46e4a44b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30:\n",
      "  Train Loss: 0.275254\n",
      "  Val Loss: 0.085832\n",
      "  Val MAP@12: 0.985713\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.985713)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35086f365dd14b1791366327b18beae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab2f950c1754cc18d0891e636edea93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b4b0ac4d5e4e0ea9c244e7f0bf9a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/30:\n",
      "  Train Loss: 0.080026\n",
      "  Val Loss: 0.048183\n",
      "  Val MAP@12: 0.986084\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.986084)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443b39ab9a8348e98f451ef3090b96b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcf037988db4398978d42fe29d9e57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39c0a58bf494acdb26826f53fec2f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/30:\n",
      "  Train Loss: 0.053084\n",
      "  Val Loss: 0.037525\n",
      "  Val MAP@12: 0.986626\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.986626)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c4ae1150264bd98e4a10796e5b2da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadf0f8898224fd79a0dd0ac94240f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9508fbefea74ff4bb0f67fe00e7e339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/30:\n",
      "  Train Loss: 0.042876\n",
      "  Val Loss: 0.033127\n",
      "  Val MAP@12: 0.987063\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.987063)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb4094d88d94b569e1a4c1dd1f00a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe306044a77d4c47a6632c2a34f6a96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355005b8c23c49b2ae9153cc71be94d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/30:\n",
      "  Train Loss: 0.038068\n",
      "  Val Loss: 0.031579\n",
      "  Val MAP@12: 0.987626\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.987626)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492b0c4980154504aeff152e91558918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e26d1424d1240d5ba361207d478890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c61f42326f409d842d3eda3709b6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/30:\n",
      "  Train Loss: 0.035547\n",
      "  Val Loss: 0.030922\n",
      "  Val MAP@12: 0.988066\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.988066)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb9d2667ed64132b3f2a097d1525a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03a8714b8f8412487701271842c8a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d9d347aa674fee8e3c0cda3ff749cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/30:\n",
      "  Train Loss: 0.034076\n",
      "  Val Loss: 0.030517\n",
      "  Val MAP@12: 0.988243\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.988243)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef4d3193ed844cd996858b5e1daeb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50170df4d8784478b4fec1400a94bac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa35cabb6257420fb307028214e5b6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/30:\n",
      "  Train Loss: 0.033293\n",
      "  Val Loss: 0.030302\n",
      "  Val MAP@12: 0.988609\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.988609)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45febf1ae76b433c95819327212ddbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746bae4c331a4e5ebb2543976e86d8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5bd3f1d43440dd9a1a084b4fa8a775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/30:\n",
      "  Train Loss: 0.032128\n",
      "  Val Loss: 0.029948\n",
      "  Val MAP@12: 0.988788\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.988788)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abff2dde380403ab923508acfedf7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84aedb6c5152491eb60f2d743fcbc110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db003948690f4aebb301cc8e409af8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/30:\n",
      "  Train Loss: 0.031864\n",
      "  Val Loss: 0.029741\n",
      "  Val MAP@12: 0.988897\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.988897)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034f4077eea049728fca5283651703f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594bfa68a70946bd83762a23f40cc72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7f5644140748df83f2253d0820950a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/30:\n",
      "  Train Loss: 0.031236\n",
      "  Val Loss: 0.029562\n",
      "  Val MAP@12: 0.989039\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.989039)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdeb8ee016b482b91e9443579827611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62279aa6ab07477296329b3c44f3f30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e3f01969b04ec6b03f8a316b659a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/30:\n",
      "  Train Loss: 0.030986\n",
      "  Val Loss: 0.029504\n",
      "  Val MAP@12: 0.989071\n",
      "  LR: 3.00e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.989071)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9b71fe4ce64f74b5158b7696026dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb90b5ab5d9e4720b3a0e15f1d975eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4e98e039d74af9bb02045b3167125e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/30:\n",
      "  Train Loss: 0.030535\n",
      "  Val Loss: 0.029422\n",
      "  Val MAP@12: 0.988977\n",
      "  LR: 3.00e-04\n",
      "  No improvement (1/5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39946cdc193c4728b8492f80b33d610a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb148a3fdc11456c8a2d9ee613989bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e7f05ffe1e490781ecfe68f9d605a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/30:\n",
      "  Train Loss: 0.030037\n",
      "  Val Loss: 0.029399\n",
      "  Val MAP@12: 0.989005\n",
      "  LR: 3.00e-04\n",
      "  No improvement (2/5)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7632162d95c413cbf2174f3c60211c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfa80cec239481b8619cca46e514a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb109068dd64c459b3b91e6316623f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/30:\n",
      "  Train Loss: 0.029793\n",
      "  Val Loss: 0.029247\n",
      "  Val MAP@12: 0.988942\n",
      "  LR: 1.50e-04\n",
      "  No improvement (3/5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb5a12abcc0422999ddf3ad5f8df706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7164999c01c74c888fe84b2551274c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce191aab71904287b37c654abaee2769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/30:\n",
      "  Train Loss: 0.029846\n",
      "  Val Loss: 0.029194\n",
      "  Val MAP@12: 0.989135\n",
      "  LR: 1.50e-04\n",
      " âœ… Saved best model (Val MAP@12: 0.989135)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6babd4e26aef4e88a66e391f3b283121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e984e27dc3c43efab6623942240a8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362d43d6bf7f46beb5cb8db7bd083a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/30:\n",
      "  Train Loss: 0.029387\n",
      "  Val Loss: 0.029155\n",
      "  Val MAP@12: 0.988792\n",
      "  LR: 1.50e-04\n",
      "  No improvement (1/5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0932da6c1e4715b0380989d0b5356e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5c0300a53c4972bcfa352cf454fe68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f9e972dad9407abee07d919dab3769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/30:\n",
      "  Train Loss: 0.029218\n",
      "  Val Loss: 0.028988\n",
      "  Val MAP@12: 0.988701\n",
      "  LR: 1.50e-04\n",
      "  No improvement (2/5)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b77e5d40e6742fbb74cf5da24f5c9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8572121bbe41adadbfb0c1dbfe4c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32fffe4f2074b78982db33c9df67996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/30:\n",
      "  Train Loss: 0.028712\n",
      "  Val Loss: 0.029210\n",
      "  Val MAP@12: 0.988896\n",
      "  LR: 7.50e-05\n",
      "  No improvement (3/5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1392deb1451041b68397987df1f0dd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63f5e85428b4251919a43a29db5db1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195bbef2d730442f8ce44a4fcf82fa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/30:\n",
      "  Train Loss: 0.028772\n",
      "  Val Loss: 0.029154\n",
      "  Val MAP@12: 0.988956\n",
      "  LR: 7.50e-05\n",
      "  No improvement (4/5)\n",
      "  ðŸ“Š Saved MAP@12 visualization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69300600ca0e46188f012e2d1baacbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30 [Train]:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a02ed65cab241958ff35a0fa0550b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30 [Val]:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d9945bf54d4aa4a88c0ef70c5523a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30 [Test]:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/30:\n",
      "  Train Loss: 0.028570\n",
      "  Val Loss: 0.029266\n",
      "  Val MAP@12: 0.988891\n",
      "  LR: 7.50e-05\n",
      "  No improvement (5/5)\n",
      "\n",
      " Early stopping triggered after 21 epochs\n",
      "   Best MAP@12: 0.989135 at epoch 16\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE\n",
      "================================================================================\n",
      "\n",
      " Best Model (selected based on Val MAP@12):\n",
      "  Epoch: 16\n",
      "  Val MAP@12: 0.989135\n",
      "\n",
      "âœ“ Loaded best model from checkpoint\n",
      " Saved final model to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/neural_tower_model.pt\n",
      "âœ“ Saved training history to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/neural_tower_history.json\n",
      "\n",
      " Neural Tower training complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRAINING LOOP WITH MAP@12 EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING NEURAL TOWER MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_map12': [],\n",
    "    'test_map12': []  # Add test MAP@12 tracking\n",
    "}\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "best_map12 = 0.0\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# Checkpoint directory\n",
    "checkpoint_dir = NeuralTowerConfig.MODEL_PATH / 'checkpoints'\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for epoch in range(NeuralTowerConfig.N_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NeuralTowerConfig.N_EPOCHS} [Train]\")\n",
    "    for user_feat, item_feat, image_feat, labels in train_pbar:\n",
    "        # Move to device\n",
    "        user_feat = user_feat.to(NeuralTowerConfig.DEVICE)\n",
    "        item_feat = item_feat.to(NeuralTowerConfig.DEVICE)\n",
    "        image_feat = image_feat.to(NeuralTowerConfig.DEVICE)\n",
    "        labels = labels.to(NeuralTowerConfig.DEVICE).squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_feat, item_feat, image_feat)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    if (epoch + 1) % NeuralTowerConfig.VALIDATION_FREQ == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        val_customer_ids_list = []\n",
    "        val_article_ids_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NeuralTowerConfig.N_EPOCHS} [Val]\")\n",
    "            for batch_idx, (user_feat, item_feat, image_feat, labels) in enumerate(val_pbar):\n",
    "                # Move to device\n",
    "                user_feat = user_feat.to(NeuralTowerConfig.DEVICE)\n",
    "                item_feat = item_feat.to(NeuralTowerConfig.DEVICE)\n",
    "                image_feat = image_feat.to(NeuralTowerConfig.DEVICE)\n",
    "                labels = labels.to(NeuralTowerConfig.DEVICE).squeeze()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(user_feat, item_feat, image_feat)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Store predictions and labels for MAP@12 calculation\n",
    "                predictions = outputs.cpu().numpy()\n",
    "                labels_np = labels.cpu().numpy()\n",
    "                \n",
    "                # Get customer and article IDs for this batch\n",
    "                start_idx = batch_idx * NeuralTowerConfig.BATCH_SIZE\n",
    "                end_idx = min(start_idx + len(predictions), len(val_data))\n",
    "                batch_customer_ids = val_customer_ids.iloc[start_idx:end_idx].values\n",
    "                batch_article_ids = val_article_ids.iloc[start_idx:end_idx].values\n",
    "                \n",
    "                all_predictions.extend(predictions)\n",
    "                all_labels.extend(labels_np)\n",
    "                val_customer_ids_list.extend(batch_customer_ids)\n",
    "                val_article_ids_list.extend(batch_article_ids)\n",
    "                \n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        \n",
    "        # Calculate MAP@12\n",
    "        val_eval_df = pd.DataFrame({\n",
    "            'customer_id': val_customer_ids_list,\n",
    "            'article_id': val_article_ids_list,\n",
    "            'label': all_labels,\n",
    "            'pred_score': all_predictions\n",
    "        })\n",
    "        \n",
    "        # Calculate MAP@12 using the standard evaluation function\n",
    "        # (no extra keyword arguments â€“ matches evaluate_map_at_12 signature)\n",
    "        map12_score = evaluate_map_at_12(\n",
    "            val_eval_df,\n",
    "            np.array(all_predictions)\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(map12_score)\n",
    "        \n",
    "        # Test evaluation (if test set is available)\n",
    "        test_map12_score = None\n",
    "        if use_test_set and test_loader is not None:\n",
    "            model.eval()\n",
    "            test_predictions = []\n",
    "            test_labels = []\n",
    "            test_customer_ids_list = []\n",
    "            test_article_ids_list = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NeuralTowerConfig.N_EPOCHS} [Test]\", leave=False)\n",
    "                for batch_idx, (user_feat, item_feat, image_feat, labels) in enumerate(test_pbar):\n",
    "                    # Move to device\n",
    "                    user_feat = user_feat.to(NeuralTowerConfig.DEVICE)\n",
    "                    item_feat = item_feat.to(NeuralTowerConfig.DEVICE)\n",
    "                    image_feat = image_feat.to(NeuralTowerConfig.DEVICE)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(user_feat, item_feat, image_feat)\n",
    "                    predictions = outputs.cpu().numpy()\n",
    "                    labels_np = labels.cpu().numpy()\n",
    "                    \n",
    "                    # Get customer and article IDs for this batch\n",
    "                    start_idx = batch_idx * NeuralTowerConfig.BATCH_SIZE\n",
    "                    end_idx = min(start_idx + len(predictions), len(test_data))\n",
    "                    batch_customer_ids = test_customer_ids.iloc[start_idx:end_idx].values\n",
    "                    batch_article_ids = test_article_ids.iloc[start_idx:end_idx].values\n",
    "                    \n",
    "                    test_predictions.extend(predictions)\n",
    "                    test_labels.extend(labels_np)\n",
    "                    test_customer_ids_list.extend(batch_customer_ids)\n",
    "                    test_article_ids_list.extend(batch_article_ids)\n",
    "            \n",
    "            # Calculate test MAP@12\n",
    "            test_eval_df = pd.DataFrame({\n",
    "                'customer_id': test_customer_ids_list,\n",
    "                'article_id': test_article_ids_list,\n",
    "                'label': test_labels,\n",
    "                'pred_score': test_predictions\n",
    "            })\n",
    "            \n",
    "            # Use consistent evaluation function (no filtering needed for test set)\n",
    "            # This ensures test MAP@12 uses the same calculation logic\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_map12'].append(map12_score)\n",
    "        if test_map12_score is not None:\n",
    "            history['test_map12'].append(test_map12_score)\n",
    "        else:\n",
    "            history['test_map12'].append(None)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1}/{NeuralTowerConfig.N_EPOCHS}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.6f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.6f}\")\n",
    "        print(f\"  Val MAP@12: {map12_score:.6f}\")\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Save best model based on VALIDATION MAP@12 (not test MAP@12)\n",
    "        # This ensures we select the model that generalizes best to validation users\n",
    "        if map12_score > best_map12:\n",
    "            best_map12 = map12_score\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'map12': map12_score,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'test_map12': test_map12_score if test_map12_score is not None else None,\n",
    "                'history': history\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_dir / 'best_model.pt')\n",
    "            print(f\" âœ… Saved best model (Val MAP@12: {map12_score:.6f})\")\n",
    "            if test_map12_score is not None:\n",
    "                print(f\"    Test MAP@12 at this epoch: {test_map12_score:.6f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{NeuralTowerConfig.EARLY_STOPPING_PATIENCE})\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= NeuralTowerConfig.EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\n Early stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"   Best MAP@12: {best_map12:.6f} at epoch {best_epoch}\")\n",
    "            break\n",
    "        \n",
    "        # Visualize MAP@12 progress every few epochs\n",
    "        if (epoch + 1) % 2 == 0 or (epoch + 1) == NeuralTowerConfig.N_EPOCHS or patience_counter >= NeuralTowerConfig.EARLY_STOPPING_PATIENCE:\n",
    "            if len(history['val_map12']) > 0:\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "                epochs_list = list(range(1, len(history['val_map12']) + 1))\n",
    "                ax.plot(epochs_list, history['val_map12'], 'b-o', label='Val MAP@12', linewidth=2, markersize=6)\n",
    "                if use_test_set and any(x is not None for x in history['test_map12']):\n",
    "                    test_scores = [x if x is not None else 0 for x in history['test_map12']]\n",
    "                    ax.plot(epochs_list, test_scores, 'r-s', label='Test MAP@12', linewidth=2, markersize=6)\n",
    "                ax.axvline(x=best_epoch, color='g', linestyle='--', linewidth=2, label=f'Best Epoch ({best_epoch})')\n",
    "                ax.set_xlabel('Epoch', fontsize=12)\n",
    "                ax.set_ylabel('MAP@12', fontsize=12)\n",
    "                ax.set_title('MAP@12 Progress During Training', fontsize=14, fontweight='bold')\n",
    "                ax.legend(fontsize=10)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(NeuralTowerConfig.MODEL_PATH / 'map12_progress.png', dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"  ðŸ“Š Saved MAP@12 visualization\")\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n Best Model (selected based on Val MAP@12):\")\n",
    "print(f\"  Epoch: {best_epoch}\")\n",
    "print(f\"  Val MAP@12: {best_map12:.6f}\")\n",
    "if use_test_set and len(history['test_map12']) > 0 and history['test_map12'][best_epoch-1] is not None:\n",
    "    test_map12_at_best = history['test_map12'][best_epoch-1]\n",
    "    print(f\"  Test MAP@12: {test_map12_at_best:.6f}\")\n",
    "    print(f\"\\nðŸ“Š Consistency Check:\")\n",
    "    print(f\"  Val and Test MAP@12 should be similar if evaluation is consistent\")\n",
    "    print(f\"  Difference: {abs(best_map12 - test_map12_at_best):.6f}\")\n",
    "\n",
    "# Load best model\n",
    "# Note: weights_only=False is needed for PyTorch 2.6+ when loading checkpoints with numpy arrays\n",
    "checkpoint = torch.load(checkpoint_dir / 'best_model.pt', weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"\\nâœ“ Loaded best model from checkpoint\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = NeuralTowerConfig.MODEL_PATH / 'neural_tower_model.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'user_feature_dim': len(user_feature_cols),\n",
    "        'item_feature_dim': len(item_feature_cols),\n",
    "        'image_feature_dim': len(image_feature_cols),\n",
    "        'user_embedding_dim': NeuralTowerConfig.USER_EMBEDDING_DIM,\n",
    "        'item_embedding_dim': NeuralTowerConfig.ITEM_EMBEDDING_DIM,\n",
    "        'image_embedding_dim': NeuralTowerConfig.IMAGE_EMBEDDING_DIM,\n",
    "        'fusion_hidden_dims': NeuralTowerConfig.FUSION_HIDDEN_DIMS,\n",
    "        'dropout_rate': NeuralTowerConfig.DROPOUT_RATE\n",
    "    },\n",
    "    'feature_cols': {\n",
    "        'user': user_feature_cols,\n",
    "        'item': item_feature_cols,\n",
    "        'image': image_feature_cols\n",
    "    },\n",
    "    'scalers': {\n",
    "        'user': scaler_user,\n",
    "        'item': scaler_item,\n",
    "        'image': scaler_image\n",
    "    },\n",
    "    'best_map12': best_map12,\n",
    "    'best_epoch': best_epoch,\n",
    "    'history': history\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\" Saved final model to {final_model_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = NeuralTowerConfig.MODEL_PATH / 'neural_tower_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"âœ“ Saved training history to {history_path}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n Neural Tower training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4090a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MAP@12 TRAINING PROGRESS VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "âœ… Saved training progress visualization to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/training_progress.png\n",
      "   - Val MAP@12: 0.988891 (final)\n",
      "   - Best Val MAP@12: 0.989135 (epoch 16)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE MAP@12 PROGRESS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MAP@12 TRAINING PROGRESS VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(history['val_map12']) > 0:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    epochs_list = list(range(1, len(history['val_map12']) + 1))\n",
    "    \n",
    "    # Plot 1: MAP@12 Progress\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(epochs_list, history['val_map12'], 'b-o', label='Val MAP@12', linewidth=2.5, markersize=8)\n",
    "    if use_test_set and any(x is not None for x in history['test_map12']):\n",
    "        test_scores = [x if x is not None else 0 for x in history['test_map12']]\n",
    "        ax1.plot(epochs_list, test_scores, 'r-s', label='Test MAP@12', linewidth=2.5, markersize=8)\n",
    "    ax1.axvline(x=best_epoch, color='g', linestyle='--', linewidth=2, label=f'Best Epoch ({best_epoch})')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('MAP@12', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('MAP@12 Progress During Training', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11, loc='best')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 1.0])\n",
    "    \n",
    "    # Plot 2: Loss Progress\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(epochs_list, history['train_loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)\n",
    "    ax2.plot(epochs_list, history['val_loss'], 'r-s', label='Val Loss', linewidth=2, markersize=6)\n",
    "    ax2.axvline(x=best_epoch, color='g', linestyle='--', linewidth=2, label=f'Best Epoch ({best_epoch})')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Loss Progress During Training', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11, loc='best')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    viz_path = NeuralTowerConfig.MODEL_PATH / 'training_progress.png'\n",
    "    plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nâœ… Saved training progress visualization to {viz_path}\")\n",
    "    print(f\"   - Val MAP@12: {history['val_map12'][-1]:.6f} (final)\")\n",
    "    print(f\"   - Best Val MAP@12: {best_map12:.6f} (epoch {best_epoch})\")\n",
    "else:\n",
    "    print(\"âš ï¸  No training history available for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "385473b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CANDIDATE QUALITY DIAGNOSTIC\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Analyzing candidate pool quality...\n",
      "\n",
      "âœ“ Candidate Coverage Analysis (sample of 100 validation users):\n",
      "  Average purchased items per user: 3.09\n",
      "  Average candidates per user: 1.24\n",
      "  Average coverage: 1.33%\n",
      "  Users with 100% coverage: 1/100\n",
      "  Users with <50% coverage: 99/100\n",
      "\n",
      "âš ï¸  WARNING: Low candidate coverage!\n",
      "   Only 1.3% of purchased items are in candidate pool\n",
      "   This severely limits MAP@12 - improve candidate generation!\n",
      "\n",
      "ðŸ“Š Validation Set Label Distribution:\n",
      "  Positive samples: 48,444 (39.98%)\n",
      "  Negative samples: 72,720 (60.02%)\n",
      "\n",
      "ðŸ“Š Per-User Statistics (Validation Users):\n",
      "  Average candidates per user: 16.12\n",
      "  Average positives per user: 6.79\n",
      "  Users with 0 positives: 418\n",
      "  Users with 1+ positives: 6714\n",
      "\n",
      "ðŸ’¡ Key Insight for MAP@12:\n",
      "   MAP@12 requires ranking positives in top 12\n",
      "   If users have many candidates but few positives, ranking is harder\n",
      "   Focus on: Better candidate quality > More candidates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32228"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC: CHECK CANDIDATE QUALITY FOR MAP@12\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CANDIDATE QUALITY DIAGNOSTIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load validation data\n",
    "val_data_diag = pd.read_parquet(NeuralTowerConfig.MODEL_PATH / 'val_data.parquet')\n",
    "\n",
    "# Check if purchased items are in candidate pool\n",
    "print(\"\\nðŸ“Š Analyzing candidate pool quality...\")\n",
    "\n",
    "# For validation users, check if their purchased items are in candidates\n",
    "if 'user_type' in val_data_diag.columns:\n",
    "    val_users_data = val_data_diag[val_data_diag['user_type'] == 'validation'].copy()\n",
    "    \n",
    "    # Load ground truth\n",
    "    try:\n",
    "        val_ground_truth = pd.read_parquet(NeuralTowerConfig.DATA_PATH / 'val_ground_truth.parquet')\n",
    "        \n",
    "        # Check coverage: Are purchased items in candidate pool?\n",
    "        coverage_stats = []\n",
    "        for _, row in val_ground_truth.head(100).iterrows():  # Sample 100 users\n",
    "            user_id = row['customer_id']\n",
    "            purchased_items = set(row['purchased_articles'])\n",
    "            \n",
    "            user_candidates = set(val_users_data[val_users_data['customer_id'] == user_id]['article_id'].values)\n",
    "            \n",
    "            # Calculate coverage\n",
    "            items_in_candidates = purchased_items.intersection(user_candidates)\n",
    "            coverage = len(items_in_candidates) / len(purchased_items) if len(purchased_items) > 0 else 0\n",
    "            \n",
    "            coverage_stats.append({\n",
    "                'user_id': user_id,\n",
    "                'purchased_count': len(purchased_items),\n",
    "                'candidates_count': len(user_candidates),\n",
    "                'items_in_candidates': len(items_in_candidates),\n",
    "                'coverage': coverage\n",
    "            })\n",
    "        \n",
    "        coverage_df = pd.DataFrame(coverage_stats)\n",
    "        \n",
    "        print(f\"\\nâœ“ Candidate Coverage Analysis (sample of 100 validation users):\")\n",
    "        print(f\"  Average purchased items per user: {coverage_df['purchased_count'].mean():.2f}\")\n",
    "        print(f\"  Average candidates per user: {coverage_df['candidates_count'].mean():.2f}\")\n",
    "        print(f\"  Average coverage: {coverage_df['coverage'].mean():.2%}\")\n",
    "        print(f\"  Users with 100% coverage: {(coverage_df['coverage'] == 1.0).sum()}/{len(coverage_df)}\")\n",
    "        print(f\"  Users with <50% coverage: {(coverage_df['coverage'] < 0.5).sum()}/{len(coverage_df)}\")\n",
    "        \n",
    "        if coverage_df['coverage'].mean() < 0.8:\n",
    "            print(f\"\\nâš ï¸  WARNING: Low candidate coverage!\")\n",
    "            print(f\"   Only {coverage_df['coverage'].mean():.1%} of purchased items are in candidate pool\")\n",
    "            print(f\"   This severely limits MAP@12 - improve candidate generation!\")\n",
    "        else:\n",
    "            print(f\"\\nâœ… Good candidate coverage: {coverage_df['coverage'].mean():.1%}\")\n",
    "            print(f\"   Most purchased items are in the candidate pool\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not load ground truth: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸  user_type column not found in validation data\")\n",
    "\n",
    "# Check positive ratio in validation set\n",
    "print(\"\\nðŸ“Š Validation Set Label Distribution:\")\n",
    "val_label_dist = val_data_diag['label'].value_counts()\n",
    "print(f\"  Positive samples: {val_label_dist.get(1, 0):,} ({val_label_dist.get(1, 0)/len(val_data_diag)*100:.2f}%)\")\n",
    "print(f\"  Negative samples: {val_label_dist.get(0, 0):,} ({val_label_dist.get(0, 0)/len(val_data_diag)*100:.2f}%)\")\n",
    "\n",
    "# Check per-user statistics\n",
    "val_user_stats = val_data_diag.groupby('customer_id').agg({\n",
    "    'article_id': 'nunique',\n",
    "    'label': 'sum'\n",
    "}).reset_index()\n",
    "val_user_stats.columns = ['customer_id', 'candidates', 'positives']\n",
    "\n",
    "print(f\"\\nðŸ“Š Per-User Statistics (Validation Users):\")\n",
    "print(f\"  Average candidates per user: {val_user_stats['candidates'].mean():.2f}\")\n",
    "print(f\"  Average positives per user: {val_user_stats['positives'].mean():.2f}\")\n",
    "print(f\"  Users with 0 positives: {(val_user_stats['positives'] == 0).sum()}\")\n",
    "print(f\"  Users with 1+ positives: {(val_user_stats['positives'] >= 1).sum()}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight for MAP@12:\")\n",
    "print(\"   MAP@12 requires ranking positives in top 12\")\n",
    "print(\"   If users have many candidates but few positives, ranking is harder\")\n",
    "print(\"   Focus on: Better candidate quality > More candidates\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c169ce00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NEURAL TOWER - FINAL EVALUATION\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a6457112584984aff37a7fd25944fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Performance:\n",
      "  Best MAP@12 (during training): 0.989036\n",
      "\n",
      "âœ“ Saved predictions to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/neural_tower_predictions_val.parquet\n",
      "\n",
      " Training Summary:\n",
      "  Total epochs: 19\n",
      "  Best epoch: 14\n",
      "  Final train loss: 0.028723\n",
      "  Final val loss: 0.029836\n",
      "\n",
      " Saved Files:\n",
      "  Model: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/neural_tower_model.pt\n",
      "  Checkpoint: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/checkpoints/best_model.pt\n",
      "  History: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/neural_tower_history.json\n",
      "  Predictions: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/neural_tower_predictions_val.parquet\n"
     ]
    }
   ],
   "source": [
    "# FINAL EVALUATION AND SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEURAL TOWER - FINAL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "val_customer_ids_list = []\n",
    "val_article_ids_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (user_feat, item_feat, image_feat, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating\")):\n",
    "        user_feat = user_feat.to(NeuralTowerConfig.DEVICE)\n",
    "        item_feat = item_feat.to(NeuralTowerConfig.DEVICE)\n",
    "        image_feat = image_feat.to(NeuralTowerConfig.DEVICE)\n",
    "        \n",
    "        outputs = model(user_feat, item_feat, image_feat)\n",
    "        predictions = outputs.cpu().numpy().flatten()  # Flatten to 1D\n",
    "        labels_np = labels.numpy().flatten()  # Flatten to 1D\n",
    "        \n",
    "        # Get customer and article IDs for this batch\n",
    "        start_idx = batch_idx * NeuralTowerConfig.BATCH_SIZE\n",
    "        end_idx = min(start_idx + len(predictions), len(val_data))\n",
    "        batch_customer_ids = val_customer_ids.iloc[start_idx:end_idx].values\n",
    "        batch_article_ids = val_article_ids.iloc[start_idx:end_idx].values\n",
    "        \n",
    "        all_predictions.extend(predictions.tolist())  # Convert to list\n",
    "        all_labels.extend(labels_np.tolist())  # Convert to list\n",
    "        val_customer_ids_list.extend(batch_customer_ids)\n",
    "        val_article_ids_list.extend(batch_article_ids)\n",
    "\n",
    "# Create evaluation dataframe\n",
    "val_eval_df = pd.DataFrame({\n",
    "    'customer_id': val_customer_ids_list[:len(all_predictions)],\n",
    "    'article_id': val_article_ids_list[:len(all_predictions)],\n",
    "    'label': all_labels[:len(all_predictions)],\n",
    "    'pred_score': all_predictions\n",
    "})\n",
    "\n",
    "# Ensure label column is numeric (not object/list)\n",
    "val_eval_df['label'] = pd.to_numeric(val_eval_df['label'], errors='coerce')\n",
    "val_eval_df['pred_score'] = pd.to_numeric(val_eval_df['pred_score'], errors='coerce')\n",
    "\n",
    "# Calculate final MAP@12 using consistent evaluation function\n",
    "# Load val_data for filtering (same as training)\n",
    "val_data_for_eval = pd.read_parquet(NeuralTowerConfig.MODEL_PATH / 'val_data.parquet')\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Performance:\")\n",
    "print(f\"  Best MAP@12 (during training): {best_map12:.6f}\")\n",
    "\n",
    "# Save predictions\n",
    "predictions_path = NeuralTowerConfig.MODEL_PATH / 'neural_tower_predictions_val.parquet'\n",
    "val_eval_df.to_parquet(predictions_path, index=False)\n",
    "print(f\"\\nâœ“ Saved predictions to {predictions_path}\")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\n Training Summary:\")\n",
    "print(f\"  Total epochs: {len(history['train_loss'])}\")\n",
    "print(f\"  Best epoch: {best_epoch}\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"  Final val loss: {history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\n Saved Files:\")\n",
    "print(f\"  Model: {final_model_path}\")\n",
    "print(f\"  Checkpoint: {checkpoint_dir / 'best_model.pt'}\")\n",
    "print(f\"  History: {history_path}\")\n",
    "print(f\"  Predictions: {predictions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68775a88",
   "metadata": {},
   "source": [
    "### Stage 7: Evaluation & Metrics\n",
    "\n",
    "This stage provides comprehensive evaluation and comparison of all models:\n",
    "- **Model Comparison**: LightGBM vs Neural Towers performance\n",
    "- **Ensemble Evaluation**: Weighted combination of best models\n",
    "- **Detailed Metrics**: MAP@12, Precision@K, Recall@K, NDCG@K\n",
    "- **Feature Analysis**: Importance analysis and ablation studies\n",
    "- **Final Ranking**: Generate top-12 predictions for each user\n",
    "- **Submission Preparation**: Format predictions for Kaggle submission\n",
    "\n",
    "**Key Features:**\n",
    "- Comprehensive metric suite\n",
    "- Model ensemble strategies\n",
    "- Performance visualization\n",
    "- Submission file generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ce2850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Evaluation Configuration loaded\n",
      "  Model path: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND CONFIGURATION FOR EVALUATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "class EvaluationConfig:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    MODEL_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models')\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    K_VALUES = [1, 3, 5, 10, 12]  # Different K values for evaluation\n",
    "    \n",
    "    # Ensemble weights (can be tuned)\n",
    "    ENSEMBLE_WEIGHTS = {\n",
    "        'lgb_classifier': 0.2,\n",
    "        'lgb_ranker_lambdarank': 0.3,\n",
    "        'lgb_ranker_xendcg': 0.2,\n",
    "        'neural_tower': 0.3\n",
    "    }\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "print(\"âœ“ Evaluation Configuration loaded\")\n",
    "print(f\"  Model path: {EvaluationConfig.MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "686896b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Evaluation metrics functions defined\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE EVALUATION METRICS\n",
    "\n",
    "def calculate_map_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Mean Average Precision at K (MAP@K)\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    aps = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate AP for this user\n",
    "        hits = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        for i, pred_item in enumerate(pred_items):\n",
    "            if pred_item in true_items:\n",
    "                hits += 1\n",
    "                precision_sum += hits / (i + 1)\n",
    "        \n",
    "        if hits > 0:\n",
    "            ap = precision_sum / len(true_items)\n",
    "            aps.append(ap)\n",
    "    \n",
    "    return np.mean(aps) if len(aps) > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_precision_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Precision@K\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    precisions = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(pred_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        hits = sum(1 for item in pred_items if item in true_items)\n",
    "        precision = hits / len(pred_items)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    return np.mean(precisions) if len(precisions) > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Recall@K\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    recalls = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        hits = sum(1 for item in pred_items if item in true_items)\n",
    "        recall = hits / len(true_items)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return np.mean(recalls) if len(recalls) > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_ndcg_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Normalized Discounted Cumulative Gain at K (NDCG@K)\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    ndcgs = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0.0\n",
    "        for i, pred_item in enumerate(pred_items):\n",
    "            if pred_item in true_items:\n",
    "                dcg += 1.0 / np.log2(i + 2)  # i+2 because log2(1) = 0\n",
    "        \n",
    "        # Calculate IDCG (ideal DCG)\n",
    "        idcg = 0.0\n",
    "        num_relevant = min(len(true_items), len(pred_items))\n",
    "        for i in range(num_relevant):\n",
    "            idcg += 1.0 / np.log2(i + 2)\n",
    "        \n",
    "        if idcg > 0:\n",
    "            ndcg = dcg / idcg\n",
    "            ndcgs.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcgs) if len(ndcgs) > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_all_metrics(df, predictions, k_values=[1, 3, 5, 10, 12]):\n",
    "    \"\"\"\n",
    "    Evaluate all metrics for different K values\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['customer_id', 'article_id', 'label']\n",
    "        predictions: Array of prediction scores\n",
    "        k_values: List of K values to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Group by customer\n",
    "    grouped = df.groupby('customer_id')\n",
    "    \n",
    "    # Prepare true and predicted items for each user\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for customer_id, group in grouped:\n",
    "        # True items (purchased articles)\n",
    "        true_items = set(group[group['label'] == 1]['article_id'].values)\n",
    "        y_true.append(true_items)\n",
    "        \n",
    "        # Predicted items (sorted by score)\n",
    "        customer_df = group.copy()\n",
    "        customer_df['pred_score'] = predictions[:len(customer_df)]\n",
    "        customer_df = customer_df.sort_values('pred_score', ascending=False)\n",
    "        pred_items = customer_df['article_id'].values.tolist()\n",
    "        y_pred.append(pred_items)\n",
    "        \n",
    "        # Remove used predictions\n",
    "        predictions = predictions[len(customer_df):]\n",
    "    \n",
    "    # Calculate metrics for each K\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        results[f'MAP@{k}'] = calculate_map_at_k(y_true, y_pred, k)\n",
    "        results[f'Precision@{k}'] = calculate_precision_at_k(y_true, y_pred, k)\n",
    "        results[f'Recall@{k}'] = calculate_recall_at_k(y_true, y_pred, k)\n",
    "        results[f'NDCG@{k}'] = calculate_ndcg_at_k(y_true, y_pred, k)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ“ Evaluation metrics functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3038ea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING MODEL PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Loading validation data...\n",
      "âœ“ Loaded 121,164 validation samples\n",
      "\n",
      "Loading LightGBM predictions...\n",
      "âœ“ Loaded ensemble predictions\n",
      "\n",
      "Loading Neural Tower predictions...\n",
      "âœ“ Loaded Neural Tower predictions\n",
      "\n",
      "âœ“ Total models loaded: 1\n",
      "  Models: ['neural_tower']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36706"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD ALL MODEL PREDICTIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING MODEL PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load validation data\n",
    "print(\"\\nLoading validation data...\")\n",
    "val_data = pd.read_parquet(EvaluationConfig.MODEL_PATH / 'val_data.parquet')\n",
    "print(f\"âœ“ Loaded {len(val_data):,} validation samples\")\n",
    "\n",
    "# Load LightGBM predictions\n",
    "lgb_predictions = {}\n",
    "print(\"\\nLoading LightGBM predictions...\")\n",
    "try:\n",
    "    ensemble_preds = pd.read_parquet(EvaluationConfig.MODEL_PATH / 'ensemble_predictions_val.parquet')\n",
    "    if 'ensemble_weighted' in ensemble_preds.columns:\n",
    "        lgb_predictions['ensemble_weighted'] = ensemble_preds['ensemble_weighted'].values\n",
    "    if 'ensemble_average' in ensemble_preds.columns:\n",
    "        lgb_predictions['ensemble_average'] = ensemble_preds['ensemble_average'].values\n",
    "    print(f\"âœ“ Loaded ensemble predictions\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load ensemble predictions: {e}\")\n",
    "\n",
    "# Load individual LightGBM model predictions if available\n",
    "lgb_models = ['lgb_classifier', 'lgb_ranker_lambdarank', 'lgb_ranker_xendcg', 'lgb_classifier_deep']\n",
    "for model_name in lgb_models:\n",
    "    try:\n",
    "        pred_file = EvaluationConfig.MODEL_PATH / f'{model_name}_predictions_val.parquet'\n",
    "        if pred_file.exists():\n",
    "            preds = pd.read_parquet(pred_file)\n",
    "            if 'pred_score' in preds.columns:\n",
    "                lgb_predictions[model_name] = preds['pred_score'].values\n",
    "                print(f\"âœ“ Loaded {model_name} predictions\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not load {model_name}: {e}\")\n",
    "\n",
    "# Load Neural Tower predictions\n",
    "neural_predictions = {}\n",
    "print(\"\\nLoading Neural Tower predictions...\")\n",
    "try:\n",
    "    neural_preds = pd.read_parquet(EvaluationConfig.MODEL_PATH / 'neural_tower_predictions_val.parquet')\n",
    "    if 'pred_score' in neural_preds.columns:\n",
    "        neural_predictions['neural_tower'] = neural_preds['pred_score'].values\n",
    "        print(f\"âœ“ Loaded Neural Tower predictions\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load Neural Tower predictions: {e}\")\n",
    "\n",
    "# Combine all predictions\n",
    "all_predictions = {**lgb_predictions, **neural_predictions}\n",
    "\n",
    "print(f\"\\nâœ“ Total models loaded: {len(all_predictions)}\")\n",
    "print(f\"  Models: {list(all_predictions.keys())}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "966495fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING ALL MODELS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910c17011a74432395555562e80fd2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating models:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating neural_tower...\n",
      "  MAP@12: 0.389415\n",
      "  Precision@12: 0.319977\n",
      "  Recall@12: 0.846748\n",
      "  NDCG@12: 0.581958\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "                MAP@1  Precision@1  Recall@1    NDCG@1     MAP@3  Precision@3  Recall@3    NDCG@3     MAP@5  Precision@5  Recall@5    NDCG@5    MAP@10  Precision@10  Recall@10   NDCG@10    MAP@12  Precision@12  Recall@12   NDCG@12\n",
      "neural_tower  0.23623     0.318704  0.079975  0.338546  0.255288     0.319242  0.240225  0.381411  0.282609     0.319233  0.396367  0.429862  0.359953      0.319653   0.745062  0.546265  0.389415      0.319977   0.846748  0.581958\n",
      "\n",
      "âœ“ Saved comparison results to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/model_comparison.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EVALUATE ALL MODELS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store all evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, predictions in tqdm(all_predictions.items(), desc=\"Evaluating models\"):\n",
    "    print(f\"\\n Evaluating {model_name}...\")\n",
    "    \n",
    "    # Ensure predictions match validation data length\n",
    "    if len(predictions) != len(val_data):\n",
    "        print(f\" Prediction length mismatch: {len(predictions)} vs {len(val_data)}\")\n",
    "        min_len = min(len(predictions), len(val_data))\n",
    "        predictions = predictions[:min_len]\n",
    "        val_data_eval = val_data.iloc[:min_len].copy()\n",
    "    else:\n",
    "        val_data_eval = val_data.copy()\n",
    "    \n",
    "    # Evaluate all metrics\n",
    "    metrics = evaluate_all_metrics(val_data_eval, predictions.copy(), k_values=EvaluationConfig.K_VALUES)\n",
    "    evaluation_results[model_name] = metrics\n",
    "    \n",
    "    # Print key metrics\n",
    "    print(f\"  MAP@12: {metrics['MAP@12']:.6f}\")\n",
    "    print(f\"  Precision@12: {metrics['Precision@12']:.6f}\")\n",
    "    print(f\"  Recall@12: {metrics['Recall@12']:.6f}\")\n",
    "    print(f\"  NDCG@12: {metrics['NDCG@12']:.6f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(evaluation_results).T\n",
    "comparison_df = comparison_df.sort_values('MAP@12', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + comparison_df.to_string())\n",
    "\n",
    "# Save comparison results\n",
    "comparison_path = EvaluationConfig.MODEL_PATH / 'model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path)\n",
    "print(f\"\\nâœ“ Saved comparison results to {comparison_path}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74361d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING FINAL ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "ðŸ“¦ Available models for ensemble: ['neural_tower']\n",
      "  neural_tower: [0.0005, 0.9999] -> [0, 1]\n",
      "\n",
      "âš–ï¸  Ensemble weights: {'neural_tower': 1.0}\n",
      "\n",
      "ðŸ“Š Evaluating final ensemble...\n",
      "  MAP@12: 0.776194\n",
      "  Precision@12: 0.332567\n",
      "  Recall@12: 1.022509\n",
      "  NDCG@12: 0.842597\n",
      "\n",
      "âœ“ Saved ensemble predictions to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/final_ensemble_predictions_val.parquet\n",
      "\n",
      "================================================================================\n",
      "UPDATED MODEL COMPARISON (with ensemble)\n",
      "================================================================================\n",
      "\n",
      "                  MAP@1  Precision@1  Recall@1    NDCG@1     MAP@3  Precision@3  Recall@3    NDCG@3     MAP@5  Precision@5  Recall@5    NDCG@5    MAP@10  Precision@10  Recall@10   NDCG@10    MAP@12  Precision@12  Recall@12   NDCG@12\n",
      "neural_tower    0.63528     0.332004  0.385652  0.607058  0.693509     0.331751  0.835445  0.767134  0.747797     0.332384  0.976887  0.822544  0.775606      0.332574   1.021713  0.842191  0.776194      0.332567   1.022509  0.842597\n",
      "final_ensemble  0.63528     0.332004  0.385652  0.607058  0.693509     0.331751  0.835445  0.767134  0.747797     0.332384  0.976887  0.822544  0.775606      0.332574   1.021713  0.842191  0.776194      0.332567   1.022509  0.842597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE FINAL ENSEMBLE\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING FINAL ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select best models for ensemble\n",
    "best_models = ['neural_tower', 'lgb_ranker_lambdarank', 'lgb_classifier']\n",
    "\n",
    "# Filter to available models\n",
    "available_models = [m for m in best_models if m in all_predictions]\n",
    "print(f\"\\nðŸ“¦ Available models for ensemble: {available_models}\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    print(\"âš ï¸  No models available for ensemble!\")\n",
    "else:\n",
    "    # Normalize predictions to [0, 1] range\n",
    "    normalized_preds = {}\n",
    "    for model_name in available_models:\n",
    "        preds = all_predictions[model_name].copy()\n",
    "        min_pred = preds.min()\n",
    "        max_pred = preds.max()\n",
    "        if max_pred > min_pred:\n",
    "            normalized = (preds - min_pred) / (max_pred - min_pred)\n",
    "        else:\n",
    "            normalized = preds\n",
    "        normalized_preds[model_name] = normalized\n",
    "        print(f\"  {model_name}: [{preds.min():.4f}, {preds.max():.4f}] -> [0, 1]\")\n",
    "    \n",
    "    # Create ensemble with equal weights (can be tuned)\n",
    "    ensemble_weights = {m: 1.0 / len(available_models) for m in available_models}\n",
    "    print(f\"\\nâš–ï¸  Ensemble weights: {ensemble_weights}\")\n",
    "    \n",
    "    # Calculate weighted ensemble\n",
    "    ensemble_pred = np.zeros(len(normalized_preds[available_models[0]]))\n",
    "    for model_name, weight in ensemble_weights.items():\n",
    "        ensemble_pred += weight * normalized_preds[model_name]\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    print(\"\\nðŸ“Š Evaluating final ensemble...\")\n",
    "    ensemble_metrics = evaluate_all_metrics(val_data, ensemble_pred.copy(), k_values=EvaluationConfig.K_VALUES)\n",
    "    evaluation_results['final_ensemble'] = ensemble_metrics\n",
    "    \n",
    "    print(f\"  MAP@12: {ensemble_metrics['MAP@12']:.6f}\")\n",
    "    print(f\"  Precision@12: {ensemble_metrics['Precision@12']:.6f}\")\n",
    "    print(f\"  Recall@12: {ensemble_metrics['Recall@12']:.6f}\")\n",
    "    print(f\"  NDCG@12: {ensemble_metrics['NDCG@12']:.6f}\")\n",
    "    \n",
    "    # Save ensemble predictions\n",
    "    ensemble_df = val_data[['customer_id', 'article_id', 'label']].copy()\n",
    "    ensemble_df['pred_score'] = ensemble_pred\n",
    "    ensemble_path = EvaluationConfig.MODEL_PATH / 'final_ensemble_predictions_val.parquet'\n",
    "    ensemble_df.to_parquet(ensemble_path, index=False)\n",
    "    print(f\"\\nâœ“ Saved ensemble predictions to {ensemble_path}\")\n",
    "    \n",
    "    # Update comparison\n",
    "    comparison_df = pd.DataFrame(evaluation_results).T\n",
    "    comparison_df = comparison_df.sort_values('MAP@12', ascending=False)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UPDATED MODEL COMPARISON (with ensemble)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n\" + comparison_df.to_string())\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c638b626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING FINAL RANKINGS\n",
      "================================================================================\n",
      "\n",
      "âœ… Using final ensemble for submission\n",
      "\n",
      "ðŸ“Š Generating top-12 rankings per user...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d042e05b79714fe4a36f969483d1b24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking users:   0%|          | 0/42126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Generated rankings for 42,126 users\n",
      "  Average articles per user: 2.87\n",
      "\n",
      "âœ“ Saved submission file to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/submission.csv\n",
      "\n",
      "ðŸ“„ Sample submission (first 5 rows):\n",
      "                                                     customer_id                              prediction\n",
      "0000945f66de1a11d9447609b8b41b1bc987ba185a5496ae8831e8493afa24ff                               811899002\n",
      "00012315fd38859ff2c446876ca507abbcbcf582d0e266b1b696941c16e777a2                               872600009\n",
      "00061a04f030bdf3665b09829192ca8c13c4de6dd9ae9d38d0d0b5ce3a1cfc6f                     799365013 883724001\n",
      "00089f13f465ec902e5c49a3bb408c5e31205096d6f267543f1893303e456016                               858052005\n",
      "000e3f587242eb077685a487ad27dad632a4801576dfd16967280f0da3a78c2e 706016001 684209004 620425012 857713001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210688"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GENERATE FINAL RANKINGS FOR SUBMISSION\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING FINAL RANKINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use best model (or ensemble if available)\n",
    "if 'final_ensemble' in evaluation_results:\n",
    "    best_model_name = 'final_ensemble'\n",
    "    best_predictions = ensemble_pred\n",
    "    print(f\"\\nâœ… Using final ensemble for submission\")\n",
    "elif 'neural_tower' in all_predictions:\n",
    "    best_model_name = 'neural_tower'\n",
    "    best_predictions = all_predictions['neural_tower']\n",
    "    print(f\"\\nâœ… Using Neural Tower for submission\")\n",
    "elif len(all_predictions) > 0:\n",
    "    # Use model with best MAP@12\n",
    "    best_model_name = comparison_df.index[0]\n",
    "    best_predictions = all_predictions[best_model_name]\n",
    "    print(f\"\\nâœ… Using {best_model_name} for submission\")\n",
    "else:\n",
    "    raise ValueError(\"No predictions available!\")\n",
    "\n",
    "# Create predictions DataFrame\n",
    "pred_df = val_data[['customer_id', 'article_id']].copy()\n",
    "pred_df['pred_score'] = best_predictions[:len(pred_df)]\n",
    "\n",
    "# Generate top-12 predictions for each user\n",
    "print(\"\\nðŸ“Š Generating top-12 rankings per user...\")\n",
    "rankings = []\n",
    "for customer_id, group in tqdm(pred_df.groupby('customer_id'), desc=\"Ranking users\"):\n",
    "    # Sort by prediction score (descending)\n",
    "    group_sorted = group.sort_values('pred_score', ascending=False)\n",
    "    \n",
    "    # Get top 12 article IDs\n",
    "    top_articles = group_sorted.head(12)['article_id'].values\n",
    "    \n",
    "    # Format as space-separated string\n",
    "    predictions_str = ' '.join([str(art) for art in top_articles])\n",
    "    \n",
    "    rankings.append({\n",
    "        'customer_id': customer_id,\n",
    "        'prediction': predictions_str\n",
    "    })\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(rankings)\n",
    "submission_df = submission_df.sort_values('customer_id')\n",
    "\n",
    "print(f\"\\nâœ“ Generated rankings for {len(submission_df):,} users\")\n",
    "print(f\"  Average articles per user: {submission_df['prediction'].str.split().str.len().mean():.2f}\")\n",
    "\n",
    "# Save submission file\n",
    "submission_path = EvaluationConfig.MODEL_PATH / 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"\\nâœ“ Saved submission file to {submission_path}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nðŸ“„ Sample submission (first 5 rows):\")\n",
    "print(submission_df.head().to_string(index=False))\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b8f70db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ† Best Model: neural_tower\n",
      "   MAP@12: 0.776194\n",
      "\n",
      "ðŸ“Š Model Rankings (by MAP@12):\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ¥‡ 1. neural_tower                   MAP@12: 0.776194\n",
      "ðŸ¥ˆ 2. final_ensemble                 MAP@12: 0.776194\n",
      "\n",
      "ðŸ“ˆ Detailed Metrics for Best Model (neural_tower):\n",
      "--------------------------------------------------------------------------------\n",
      "  MAP@12              : 0.776194\n",
      "  Precision@12        : 0.332567\n",
      "  Recall@12           : 1.022509\n",
      "  NDCG@12             : 0.842597\n",
      "\n",
      "ðŸ’¾ Generated Files:\n",
      "--------------------------------------------------------------------------------\n",
      "  Model Comparison: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/model_comparison.csv\n",
      "  Ensemble Predictions: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/final_ensemble_predictions_val.parquet\n",
      "  Submission File: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/submission.csv\n",
      "\n",
      "ðŸ“Š Performance Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "  Total Models Evaluated: 2\n",
      "  Best MAP@12: 0.776194\n",
      "  Improvement over baseline: 0.00%\n",
      "\n",
      "================================================================================\n",
      "âœ… Step 3 Complete: Evaluation & Metrics\n",
      "   Ready for final submission!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL SUMMARY AND RESULTS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best model\n",
    "best_model = comparison_df.index[0]\n",
    "best_map12 = comparison_df.loc[best_model, 'MAP@12']\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {best_model}\")\n",
    "print(f\"   MAP@12: {best_map12:.6f}\")\n",
    "\n",
    "# Model rankings\n",
    "print(f\"\\nðŸ“Š Model Rankings (by MAP@12):\")\n",
    "print(\"-\" * 80)\n",
    "for idx, (model_name, row) in enumerate(comparison_df.iterrows(), 1):\n",
    "    marker = \"ðŸ¥‡\" if idx == 1 else \"ðŸ¥ˆ\" if idx == 2 else \"ðŸ¥‰\" if idx == 3 else \"  \"\n",
    "    print(f\"{marker} {idx}. {model_name:30s} MAP@12: {row['MAP@12']:.6f}\")\n",
    "\n",
    "# Key metrics for best model\n",
    "print(f\"\\nðŸ“ˆ Detailed Metrics for Best Model ({best_model}):\")\n",
    "print(\"-\" * 80)\n",
    "best_metrics = comparison_df.loc[best_model]\n",
    "for metric_name in ['MAP@12', 'Precision@12', 'Recall@12', 'NDCG@12']:\n",
    "    print(f\"  {metric_name:20s}: {best_metrics[metric_name]:.6f}\")\n",
    "\n",
    "# Files saved\n",
    "print(f\"\\nðŸ’¾ Generated Files:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Model Comparison: {EvaluationConfig.MODEL_PATH / 'model_comparison.csv'}\")\n",
    "if 'final_ensemble' in evaluation_results:\n",
    "    print(f\"  Ensemble Predictions: {EvaluationConfig.MODEL_PATH / 'final_ensemble_predictions_val.parquet'}\")\n",
    "print(f\"  Submission File: {EvaluationConfig.MODEL_PATH / 'submission.csv'}\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nðŸ“Š Performance Summary:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Total Models Evaluated: {len(comparison_df)}\")\n",
    "print(f\"  Best MAP@12: {best_map12:.6f}\")\n",
    "print(f\"  Improvement over baseline: {((best_map12 - comparison_df['MAP@12'].min()) / comparison_df['MAP@12'].min() * 100):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Step 3 Complete: Evaluation & Metrics\")\n",
    "print(\"   Ready for final submission!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c166ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "assignment2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
