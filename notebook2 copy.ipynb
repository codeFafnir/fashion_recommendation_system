{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c66d65",
   "metadata": {},
   "source": [
    "### Stage 1: Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d746cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4100b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths (adjust based on your Kaggle dataset location)\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/h-and-m-personalized-fashion-recommendations')\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_2')\n",
    "    \n",
    "     # Temporal configuration\n",
    "    N_TRAIN_WEEKS = 11  # Number of weeks for training\n",
    "    N_VAL_WEEKS = 1     # Validation week\n",
    "    TOTAL_WEEKS = 24    # Total weeks to consider (16-24 range)\n",
    "    \n",
    "    # User sampling configuration\n",
    "    TARGET_USERS = 50000  # Target number of users to sample\n",
    "    MIN_USER_PURCHASES = 1  # Minimum purchases across all weeks for a user\n",
    "    \n",
    "    # Cold start configuration\n",
    "    INCLUDE_COLD_START = True  # Include users with limited history\n",
    "    COLD_START_RATIO = 0.15  # 15% of sampled users will be cold start\n",
    "    COLD_START_MAX_PURCHASES = 1  # Users with <= this many purchases are \"cold start\"\n",
    "    \n",
    "    # Stratification configuration\n",
    "    STRATIFY_BY_ACTIVITY = True  # Stratify users by activity level\n",
    "    ACTIVITY_BINS = [0, 5, 10, 20, 50, np.inf]  # Purchase count bins\n",
    "    ACTIVITY_LABELS = ['low', 'medium', 'high', 'very_high', 'extreme']\n",
    "    \n",
    "    # Item filtering\n",
    "    MIN_ITEM_PURCHASES = 5  # Minimum purchases for an item to be included\n",
    "    \n",
    "    # Memory optimization\n",
    "    CHUNK_SIZE = 500_000  # Process transactions in chunks\n",
    "    \n",
    "    # Random seed\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()\n",
    "config.OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "np.random.seed(config.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50955d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduce memory usage of a dataframe by optimizing dtypes\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Skip datetime and object columns\n",
    "        if col_type == object or pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            continue\n",
    "            \n",
    "        c_min = df[col].min()\n",
    "        c_max = df[col].max()\n",
    "        \n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "        else:\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Memory usage decreased from {start_mem:.2f} MB to {end_mem:.2f} MB '\n",
    "              f'({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"Pretty print section headers\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ad57ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "Loading transactions...\n",
      "‚úì Loaded 31,788,324 transactions\n",
      "  Date range: 2018-09-20 00:00:00 to 2020-09-22 00:00:00\n",
      "  Unique customers: 1,362,281\n",
      "  Unique articles: 104,547\n",
      "\n",
      "Loading customers...\n",
      "‚úì Loaded 1,371,980 customers\n",
      "\n",
      "Loading articles...\n",
      "‚úì Loaded 105,542 articles\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: LOAD AND EXPLORE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 1: LOADING DATA\")\n",
    "\n",
    "# Load transactions\n",
    "print(\"Loading transactions...\")\n",
    "transactions = pd.read_csv(\n",
    "    config.DATA_PATH / 'transactions_train.csv',\n",
    "    dtype={\n",
    "        'article_id': 'int32',\n",
    "        'price': 'float32',\n",
    "        'sales_channel_id': 'int8'\n",
    "    },\n",
    "    parse_dates=['t_dat']\n",
    ")\n",
    "\n",
    "print(f\"‚úì Loaded {len(transactions):,} transactions\")\n",
    "print(f\"  Date range: {transactions['t_dat'].min()} to {transactions['t_dat'].max()}\")\n",
    "print(f\"  Unique customers: {transactions['customer_id'].nunique():,}\")\n",
    "print(f\"  Unique articles: {transactions['article_id'].nunique():,}\")\n",
    "\n",
    "# Load customers\n",
    "print(\"\\nLoading customers...\")\n",
    "customers = pd.read_csv(\n",
    "    config.DATA_PATH / 'customers.csv',\n",
    "    dtype={\n",
    "        'FN': 'float32',\n",
    "        'Active': 'float32',\n",
    "        'age': 'float32'\n",
    "    }\n",
    ")\n",
    "customers = reduce_mem_usage(customers, verbose=False)\n",
    "print(f\"‚úì Loaded {len(customers):,} customers\")\n",
    "\n",
    "# Load articles\n",
    "print(\"\\nLoading articles...\")\n",
    "articles = pd.read_csv(\n",
    "    config.DATA_PATH / 'articles.csv',\n",
    "    dtype={'article_id': 'int32'}\n",
    ")\n",
    "articles = reduce_mem_usage(articles, verbose=False)\n",
    "print(f\"‚úì Loaded {len(articles):,} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "006b29ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 2: SELECTING TEMPORAL WINDOW\n",
      "================================================================================\n",
      "Last transaction date: 2020-09-22 00:00:00\n",
      "\n",
      "Using 24 weeks of data for sampling\n",
      "Window: 2020-04-07 to 2020-09-22\n",
      "\n",
      "Filtering transactions from 2020-04-07 onwards...\n",
      "‚úì Retained 7,561,154 transactions (7.56M)\n",
      "  Week range: 0 to 24\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: TEMPORAL WINDOW SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 2: SELECTING TEMPORAL WINDOW\")\n",
    "\n",
    "# Get the last date in transactions\n",
    "max_date = transactions['t_dat'].max()\n",
    "print(f\"Last transaction date: {max_date}\")\n",
    "\n",
    "# Calculate cutoff dates for the full window\n",
    "window_start = max_date - timedelta(weeks=config.TOTAL_WEEKS)\n",
    "print(f\"\\nUsing {config.TOTAL_WEEKS} weeks of data for sampling\")\n",
    "print(f\"Window: {window_start.date()} to {max_date.date()}\")\n",
    "\n",
    "# Filter transactions to our window\n",
    "print(f\"\\nFiltering transactions from {window_start.date()} onwards...\")\n",
    "transactions = transactions[transactions['t_dat'] >= window_start].copy()\n",
    "print(f\"‚úì Retained {len(transactions):,} transactions ({len(transactions)/1e6:.2f}M)\")\n",
    "\n",
    "# Add week number (relative to window start)\n",
    "transactions['week'] = ((transactions['t_dat'] - window_start).dt.days // 7).astype(np.int8)\n",
    "print(f\"  Week range: {transactions['week'].min()} to {transactions['week'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e7e0e80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 3: USER-BASED STRATIFIED SAMPLING\n",
      "================================================================================\n",
      "Calculating user activity metrics...\n",
      "Total users in window: 719,806\n",
      "  Avg purchases per user: 10.50\n",
      "  Avg active weeks per user: 2.61\n",
      "\n",
      "User segments:\n",
      "  Cold start users (‚â§1 purchases): 72,280\n",
      "  Regular users (‚â•1 purchases): 719,806\n",
      "\n",
      "Sampling targets:\n",
      "  Cold start: 7,500 users (15.0%)\n",
      "  Regular: 42,500 users (85.0%)\n",
      "\n",
      "‚úì Sampled 7,500 cold start users\n",
      "\n",
      "Regular user activity distribution:\n",
      "  low: 331,386 users (46.0%)\n",
      "  medium: 161,171 users (22.4%)\n",
      "  high: 133,525 users (18.6%)\n",
      "  very_high: 79,738 users (11.1%)\n",
      "  extreme: 13,986 users (1.9%)\n",
      "\n",
      "Performing stratified sampling to get 42,500 regular users...\n",
      "\n",
      "Samples per activity level:\n",
      "  low: 19,566 users\n",
      "  medium: 9,516 users\n",
      "  high: 7,884 users\n",
      "  very_high: 4,708 users\n",
      "  extreme: 826 users\n",
      "\n",
      "‚úì Total selected users: 49,576\n",
      "  - Cold start: 7,500 (15.1%)\n",
      "  - Regular: 42,500 (85.7%)\n",
      "\n",
      "Sampled users statistics:\n",
      "  Avg purchases: 9.14\n",
      "  Median purchases: 5.00\n",
      "  Min purchases: 1\n",
      "  Max purchases: 342\n",
      "  Avg active weeks: 2.38\n",
      "  Purchases std: 13.05\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: USER-BASED STRATIFIED SAMPLING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 3: USER-BASED STRATIFIED SAMPLING\")\n",
    "\n",
    "# Calculate user activity across all weeks\n",
    "print(\"Calculating user activity metrics...\")\n",
    "user_activity = transactions.groupby('customer_id').agg({\n",
    "    'article_id': 'count',  # Total purchases\n",
    "    'week': ['min', 'max', 'nunique']  # Week span and diversity\n",
    "}).reset_index()\n",
    "\n",
    "user_activity.columns = ['customer_id', 'total_purchases', 'first_week', 'last_week', 'active_weeks']\n",
    "user_activity['week_span'] = user_activity['last_week'] - user_activity['first_week'] + 1\n",
    "\n",
    "print(f\"Total users in window: {len(user_activity):,}\")\n",
    "print(f\"  Avg purchases per user: {user_activity['total_purchases'].mean():.2f}\")\n",
    "print(f\"  Avg active weeks per user: {user_activity['active_weeks'].mean():.2f}\")\n",
    "\n",
    "# Separate cold start and regular users\n",
    "if config.INCLUDE_COLD_START:\n",
    "    cold_start_users = user_activity[\n",
    "        user_activity['total_purchases'] <= config.COLD_START_MAX_PURCHASES\n",
    "    ].copy()\n",
    "    regular_users = user_activity[\n",
    "        user_activity['total_purchases'] >= config.MIN_USER_PURCHASES\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nUser segments:\")\n",
    "    print(f\"  Cold start users (‚â§{config.COLD_START_MAX_PURCHASES} purchases): {len(cold_start_users):,}\")\n",
    "    print(f\"  Regular users (‚â•{config.MIN_USER_PURCHASES} purchases): {len(regular_users):,}\")\n",
    "    \n",
    "    # Calculate target counts\n",
    "    n_cold_start_target = int(config.TARGET_USERS * config.COLD_START_RATIO)\n",
    "    n_regular_target = config.TARGET_USERS - n_cold_start_target\n",
    "    \n",
    "    print(f\"\\nSampling targets:\")\n",
    "    print(f\"  Cold start: {n_cold_start_target:,} users ({config.COLD_START_RATIO*100:.1f}%)\")\n",
    "    print(f\"  Regular: {n_regular_target:,} users ({(1-config.COLD_START_RATIO)*100:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    # Filter users with minimum activity\n",
    "    regular_users = user_activity[\n",
    "        user_activity['total_purchases'] >= config.MIN_USER_PURCHASES\n",
    "    ].copy()\n",
    "    cold_start_users = pd.DataFrame()\n",
    "    n_cold_start_target = 0\n",
    "    n_regular_target = config.TARGET_USERS\n",
    "    \n",
    "    print(f\"\\nUsers with >= {config.MIN_USER_PURCHASES} purchases: {len(regular_users):,}\")\n",
    "\n",
    "# Sample cold start users (if enabled)\n",
    "sampled_cold_start = []\n",
    "if config.INCLUDE_COLD_START and len(cold_start_users) > 0:\n",
    "    n_cold_sample = min(n_cold_start_target, len(cold_start_users))\n",
    "    sampled_cold_start = cold_start_users['customer_id'].sample(\n",
    "        n=n_cold_sample, \n",
    "        random_state=config.RANDOM_STATE\n",
    "    ).tolist()\n",
    "    print(f\"\\n‚úì Sampled {len(sampled_cold_start):,} cold start users\")\n",
    "\n",
    "# Sample regular users with stratification\n",
    "if config.STRATIFY_BY_ACTIVITY and len(regular_users) > 0:\n",
    "    regular_users['activity_level'] = pd.cut(\n",
    "        regular_users['total_purchases'],\n",
    "        bins=config.ACTIVITY_BINS,\n",
    "        labels=config.ACTIVITY_LABELS\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRegular user activity distribution:\")\n",
    "    activity_dist = regular_users['activity_level'].value_counts().sort_index()\n",
    "    for level, count in activity_dist.items():\n",
    "        print(f\"  {level}: {count:,} users ({100*count/len(regular_users):.1f}%)\")\n",
    "    \n",
    "    # Stratified sampling for regular users\n",
    "    print(f\"\\nPerforming stratified sampling to get {n_regular_target:,} regular users...\")\n",
    "    \n",
    "    # Calculate samples per stratum (proportional)\n",
    "    samples_per_stratum = (activity_dist / activity_dist.sum() * n_regular_target).round().astype(int)\n",
    "    \n",
    "    # Adjust for rounding errors\n",
    "    diff = n_regular_target - samples_per_stratum.sum()\n",
    "    if diff != 0:\n",
    "        largest_stratum = samples_per_stratum.idxmax()\n",
    "        samples_per_stratum[largest_stratum] += diff\n",
    "    \n",
    "    print(\"\\nSamples per activity level:\")\n",
    "    for level, n_samples in samples_per_stratum.items():\n",
    "        print(f\"  {level}: {n_samples:,} users\")\n",
    "    \n",
    "    # Sample from each stratum\n",
    "    sampled_regular = []\n",
    "    for level in config.ACTIVITY_LABELS:\n",
    "        stratum_users = regular_users[regular_users['activity_level'] == level]['customer_id']\n",
    "        n_sample = min(samples_per_stratum[level], len(stratum_users))\n",
    "        if n_sample > 0:\n",
    "            sampled = stratum_users.sample(n=n_sample, random_state=config.RANDOM_STATE)\n",
    "            sampled_regular.extend(sampled.tolist())\n",
    "\n",
    "else:\n",
    "    # Simple random sampling for regular users\n",
    "    print(f\"\\nPerforming random sampling to get {n_regular_target:,} regular users...\")\n",
    "    n_sample = min(n_regular_target, len(regular_users))\n",
    "    sampled_regular = regular_users['customer_id'].sample(\n",
    "        n=n_sample, \n",
    "        random_state=config.RANDOM_STATE\n",
    "    ).tolist()\n",
    "\n",
    "# Combine both groups\n",
    "selected_users = set(sampled_cold_start + sampled_regular)\n",
    "\n",
    "print(f\"\\n‚úì Total selected users: {len(selected_users):,}\")\n",
    "if config.INCLUDE_COLD_START:\n",
    "    print(f\"  - Cold start: {len(sampled_cold_start):,} ({100*len(sampled_cold_start)/len(selected_users):.1f}%)\")\n",
    "    print(f\"  - Regular: {len(sampled_regular):,} ({100*len(sampled_regular)/len(selected_users):.1f}%)\")\n",
    "\n",
    "# Verify sampling quality\n",
    "sampled_activity = user_activity[user_activity['customer_id'].isin(selected_users)]\n",
    "print(f\"\\nSampled users statistics:\")\n",
    "print(f\"  Avg purchases: {sampled_activity['total_purchases'].mean():.2f}\")\n",
    "print(f\"  Median purchases: {sampled_activity['total_purchases'].median():.2f}\")\n",
    "print(f\"  Min purchases: {sampled_activity['total_purchases'].min():.0f}\")\n",
    "print(f\"  Max purchases: {sampled_activity['total_purchases'].max():.0f}\")\n",
    "print(f\"  Avg active weeks: {sampled_activity['active_weeks'].mean():.2f}\")\n",
    "print(f\"  Purchases std: {sampled_activity['total_purchases'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d35452de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 4: FILTERING TRANSACTIONS TO SAMPLED USERS\n",
      "================================================================================\n",
      "‚úì Retained 453,143 transactions\n",
      "  Reduction: 0.0% (based on sampled users)\n",
      "\n",
      "Temporal splits (from 24 week window):\n",
      "  Training:   2020-06-29 to 2020-09-14 (11 weeks)\n",
      "  Validation: 2020-09-15 to 2020-09-22 (1 week)\n",
      "\n",
      "Dataset split:\n",
      "  Training transactions: 436,663\n",
      "  Validation transactions: 16,480\n",
      "  Users in validation: 4,943 (10.0% of sampled)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: FILTER TRANSACTIONS TO SAMPLED USERS\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 4: FILTERING TRANSACTIONS TO SAMPLED USERS\")\n",
    "\n",
    "# Filter transactions\n",
    "transactions = transactions[transactions['customer_id'].isin(selected_users)].copy()\n",
    "print(f\"‚úì Retained {len(transactions):,} transactions\")\n",
    "print(f\"  Reduction: {100 * (1 - len(transactions) / len(transactions)):.1f}% (based on sampled users)\")\n",
    "\n",
    "# Now create train/val split\n",
    "val_end_date = max_date\n",
    "val_start_date = val_end_date - timedelta(weeks=config.N_VAL_WEEKS)\n",
    "train_end_date = val_start_date - timedelta(days=1)\n",
    "train_start_date = train_end_date - timedelta(weeks=config.N_TRAIN_WEEKS)\n",
    "\n",
    "print(f\"\\nTemporal splits (from {config.TOTAL_WEEKS} week window):\")\n",
    "print(f\"  Training:   {train_start_date.date()} to {train_end_date.date()} ({config.N_TRAIN_WEEKS} weeks)\")\n",
    "print(f\"  Validation: {val_start_date.date()} to {val_end_date.date()} ({config.N_VAL_WEEKS} week)\")\n",
    "\n",
    "# Split transactions\n",
    "train_transactions = transactions[transactions['t_dat'] <= train_end_date].copy()\n",
    "val_transactions = transactions[transactions['t_dat'] > train_end_date].copy()\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training transactions: {len(train_transactions):,}\")\n",
    "print(f\"  Validation transactions: {len(val_transactions):,}\")\n",
    "\n",
    "# Check how many sampled users appear in validation\n",
    "val_users = set(val_transactions['customer_id'].unique())\n",
    "print(f\"  Users in validation: {len(val_users):,} ({100*len(val_users)/len(selected_users):.1f}% of sampled)\")\n",
    "\n",
    "del transactions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06bd869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 5: ITEM FILTERING\n",
      "================================================================================\n",
      "Unique items in training: 28,951\n",
      "Items with >= 5 purchases: 14,851\n",
      "Items in validation: 5,730\n",
      "\n",
      "Total selected items: 16,616\n",
      "\n",
      "After item filtering:\n",
      "  Training transactions: 412,156\n",
      "  Validation transactions: 16,480\n",
      "  Articles retained: 16,616\n",
      "  Customers retained: 49,576\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: ITEM FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 5: ITEM FILTERING\")\n",
    "\n",
    "# Count purchases per item in training window\n",
    "item_counts = train_transactions['article_id'].value_counts()\n",
    "print(f\"Unique items in training: {len(item_counts):,}\")\n",
    "\n",
    "# Keep items with minimum purchases\n",
    "valid_items = set(item_counts[item_counts >= config.MIN_ITEM_PURCHASES].index)\n",
    "print(f\"Items with >= {config.MIN_ITEM_PURCHASES} purchases: {len(valid_items):,}\")\n",
    "\n",
    "# Also include all items from validation (even if rare in training)\n",
    "val_items = set(val_transactions['article_id'].unique())\n",
    "print(f\"Items in validation: {len(val_items):,}\")\n",
    "\n",
    "# Combine\n",
    "selected_items = valid_items.union(val_items)\n",
    "print(f\"\\nTotal selected items: {len(selected_items):,}\")\n",
    "\n",
    "# Filter transactions\n",
    "train_transactions = train_transactions[train_transactions['article_id'].isin(selected_items)].copy()\n",
    "val_transactions = val_transactions[val_transactions['article_id'].isin(selected_items)].copy()\n",
    "\n",
    "print(f\"\\nAfter item filtering:\")\n",
    "print(f\"  Training transactions: {len(train_transactions):,}\")\n",
    "print(f\"  Validation transactions: {len(val_transactions):,}\")\n",
    "\n",
    "# Filter articles and customers tables\n",
    "articles = articles[articles['article_id'].isin(selected_items)].copy()\n",
    "customers = customers[customers['customer_id'].isin(selected_users)].copy()\n",
    "\n",
    "print(f\"  Articles retained: {len(articles):,}\")\n",
    "print(f\"  Customers retained: {len(customers):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae3ed3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 6: MEMORY OPTIMIZATION\n",
      "================================================================================\n",
      "Before optimization:\n",
      "  train_transactions: 57.78 MB\n",
      "  val_transactions: 2.31 MB\n",
      "  customers: 18.34 MB\n",
      "  articles: 17.70 MB\n",
      "Memory usage decreased from 13.36 MB to 13.36 MB (0.0% reduction)\n",
      "Memory usage decreased from 0.53 MB to 0.53 MB (0.0% reduction)\n",
      "\n",
      "After optimization:\n",
      "  train_transactions: 57.78 MB\n",
      "  val_transactions: 2.31 MB\n",
      "  customers: 12.94 MB\n",
      "  articles: 16.92 MB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: DATA TYPE OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 6: MEMORY OPTIMIZATION\")\n",
    "\n",
    "print(\"Before optimization:\")\n",
    "print(f\"  train_transactions: {train_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  val_transactions: {val_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  customers: {customers.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  articles: {articles.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Optimize transactions\n",
    "train_transactions = reduce_mem_usage(train_transactions)\n",
    "val_transactions = reduce_mem_usage(val_transactions)\n",
    "\n",
    "# Convert categorical columns\n",
    "for col in ['product_code', 'product_type_no', 'graphical_appearance_no', \n",
    "            'colour_group_code', 'perceived_colour_value_id', 'perceived_colour_master_id',\n",
    "            'department_no', 'index_code', 'index_group_no', 'section_no', 'garment_group_no']:\n",
    "    if col in articles.columns:\n",
    "        articles[col] = articles[col].astype('category')\n",
    "\n",
    "# Optimize customer categoricals\n",
    "for col in ['club_member_status', 'fashion_news_frequency', 'postal_code']:\n",
    "    if col in customers.columns:\n",
    "        customers[col] = customers[col].astype('category')\n",
    "\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(f\"  train_transactions: {train_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  val_transactions: {val_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  customers: {customers.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  articles: {articles.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88760620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 7: DATA VALIDATION & EDA\n",
      "================================================================================\n",
      "Data validation:\n",
      "  ‚úì No null customer_ids in train: True\n",
      "  ‚úì No null article_ids in train: True\n",
      "\n",
      "Weekly activity distribution (sampled users):\n",
      "  Week 0: 4,926 active users\n",
      "  Week 1: 3,240 active users\n",
      "  Week 2: 3,976 active users\n",
      "  Week 3: 3,284 active users\n",
      "  Week 4: 3,771 active users\n",
      "  Week 5: 4,157 active users\n",
      "  Week 6: 5,213 active users\n",
      "  Week 7: 5,419 active users\n",
      "  Week 8: 4,900 active users\n",
      "  Week 9: 4,432 active users\n",
      "  Week 10: 7,090 active users\n",
      "  Week 11: 7,371 active users\n",
      "  Week 12: 5,377 active users\n",
      "  Week 13: 4,692 active users\n",
      "  Week 14: 4,717 active users\n",
      "  Week 15: 4,910 active users\n",
      "  Week 16: 4,985 active users\n",
      "  Week 17: 4,756 active users\n",
      "  Week 18: 4,366 active users\n",
      "  Week 19: 4,289 active users\n",
      "  Week 20: 5,006 active users\n",
      "  Week 21: 4,634 active users\n",
      "  Week 22: 4,647 active users\n",
      "\n",
      "Purchase distribution in validation week:\n",
      "  Mean purchases per user: 3.33\n",
      "  Median purchases per user: 2\n",
      "  Users with 1 purchase: 1,689\n",
      "  Users with 2-5 purchases: 2,430\n",
      "  Users with 6+ purchases: 824\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: DATA VALIDATION & EDA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 7: DATA VALIDATION & EDA\")\n",
    "\n",
    "# Validation checks\n",
    "print(\"Data validation:\")\n",
    "print(f\"  ‚úì No null customer_ids in train: {train_transactions['customer_id'].isnull().sum() == 0}\")\n",
    "print(f\"  ‚úì No null article_ids in train: {train_transactions['article_id'].isnull().sum() == 0}\")\n",
    "\n",
    "# Weekly distribution\n",
    "print(\"\\nWeekly activity distribution (sampled users):\")\n",
    "weekly_users = train_transactions.groupby('week')['customer_id'].nunique()\n",
    "for week, n_users in weekly_users.items():\n",
    "    print(f\"  Week {week}: {n_users:,} active users\")\n",
    "\n",
    "# Purchase distribution\n",
    "print(\"\\nPurchase distribution in validation week:\")\n",
    "if len(val_transactions) > 0:\n",
    "    val_user_purchases = val_transactions.groupby('customer_id').size()\n",
    "    print(f\"  Mean purchases per user: {val_user_purchases.mean():.2f}\")\n",
    "    print(f\"  Median purchases per user: {val_user_purchases.median():.0f}\")\n",
    "    print(f\"  Users with 1 purchase: {(val_user_purchases == 1).sum():,}\")\n",
    "    print(f\"  Users with 2-5 purchases: {((val_user_purchases >= 2) & (val_user_purchases <= 5)).sum():,}\")\n",
    "    print(f\"  Users with 6+ purchases: {(val_user_purchases >= 6).sum():,}\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è No validation transactions for sampled users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34849df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 8: CREATING VALIDATION GROUND TRUTH\n",
      "================================================================================\n",
      "Validation ground truth:\n",
      "  Users: 4,943\n",
      "  Total purchases: 16,480\n",
      "  Avg purchases per user: 3.33\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: CREATE VALIDATION GROUND TRUTH\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 8: CREATING VALIDATION GROUND TRUTH\")\n",
    "\n",
    "# Create validation ground truth\n",
    "if len(val_transactions) > 0:\n",
    "    val_ground_truth = (\n",
    "        val_transactions\n",
    "        .groupby('customer_id')['article_id']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "        .rename(columns={'article_id': 'purchased_articles'})\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation ground truth:\")\n",
    "    print(f\"  Users: {len(val_ground_truth):,}\")\n",
    "    print(f\"  Total purchases: {val_ground_truth['purchased_articles'].apply(len).sum():,}\")\n",
    "    print(f\"  Avg purchases per user: {val_ground_truth['purchased_articles'].apply(len).mean():.2f}\")\n",
    "else:\n",
    "    val_ground_truth = pd.DataFrame(columns=['customer_id', 'purchased_articles'])\n",
    "    print(\"‚ö†Ô∏è Empty validation ground truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "65aee445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 9: SAVING PROCESSED DATA\n",
      "================================================================================\n",
      "Saving files...\n",
      "  ‚úì train_transactions.parquet (412,156 rows)\n",
      "  ‚úì val_transactions.parquet (16,480 rows)\n",
      "  ‚úì customers.parquet (49,576 rows)\n",
      "  ‚úì articles.parquet (16,616 rows)\n",
      "  ‚úì val_ground_truth.parquet (4,943 rows)\n",
      "  ‚úì user_activity_stats.parquet (49,576 rows)\n",
      "  ‚úì metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: SAVE PROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 9: SAVING PROCESSED DATA\")\n",
    "\n",
    "# Save to parquet\n",
    "print(\"Saving files...\")\n",
    "\n",
    "train_transactions.to_parquet(config.OUTPUT_PATH / 'train_transactions.parquet', index=False)\n",
    "print(f\"  ‚úì train_transactions.parquet ({len(train_transactions):,} rows)\")\n",
    "\n",
    "val_transactions.to_parquet(config.OUTPUT_PATH / 'val_transactions.parquet', index=False)\n",
    "print(f\"  ‚úì val_transactions.parquet ({len(val_transactions):,} rows)\")\n",
    "\n",
    "customers.to_parquet(config.OUTPUT_PATH / 'customers.parquet', index=False)\n",
    "print(f\"  ‚úì customers.parquet ({len(customers):,} rows)\")\n",
    "\n",
    "articles.to_parquet(config.OUTPUT_PATH / 'articles.parquet', index=False)\n",
    "print(f\"  ‚úì articles.parquet ({len(articles):,} rows)\")\n",
    "\n",
    "val_ground_truth.to_parquet(config.OUTPUT_PATH / 'val_ground_truth.parquet', index=False)\n",
    "print(f\"  ‚úì val_ground_truth.parquet ({len(val_ground_truth):,} rows)\")\n",
    "\n",
    "# Save user activity for analysis\n",
    "sampled_activity.to_parquet(config.OUTPUT_PATH / 'user_activity_stats.parquet', index=False)\n",
    "print(f\"  ‚úì user_activity_stats.parquet ({len(sampled_activity):,} rows)\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_weeks': config.TOTAL_WEEKS,\n",
    "    'train_weeks': config.N_TRAIN_WEEKS,\n",
    "    'val_weeks': config.N_VAL_WEEKS,\n",
    "    'train_start_date': str(train_start_date.date()),\n",
    "    'train_end_date': str(train_end_date.date()),\n",
    "    'val_start_date': str(val_start_date.date()),\n",
    "    'val_end_date': str(val_end_date.date()),\n",
    "    'target_users': config.TARGET_USERS,\n",
    "    'actual_users': len(selected_users),\n",
    "    'users_in_validation': len(val_users),\n",
    "    'cold_start_users': len(sampled_cold_start) if config.INCLUDE_COLD_START else 0,\n",
    "    'regular_users': len(sampled_regular) if config.INCLUDE_COLD_START else len(selected_users),\n",
    "    'cold_start_ratio': config.COLD_START_RATIO if config.INCLUDE_COLD_START else 0,\n",
    "    'n_items': len(selected_items),\n",
    "    'n_train_transactions': len(train_transactions),\n",
    "    'n_val_transactions': len(val_transactions),\n",
    "    'stratified': config.STRATIFY_BY_ACTIVITY,\n",
    "    'min_user_purchases': config.MIN_USER_PURCHASES,\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(config.OUTPUT_PATH / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"  ‚úì metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30b35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PREPROCESSING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Final dataset summary:\n",
      "  üìÖ Total weeks considered: 24\n",
      "  üìÖ Training weeks: 11\n",
      "  üìÖ Validation weeks: 1\n",
      "  üë• Target users: 50,000\n",
      "  üë• Actual sampled users: 49,576\n",
      "  ‚ùÑÔ∏è  Cold start users: 7,500 (15.1%)\n",
      "  üî• Regular users: 42,500 (85.7%)\n",
      "  üë• Users in validation: 4,943 (10.0%)\n",
      "  üõçÔ∏è  Items: 16,616\n",
      "  üìä Train transactions: 412,156\n",
      "  üìä Val transactions: 16,480\n",
      "  üìä Avg transactions per user (train): 8.31\n",
      "\n",
      "  üìà Sampling was stratified by user activity level\n",
      "  ‚ùÑÔ∏è  Cold start users included for testing recommendations with limited history\n",
      "\n",
      "‚úÖ Ready for Stage 2: Recall Strategies!\n",
      "\n",
      "Next steps:\n",
      "  1. Review the saved files in /kaggle/working/\n",
      "  2. Check metadata.json for dataset info\n",
      "  3. Analyze user_activity_stats.parquet for sampling quality\n",
      "  4. Proceed to Stage 2 when ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"PREPROCESSING COMPLETE!\")\n",
    "\n",
    "print(\"\\nFinal dataset summary:\")\n",
    "print(f\"Total weeks considered: {config.TOTAL_WEEKS}\")\n",
    "print(f\"Training weeks: {config.N_TRAIN_WEEKS}\")\n",
    "print(f\"Validation weeks: {config.N_VAL_WEEKS}\")\n",
    "print(f\"Target users: {config.TARGET_USERS:,}\")\n",
    "print(f\"Actual sampled users: {len(selected_users):,}\")\n",
    "if config.INCLUDE_COLD_START:\n",
    "    print(f\"Cold start users: {len(sampled_cold_start):,} ({100*len(sampled_cold_start)/len(selected_users):.1f}%)\")\n",
    "    print(f\"Regular users: {len(sampled_regular):,} ({100*len(sampled_regular)/len(selected_users):.1f}%)\")\n",
    "print(f\"Users in validation: {len(val_users):,} ({100*len(val_users)/len(selected_users):.1f}%)\")\n",
    "print(f\"Items: {len(selected_items):,}\")\n",
    "print(f\"Train transactions: {len(train_transactions):,}\")\n",
    "print(f\"Val transactions: {len(val_transactions):,}\")\n",
    "print(f\"Avg transactions per user (train): {len(train_transactions)/len(selected_users):.2f}\")\n",
    "\n",
    "if config.STRATIFY_BY_ACTIVITY:\n",
    "    print(\"\\n Sampling was stratified by user activity level\")\n",
    "if config.INCLUDE_COLD_START:\n",
    "    print(f\"Cold start users included for testing recommendations with limited history\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a91595",
   "metadata": {},
   "source": [
    "### Stage 2: Generating Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8b514906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from text_features import integrate_text_features_stage2\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e79aefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    print(f\"  üíæ Memory: {mem:.2f} GB\")\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Aggressive garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "98b7f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_2')\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    \n",
    "    # Recall configuration - REDUCED for memory\n",
    "    N_REPURCHASE_CANDIDATES = 25  # Reduced from 30\n",
    "    N_POPULARITY_CANDIDATES = 25  # Reduced from 30\n",
    "    N_COPURCHASE_CANDIDATES = 15  # Reduced from 20\n",
    "    N_USERKNN_CANDIDATES = 15     # Reduced from 20\n",
    "    N_CATEGORY_CANDIDATES = 15    # Reduced from 20\n",
    "    \n",
    "    # Processing parameters\n",
    "    USER_CHUNK_SIZE = 1000  # Process users in chunks\n",
    "    BASKET_CHUNK_SIZE = 5000  # Process baskets in chunks\n",
    "    \n",
    "    # EMERGENCY MODE: Use only recent data for repurchase\n",
    "    USE_RECENT_ONLY_REPURCHASE = True  # Set to True if kernel keeps crashing\n",
    "    REPURCHASE_RECENT_WEEKS = 8  # Only use last 8 weeks for repurchase\n",
    "    \n",
    "    # Item-to-Item CF parameters\n",
    "    MIN_ITEM_SUPPORT = 3\n",
    "    MAX_ITEM_NEIGHBORS = 30  # Reduced from 50\n",
    "    \n",
    "    # User-KNN parameters (ONLY for validation users)\n",
    "    N_SIMILAR_USERS = 20  # Reduced from 30\n",
    "    MIN_COMMON_ITEMS = 2\n",
    "    \n",
    "    # Time decay\n",
    "    REPURCHASE_DECAY_RATE = 0.05\n",
    "    POPULARITY_WINDOW_WEEKS = 2\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4ae37e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def time_decay_score(days_ago, decay_rate=0.05):\n",
    "    \"\"\"Vectorized time decay\"\"\"\n",
    "    return np.exp(-decay_rate * days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3188332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading data files...\n",
      "‚úì Train transactions: 412,156\n",
      "  üíæ Memory: 0.35 GB\n",
      "‚úì Users: 47,543, Items: 15,932, Val users: 4,943\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"LOADING DATA\")\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "train_transactions = pd.read_parquet(config.DATA_PATH / 'train_transactions.parquet')\n",
    "val_ground_truth = pd.read_parquet(config.DATA_PATH / 'val_ground_truth.parquet')\n",
    "articles = pd.read_parquet(config.DATA_PATH / 'articles.parquet')\n",
    "\n",
    "print(f\"‚úì Train transactions: {len(train_transactions):,}\")\n",
    "print_memory()\n",
    "\n",
    "all_users = train_transactions['customer_id'].unique()\n",
    "all_items = train_transactions['article_id'].unique()\n",
    "val_users = set(val_ground_truth['customer_id'].unique())\n",
    "max_date = train_transactions['t_dat'].max()\n",
    "\n",
    "print(f\"‚úì Users: {len(all_users):,}, Items: {len(all_items):,}, Val users: {len(val_users):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2f84c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 1: REPURCHASE (CHUNKED)\n",
      "================================================================================\n",
      "\n",
      "Processing in chunks to save memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79af4e2f8ae4298a6b6d4aea1dcb48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User chunks:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 325,177 repurchase candidates\n",
      "  üíæ Memory: 1.36 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 1: REPURCHASE - CHUNKED PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 1: REPURCHASE (CHUNKED)\")\n",
    "\n",
    "print(\"Processing in chunks to save memory...\")\n",
    "repurchase_chunks = []\n",
    "\n",
    "# Split users into chunks\n",
    "user_chunks = np.array_split(all_users, max(1, len(all_users) // config.USER_CHUNK_SIZE))\n",
    "\n",
    "for i, user_chunk in enumerate(tqdm(user_chunks, desc=\"User chunks\")):\n",
    "    # Filter transactions for this chunk\n",
    "    chunk_trans = train_transactions[\n",
    "        train_transactions['customer_id'].isin(user_chunk)\n",
    "    ].copy()\n",
    "    \n",
    "    # Get last purchase per user-item\n",
    "    user_item_last = (\n",
    "        chunk_trans\n",
    "        .groupby(['customer_id', 'article_id'], as_index=False)['t_dat']\n",
    "        .max()\n",
    "    )\n",
    "    \n",
    "    # Calculate scores (vectorized) - with NaN handling\n",
    "    user_item_last['days_ago'] = (max_date - user_item_last['t_dat']).dt.days\n",
    "    \n",
    "    # Drop any NaN values before converting to int\n",
    "    user_item_last = user_item_last.dropna(subset=['days_ago'])\n",
    "    \n",
    "    # Now safe to convert to int\n",
    "    user_item_last['days_ago'] = user_item_last['days_ago'].astype(np.int16)\n",
    "    user_item_last['repurchase_score'] = time_decay_score(\n",
    "        user_item_last['days_ago'].values, \n",
    "        config.REPURCHASE_DECAY_RATE\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    # Get top N per user\n",
    "    top_candidates = (\n",
    "        user_item_last\n",
    "        .sort_values(['customer_id', 'repurchase_score'], ascending=[True, False])\n",
    "        .groupby('customer_id', as_index=False)\n",
    "        .head(config.N_REPURCHASE_CANDIDATES)\n",
    "        [['customer_id', 'article_id', 'repurchase_score']]\n",
    "    )\n",
    "    \n",
    "    repurchase_chunks.append(top_candidates)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk_trans, user_item_last, top_candidates\n",
    "    force_garbage_collection()\n",
    "\n",
    "# Combine chunks\n",
    "repurchase_candidates = pd.concat(repurchase_chunks, ignore_index=True)\n",
    "del repurchase_chunks\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"‚úì Generated {len(repurchase_candidates):,} repurchase candidates\")\n",
    "print_memory()\n",
    "\n",
    "# Save intermediate result\n",
    "repurchase_candidates.to_parquet(config.OUTPUT_PATH / 'temp_repurchase.parquet', index=False)\n",
    "del repurchase_candidates\n",
    "force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cab3709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 2: POPULARITY\n",
      "================================================================================\n",
      "\n",
      "Using 32,152 recent transactions\n",
      "‚úì Top 25 popular items\n",
      "Creating popularity candidates in chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610c0f1fb31b42ca878e0029d3139da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Popularity chunks:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 1,188,575 popularity candidates\n",
      "  üíæ Memory: 1.47 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 2: POPULARITY\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 2: POPULARITY\")\n",
    "\n",
    "cutoff_date = max_date - timedelta(weeks=config.POPULARITY_WINDOW_WEEKS)\n",
    "recent_trans = train_transactions[train_transactions['t_dat'] >= cutoff_date].copy()\n",
    "\n",
    "print(f\"Using {len(recent_trans):,} recent transactions\")\n",
    "\n",
    "# Vectorized calculations\n",
    "recent_trans['days_ago'] = (max_date - recent_trans['t_dat']).dt.days\n",
    "\n",
    "# Drop NaN values\n",
    "recent_trans = recent_trans.dropna(subset=['days_ago'])\n",
    "\n",
    "# Convert to int\n",
    "recent_trans['days_ago'] = recent_trans['days_ago'].astype(np.int16)\n",
    "recent_trans['weight'] = time_decay_score(recent_trans['days_ago'].values, 0.1).astype(np.float32)\n",
    "\n",
    "# Aggregate\n",
    "item_popularity = (\n",
    "    recent_trans\n",
    "    .groupby('article_id', as_index=False)\n",
    "    .agg({'weight': 'sum', 'customer_id': 'nunique'})\n",
    "    .rename(columns={'weight': 'weighted_purchases', 'customer_id': 'unique_buyers'})\n",
    ")\n",
    "\n",
    "item_popularity['popularity_score'] = (\n",
    "    0.7 * item_popularity['weighted_purchases'] + \n",
    "    0.3 * item_popularity['unique_buyers']\n",
    ")\n",
    "item_popularity['popularity_score'] = (\n",
    "    item_popularity['popularity_score'] / item_popularity['popularity_score'].max()\n",
    ").astype(np.float32)\n",
    "\n",
    "# Get top items\n",
    "top_items = item_popularity.nlargest(config.N_POPULARITY_CANDIDATES, 'popularity_score')\n",
    "\n",
    "print(f\"‚úì Top {len(top_items)} popular items\")\n",
    "\n",
    "# Create candidates - CHUNKED\n",
    "print(\"Creating popularity candidates in chunks...\")\n",
    "pop_chunks = []\n",
    "\n",
    "for user_chunk in tqdm(np.array_split(all_users, 20), desc=\"Popularity chunks\"):\n",
    "    chunk_df = pd.DataFrame({\n",
    "        'customer_id': np.repeat(user_chunk, len(top_items)),\n",
    "        'article_id': np.tile(top_items['article_id'].values, len(user_chunk))\n",
    "    })\n",
    "    \n",
    "    rank_penalty = np.tile(1 - np.arange(len(top_items)) * 0.01, len(user_chunk))\n",
    "    scores = np.tile(top_items['popularity_score'].values, len(user_chunk))\n",
    "    chunk_df['popularity_score'] = (scores * rank_penalty).astype(np.float32)\n",
    "    \n",
    "    pop_chunks.append(chunk_df)\n",
    "\n",
    "popularity_candidates = pd.concat(pop_chunks, ignore_index=True)\n",
    "del pop_chunks, recent_trans\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"‚úì Generated {len(popularity_candidates):,} popularity candidates\")\n",
    "print_memory()\n",
    "\n",
    "# Save\n",
    "popularity_candidates.to_parquet(config.OUTPUT_PATH / 'temp_popularity.parquet', index=False)\n",
    "item_popularity.to_parquet(config.OUTPUT_PATH / 'item_popularity.parquet', index=False)\n",
    "del popularity_candidates\n",
    "force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b0be7540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 3: CO-PURCHASE (Item-to-Item CF)\n",
      "================================================================================\n",
      "\n",
      "Building co-purchase matrix...\n",
      "  Baskets with 2+ items: 83,983\n",
      "Computing co-purchase frequencies...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bdc51b143544808d1a25443a37e9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing baskets:   0%|          | 0/83983 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Built co-purchase matrix for 15,868 items\n",
      "Computing item-to-item similarity scores...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7706dee3464d258e8be7f25c869886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities:   0%|          | 0/15868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Computed similarities for 10,603 items\n",
      "Saving item-to-item similarity matrix...\n",
      "  ‚úì Saved item_to_items.pkl (10,603 items)\n",
      "Generating co-purchase candidates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f6e2cc16274c8e91eeafa954592434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User co-purchase recommendations:   0%|          | 0/47543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 612,807 co-purchase candidates\n",
      "  Users with candidates: 45,344\n",
      "\n",
      "Saving co-purchase candidates...\n",
      "‚úì Saved temp_copurchase.parquet (612,807 rows)\n",
      "‚úì Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 3: CO-PURCHASE (Item-to-Item CF) - WITH PARQUET SAVING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 3: CO-PURCHASE (Item-to-Item CF)\")\n",
    "\n",
    "# Check if already computed\n",
    "if (config.OUTPUT_PATH / 'temp_copurchase.parquet').exists():\n",
    "    print(\"‚ö° Found existing co-purchase candidates, loading...\")\n",
    "    copurchase_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_copurchase.parquet')\n",
    "    print(f\"‚úì Loaded {len(copurchase_candidates):,} co-purchase candidates\")\n",
    "    print(f\"  Users with candidates: {copurchase_candidates['customer_id'].nunique():,}\")\n",
    "else:\n",
    "    print(\"Building co-purchase matrix...\")\n",
    "\n",
    "    # Create item-to-item co-purchase matrix\n",
    "    # Group by transaction/basket (same user, same day)\n",
    "    train_transactions['basket_id'] = (\n",
    "        train_transactions['customer_id'].astype(str) + '_' + \n",
    "        train_transactions['t_dat'].astype(str)\n",
    "    )\n",
    "\n",
    "    # Get baskets with multiple items\n",
    "    basket_items = (\n",
    "        train_transactions\n",
    "        .groupby('basket_id')['article_id']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Filter baskets with at least 2 items\n",
    "    basket_items = basket_items[basket_items['article_id'].apply(len) >= 2]\n",
    "    print(f\"  Baskets with 2+ items: {len(basket_items):,}\")\n",
    "\n",
    "    # Build co-purchase counts\n",
    "    print(\"Computing co-purchase frequencies...\")\n",
    "    copurchase_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for items in tqdm(basket_items['article_id'], desc=\"Processing baskets\"):\n",
    "        # For each pair of items in the basket\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i + 1, len(items)):\n",
    "                item1, item2 = items[i], items[j]\n",
    "                copurchase_counts[item1][item2] += 1\n",
    "                copurchase_counts[item2][item1] += 1\n",
    "\n",
    "    print(f\"‚úì Built co-purchase matrix for {len(copurchase_counts):,} items\")\n",
    "\n",
    "    # Convert to item-to-item similarity scores\n",
    "    print(\"Computing item-to-item similarity scores...\")\n",
    "    item_to_items = {}\n",
    "\n",
    "    for item1 in tqdm(copurchase_counts.keys(), desc=\"Computing similarities\"):\n",
    "        # Get co-purchased items\n",
    "        copurchased = copurchase_counts[item1]\n",
    "        \n",
    "        # Filter by minimum support\n",
    "        copurchased = {\n",
    "            item2: count \n",
    "            for item2, count in copurchased.items() \n",
    "            if count >= config.MIN_ITEM_SUPPORT\n",
    "        }\n",
    "        \n",
    "        if copurchased:\n",
    "            # Sort by count and take top K\n",
    "            top_items = sorted(\n",
    "                copurchased.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:config.MAX_ITEM_NEIGHBORS]\n",
    "            \n",
    "            # Normalize scores\n",
    "            max_count = top_items[0][1]\n",
    "            item_to_items[item1] = [\n",
    "                (item2, count / max_count) \n",
    "                for item2, count in top_items\n",
    "            ]\n",
    "\n",
    "    print(f\"‚úì Computed similarities for {len(item_to_items):,} items\")\n",
    "\n",
    "    # Save item-to-item similarity matrix for potential reuse\n",
    "    print(\"Saving item-to-item similarity matrix...\")\n",
    "    import pickle\n",
    "    with open(config.OUTPUT_PATH / 'item_to_items.pkl', 'wb') as f:\n",
    "        pickle.dump(item_to_items, f)\n",
    "    print(f\"  ‚úì Saved item_to_items.pkl ({len(item_to_items):,} items)\")\n",
    "\n",
    "    # Generate co-purchase candidates for each user\n",
    "    print(\"Generating co-purchase candidates...\")\n",
    "    copurchase_candidates = []\n",
    "\n",
    "    # Get recent purchases for each user (last 10)\n",
    "    user_recent_items = (\n",
    "        train_transactions\n",
    "        .sort_values('t_dat', ascending=False)\n",
    "        .groupby('customer_id')['article_id']\n",
    "        .apply(lambda x: list(x.unique()[:10]))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    for user in tqdm(all_users, desc=\"User co-purchase recommendations\"):\n",
    "        if user not in user_recent_items:\n",
    "            continue\n",
    "        \n",
    "        user_items = user_recent_items[user]\n",
    "        candidate_scores = defaultdict(float)\n",
    "        \n",
    "        # Aggregate scores from all user's items\n",
    "        for user_item in user_items:\n",
    "            if user_item in item_to_items:\n",
    "                for similar_item, score in item_to_items[user_item]:\n",
    "                    if similar_item not in user_items:  # Don't recommend already purchased\n",
    "                        candidate_scores[similar_item] += score\n",
    "        \n",
    "        # Get top N candidates\n",
    "        if candidate_scores:\n",
    "            top_candidates = sorted(\n",
    "                candidate_scores.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:config.N_COPURCHASE_CANDIDATES]\n",
    "            \n",
    "            for item, score in top_candidates:\n",
    "                copurchase_candidates.append({\n",
    "                    'customer_id': user,\n",
    "                    'article_id': item,\n",
    "                    'copurchase_score': score\n",
    "                })\n",
    "\n",
    "    copurchase_candidates = pd.DataFrame(copurchase_candidates)\n",
    "    print(f\"‚úì Generated {len(copurchase_candidates):,} co-purchase candidates\")\n",
    "    print(f\"  Users with candidates: {copurchase_candidates['customer_id'].nunique():,}\")\n",
    "\n",
    "    # Save to parquet\n",
    "    print(\"\\nSaving co-purchase candidates...\")\n",
    "    copurchase_candidates.to_parquet(config.OUTPUT_PATH / 'temp_copurchase.parquet', index=False)\n",
    "    print(f\"‚úì Saved temp_copurchase.parquet ({len(copurchase_candidates):,} rows)\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del basket_items, copurchase_counts, item_to_items, user_recent_items\n",
    "    force_garbage_collection()\n",
    "    print(\"‚úì Memory cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c9998ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 4: USER-KNN COLLABORATIVE FILTERING\n",
      "================================================================================\n",
      "\n",
      "Building user-item matrix...\n",
      "  Matrix size: 47,543 users x 15,932 items\n",
      "Populating user-item matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabed002e19f4ae7aa185d449f7766b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building matrix:   0%|          | 0/62015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Matrix density: 0.0072%\n",
      "Saving user-item matrix...\n",
      "  ‚úì Saved user_item_matrix.npz\n",
      "  ‚úì Saved index mappings\n",
      "Computing user similarities for 4,943 validation users...\n",
      "Computing cosine similarities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b129eb784cd34016ab1c72fe048f2b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Similarity batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 30,107 user-KNN candidates\n",
      "  Users with candidates: 2,018\n",
      "\n",
      "Saving user-KNN candidates...\n",
      "‚úì Saved temp_userknn.parquet (30,107 rows)\n",
      "‚úì Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 4: USER-KNN COLLABORATIVE FILTERING - WITH PARQUET SAVING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 4: USER-KNN COLLABORATIVE FILTERING\")\n",
    "\n",
    "# Check if already computed\n",
    "if (config.OUTPUT_PATH / 'temp_userknn.parquet').exists():\n",
    "    print(\"‚ö° Found existing user-KNN candidates, loading...\")\n",
    "    userknn_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_userknn.parquet')\n",
    "    print(f\"‚úì Loaded {len(userknn_candidates):,} user-KNN candidates\")\n",
    "    print(f\"  Users with candidates: {userknn_candidates['customer_id'].nunique():,}\")\n",
    "else:\n",
    "    print(\"Building user-item matrix...\")\n",
    "\n",
    "    # Create sparse user-item matrix (binary: 1 if purchased, 0 otherwise)\n",
    "    # Map users and items to indices\n",
    "    user_to_idx = {user: idx for idx, user in enumerate(all_users)}\n",
    "    item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
    "\n",
    "    # Create matrix\n",
    "    n_users = len(all_users)\n",
    "    n_items = len(all_items)\n",
    "\n",
    "    print(f\"  Matrix size: {n_users:,} users x {n_items:,} items\")\n",
    "\n",
    "    # Use last 4 weeks for user similarity (more recent = more relevant)\n",
    "    recent_date = max_date - timedelta(weeks=4)\n",
    "    recent_user_items = train_transactions[train_transactions['t_dat'] >= recent_date].copy()\n",
    "\n",
    "    user_item_matrix = lil_matrix((n_users, n_items), dtype=np.int8)\n",
    "\n",
    "    print(\"Populating user-item matrix...\")\n",
    "    for _, row in tqdm(recent_user_items.iterrows(), total=len(recent_user_items), desc=\"Building matrix\"):\n",
    "        user_idx = user_to_idx[row['customer_id']]\n",
    "        item_idx = item_to_idx[row['article_id']]\n",
    "        user_item_matrix[user_idx, item_idx] = 1\n",
    "\n",
    "    # Convert to CSR for efficient operations\n",
    "    user_item_matrix = user_item_matrix.tocsr()\n",
    "    print(f\"‚úì Matrix density: {user_item_matrix.nnz / (n_users * n_items) * 100:.4f}%\")\n",
    "\n",
    "    # Save user-item matrix for potential reuse\n",
    "    print(\"Saving user-item matrix...\")\n",
    "    from scipy.sparse import save_npz\n",
    "    save_npz(config.OUTPUT_PATH / 'user_item_matrix.npz', user_item_matrix)\n",
    "    print(f\"  ‚úì Saved user_item_matrix.npz\")\n",
    "\n",
    "    # Save user/item mappings\n",
    "    import pickle\n",
    "    with open(config.OUTPUT_PATH / 'user_to_idx.pkl', 'wb') as f:\n",
    "        pickle.dump(user_to_idx, f)\n",
    "    with open(config.OUTPUT_PATH / 'item_to_idx.pkl', 'wb') as f:\n",
    "        pickle.dump(item_to_idx, f)\n",
    "    print(f\"  ‚úì Saved index mappings\")\n",
    "\n",
    "    # Compute user-user similarity (only for validation users to save memory)\n",
    "    print(f\"Computing user similarities for {len(val_users):,} validation users...\")\n",
    "\n",
    "    val_user_indices = [user_to_idx[user] for user in val_users if user in user_to_idx]\n",
    "    val_user_matrix = user_item_matrix[val_user_indices]\n",
    "\n",
    "    # Normalize rows\n",
    "    val_user_matrix_norm = normalize(val_user_matrix, norm='l2', axis=1)\n",
    "    user_item_matrix_norm = normalize(user_item_matrix, norm='l2', axis=1)\n",
    "\n",
    "    # Compute similarity (batch processing to avoid memory issues)\n",
    "    print(\"Computing cosine similarities...\")\n",
    "    batch_size = 1000\n",
    "    userknn_candidates = []\n",
    "\n",
    "    for i in tqdm(range(0, len(val_user_indices), batch_size), desc=\"Similarity batches\"):\n",
    "        batch_indices = val_user_indices[i:i+batch_size]\n",
    "        batch_matrix = val_user_matrix_norm[i:i+batch_size]\n",
    "        \n",
    "        # Compute similarity with all users\n",
    "        similarities = cosine_similarity(batch_matrix, user_item_matrix_norm)\n",
    "        \n",
    "        # For each user in batch\n",
    "        for j, user_idx in enumerate(batch_indices):\n",
    "            user = all_users[user_idx]\n",
    "            user_sims = similarities[j]\n",
    "            \n",
    "            # Get top similar users (exclude self)\n",
    "            similar_user_indices = np.argsort(user_sims)[::-1][1:config.N_SIMILAR_USERS+1]\n",
    "            \n",
    "            # Get items purchased by similar users\n",
    "            candidate_scores = defaultdict(float)\n",
    "            user_purchased = set(\n",
    "                train_transactions[train_transactions['customer_id'] == user]['article_id']\n",
    "            )\n",
    "            \n",
    "            for sim_user_idx in similar_user_indices:\n",
    "                sim_score = user_sims[sim_user_idx]\n",
    "                if sim_score < 0.01:  # Skip very dissimilar users\n",
    "                    continue\n",
    "                \n",
    "                sim_user = all_users[sim_user_idx]\n",
    "                sim_user_items = train_transactions[\n",
    "                    train_transactions['customer_id'] == sim_user\n",
    "                ]['article_id'].unique()\n",
    "                \n",
    "                for item in sim_user_items:\n",
    "                    if item not in user_purchased:\n",
    "                        candidate_scores[item] += sim_score\n",
    "            \n",
    "            # Get top N candidates\n",
    "            if candidate_scores:\n",
    "                top_candidates = sorted(\n",
    "                    candidate_scores.items(), \n",
    "                    key=lambda x: x[1], \n",
    "                    reverse=True\n",
    "                )[:config.N_USERKNN_CANDIDATES]\n",
    "                \n",
    "                for item, score in top_candidates:\n",
    "                    userknn_candidates.append({\n",
    "                        'customer_id': user,\n",
    "                        'article_id': item,\n",
    "                        'userknn_score': score\n",
    "                    })\n",
    "\n",
    "    userknn_candidates = pd.DataFrame(userknn_candidates)\n",
    "    print(f\"‚úì Generated {len(userknn_candidates):,} user-KNN candidates\")\n",
    "    print(f\"  Users with candidates: {userknn_candidates['customer_id'].nunique():,}\")\n",
    "\n",
    "    # Save to parquet\n",
    "    print(\"\\nSaving user-KNN candidates...\")\n",
    "    userknn_candidates.to_parquet(config.OUTPUT_PATH / 'temp_userknn.parquet', index=False)\n",
    "    print(f\"‚úì Saved temp_userknn.parquet ({len(userknn_candidates):,} rows)\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del user_item_matrix, val_user_matrix, val_user_matrix_norm, user_item_matrix_norm\n",
    "    del user_to_idx, item_to_idx, recent_user_items\n",
    "    force_garbage_collection()\n",
    "    print(\"‚úì Memory cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6dea05b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 5: CATEGORY-BASED RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "Computing user category preferences...\n",
      "‚úì Computed preferences for 47,543 users\n",
      "Saving user category preferences...\n",
      "  ‚úì Saved user_category_preferences.parquet\n",
      "Saving category popular items...\n",
      "  ‚úì Saved category_popular_items.parquet\n",
      "Generating category-based candidates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dcfb5d931f4f5fab3b305b9d57c3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Category recommendations:   0%|          | 0/142629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generated 1,409,632 category-based candidates\n",
      "  Users with candidates: 47,543\n",
      "\n",
      "Saving category candidates...\n",
      "‚úì Saved temp_category.parquet (1,409,632 rows)\n",
      "‚úì Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 5: CATEGORY-BASED RECOMMENDATIONS - WITH PARQUET SAVING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 5: CATEGORY-BASED RECOMMENDATIONS\")\n",
    "\n",
    "# Check if already computed\n",
    "if (config.OUTPUT_PATH / 'temp_category.parquet').exists():\n",
    "    print(\"‚ö° Found existing category candidates, loading...\")\n",
    "    category_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_category.parquet')\n",
    "    print(f\"‚úì Loaded {len(category_candidates):,} category candidates\")\n",
    "    print(f\"  Users with candidates: {category_candidates['customer_id'].nunique():,}\")\n",
    "else:\n",
    "    print(\"Computing user category preferences...\")\n",
    "\n",
    "    # Get user's category preferences\n",
    "    user_categories = (\n",
    "        train_transactions\n",
    "        .merge(articles[['article_id', 'product_type_no', 'product_group_name']], on='article_id')\n",
    "        .groupby(['customer_id', 'product_type_no'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # Get top 3 categories per user\n",
    "    user_top_categories = (\n",
    "        user_categories\n",
    "        .sort_values(['customer_id', 'count'], ascending=[True, False])\n",
    "        .groupby('customer_id')\n",
    "        .head(3)\n",
    "    )\n",
    "\n",
    "    print(f\"‚úì Computed preferences for {user_top_categories['customer_id'].nunique():,} users\")\n",
    "\n",
    "    # Save user category preferences for potential reuse\n",
    "    print(\"Saving user category preferences...\")\n",
    "    user_top_categories.to_parquet(config.OUTPUT_PATH / 'user_category_preferences.parquet', index=False)\n",
    "    print(f\"  ‚úì Saved user_category_preferences.parquet\")\n",
    "\n",
    "    # Get popular items per category\n",
    "    category_popular_items = (\n",
    "        train_transactions[train_transactions['t_dat'] >= cutoff_date]\n",
    "        .merge(articles[['article_id', 'product_type_no']], on='article_id')\n",
    "        .groupby(['product_type_no', 'article_id'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "        .sort_values(['product_type_no', 'count'], ascending=[True, False])\n",
    "        .groupby('product_type_no')\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    # Save category popular items for potential reuse\n",
    "    print(\"Saving category popular items...\")\n",
    "    category_popular_items.to_parquet(config.OUTPUT_PATH / 'category_popular_items.parquet', index=False)\n",
    "    print(f\"  ‚úì Saved category_popular_items.parquet\")\n",
    "\n",
    "    print(\"Generating category-based candidates...\")\n",
    "    category_candidates = []\n",
    "\n",
    "    for _, row in tqdm(user_top_categories.iterrows(), total=len(user_top_categories), desc=\"Category recommendations\"):\n",
    "        user = row['customer_id']\n",
    "        category = row['product_type_no']\n",
    "        \n",
    "        # Get popular items in this category\n",
    "        category_items = category_popular_items[\n",
    "            category_popular_items['product_type_no'] == category\n",
    "        ]['article_id'].tolist()\n",
    "        \n",
    "        # Get user's purchased items\n",
    "        user_items = set(\n",
    "            train_transactions[train_transactions['customer_id'] == user]['article_id']\n",
    "        )\n",
    "        \n",
    "        # Recommend items not yet purchased\n",
    "        for rank, item in enumerate(category_items):\n",
    "            if item not in user_items and rank < config.N_CATEGORY_CANDIDATES:\n",
    "                category_candidates.append({\n",
    "                    'customer_id': user,\n",
    "                    'article_id': item,\n",
    "                    'category_score': 1.0 / (rank + 1)  # Rank-based score\n",
    "                })\n",
    "\n",
    "    category_candidates = pd.DataFrame(category_candidates)\n",
    "    print(f\"‚úì Generated {len(category_candidates):,} category-based candidates\")\n",
    "    print(f\"  Users with candidates: {category_candidates['customer_id'].nunique():,}\")\n",
    "\n",
    "    # Save to parquet\n",
    "    print(\"\\nSaving category candidates...\")\n",
    "    category_candidates.to_parquet(config.OUTPUT_PATH / 'temp_category.parquet', index=False)\n",
    "    print(f\"‚úì Saved temp_category.parquet ({len(category_candidates):,} rows)\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del user_categories, user_top_categories, category_popular_items\n",
    "    force_garbage_collection()\n",
    "    print(\"‚úì Memory cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7a9ef084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 6: TEXT SIMILARITY RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "  STAGE 2 ENHANCEMENT: TEXT-BASED CANDIDATES\n",
      "================================================================================\n",
      "\n",
      "Creating text corpus from articles...\n",
      "  Available text columns: 12/12\n",
      "    Processed 0 articles...\n",
      "  ‚úì Created corpus for 16,616 articles\n",
      "\n",
      "Computing text embeddings...\n",
      "  Valid documents: 16,616\n",
      "  Computing TF-IDF...\n",
      "  ‚úì TF-IDF shape: (16616, 100)\n",
      "  Reducing to 20 dimensions...\n",
      "  ‚úì Embeddings shape: (16616, 20)\n",
      "  Explained variance: 0.760\n",
      "\n",
      "Computing user text preferences...\n",
      "  Building user preference vectors...\n",
      "  ‚úì Computed preferences for 47,543 users\n",
      "\n",
      "Generating text similarity candidates...\n",
      "  Processing 47,543 users...\n",
      "    Processed 10,000 users...\n",
      "    Processed 20,000 users...\n",
      "    Processed 30,000 users...\n",
      "    Processed 40,000 users...\n",
      "  ‚úì Generated 713,145 text similarity candidates\n",
      "\n",
      "‚úì Saved text similarity candidates to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/temp_text_similarity.parquet\n",
      "\n",
      "Saving embeddings for Stage 3...\n",
      "‚úì Saved embeddings\n",
      "‚úì Saved 713,145 text similarity candidates\n"
     ]
    }
   ],
   "source": [
    "print_section(\"STRATEGY 6: TEXT SIMILARITY RECOMMENDATIONS\")\n",
    "\n",
    "# Import the text feature module (save the artifact code as text_features.py)\n",
    "from text_features import integrate_text_features_stage2\n",
    "\n",
    "# Generate text-based candidates\n",
    "text_candidates, article_embeddings, user_embeddings, text_cols = integrate_text_features_stage2(\n",
    "    all_users=all_users,\n",
    "    train_transactions=train_transactions,\n",
    "    articles=articles,\n",
    "    output_path=config.OUTPUT_PATH\n",
    ")\n",
    "\n",
    "# Save for later use\n",
    "if text_candidates is not None and len(text_candidates) > 0:\n",
    "    text_candidates.to_parquet(config.OUTPUT_PATH / 'temp_text_similarity.parquet', index=False)\n",
    "    print(f\"‚úì Saved {len(text_candidates):,} text similarity candidates\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No text candidates generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b1c67e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  COMBINING ALL RECALL STRATEGIES\n",
      "================================================================================\n",
      "\n",
      "Loading candidates from parquet files...\n",
      "  Loading repurchase candidates...\n",
      "    ‚úì 325,177 candidates\n",
      "  Loading popularity candidates...\n",
      "    ‚úì 1,188,575 candidates\n",
      "  Loading co-purchase candidates...\n",
      "    ‚úì 612,807 candidates\n",
      "  Loading user-KNN candidates...\n",
      "    ‚úì 30,107 candidates\n",
      "  Loading category candidates...\n",
      "    ‚úì 1,409,632 candidates\n",
      "  Loading text similarity candidates...\n",
      "    ‚úì 713,145 candidates\n",
      "\n",
      "Merging candidates from all strategies...\n",
      "‚úì Total unique user-item pairs: 4,044,442\n",
      "\n",
      "Candidate statistics:\n",
      "  Candidates per user: 85.07\n",
      "  Avg strategies per candidate: 1.06\n",
      "\n",
      "  Candidates by number of strategies:\n",
      "    1 strategies: 3,816,416 (94.4%)\n",
      "    2 strategies: 221,301 (5.5%)\n",
      "    3 strategies: 6,479 (0.2%)\n",
      "    4 strategies: 242 (0.0%)\n",
      "    5 strategies: 4 (0.0%)\n",
      "\n",
      "Saving merged candidates...\n",
      "‚úì Saved to all_candidates_merged.parquet (4,044,442 rows)\n",
      "\n",
      "Cleaning up temporary dataframes...\n",
      "‚úì Memory cleaned\n"
     ]
    }
   ],
   "source": [
    "print_section(\"COMBINING ALL RECALL STRATEGIES\")\n",
    "\n",
    "print(\"Loading candidates from parquet files...\")\n",
    "\n",
    "# Load all candidates (existing code)\n",
    "print(\"  Loading repurchase candidates...\")\n",
    "repurchase_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_repurchase.parquet')\n",
    "print(f\"    ‚úì {len(repurchase_candidates):,} candidates\")\n",
    "\n",
    "print(\"  Loading popularity candidates...\")\n",
    "popularity_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_popularity.parquet')\n",
    "print(f\"    ‚úì {len(popularity_candidates):,} candidates\")\n",
    "\n",
    "print(\"  Loading co-purchase candidates...\")\n",
    "copurchase_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_copurchase.parquet')\n",
    "print(f\"    ‚úì {len(copurchase_candidates):,} candidates\")\n",
    "\n",
    "print(\"  Loading user-KNN candidates...\")\n",
    "userknn_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_userknn.parquet')\n",
    "print(f\"    ‚úì {len(userknn_candidates):,} candidates\")\n",
    "\n",
    "print(\"  Loading category candidates...\")\n",
    "category_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_category.parquet')\n",
    "print(f\"    ‚úì {len(category_candidates):,} candidates\")\n",
    "\n",
    "# NEW: Load text similarity candidates\n",
    "if (config.OUTPUT_PATH / 'temp_text_similarity.parquet').exists():\n",
    "    print(\"  Loading text similarity candidates...\")\n",
    "    text_candidates = pd.read_parquet(config.OUTPUT_PATH / 'temp_text_similarity.parquet')\n",
    "    print(f\"    ‚úì {len(text_candidates):,} candidates\")\n",
    "    has_text_candidates = True\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  No text similarity candidates found\")\n",
    "    has_text_candidates = False\n",
    "\n",
    "print(\"\\nMerging candidates from all strategies...\")\n",
    "\n",
    "# Start with repurchase candidates (existing code)\n",
    "all_candidates = repurchase_candidates.copy()\n",
    "\n",
    "# Merge popularity\n",
    "all_candidates = all_candidates.merge(\n",
    "    popularity_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_pop')\n",
    ")\n",
    "\n",
    "# Merge co-purchase\n",
    "all_candidates = all_candidates.merge(\n",
    "    copurchase_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_cop')\n",
    ")\n",
    "\n",
    "# Merge user-KNN\n",
    "all_candidates = all_candidates.merge(\n",
    "    userknn_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_knn')\n",
    ")\n",
    "\n",
    "# Merge category\n",
    "all_candidates = all_candidates.merge(\n",
    "    category_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer',\n",
    "    suffixes=('', '_cat')\n",
    ")\n",
    "\n",
    "# NEW: Merge text similarity\n",
    "if has_text_candidates:\n",
    "    all_candidates = all_candidates.merge(\n",
    "        text_candidates,\n",
    "        on=['customer_id', 'article_id'],\n",
    "        how='outer',\n",
    "        suffixes=('', '_text')\n",
    "    )\n",
    "\n",
    "# Fill NaN scores with 0\n",
    "score_columns = [\n",
    "    'repurchase_score', 'popularity_score', 'copurchase_score', \n",
    "    'userknn_score', 'category_score'\n",
    "]\n",
    "\n",
    "# NEW: Add text similarity score\n",
    "if has_text_candidates:\n",
    "    score_columns.append('text_similarity_score')\n",
    "\n",
    "all_candidates[score_columns] = all_candidates[score_columns].fillna(0)\n",
    "\n",
    "print(f\"‚úì Total unique user-item pairs: {len(all_candidates):,}\")\n",
    "\n",
    "# Count how many strategies recommend each item\n",
    "all_candidates['n_strategies'] = (all_candidates[score_columns] > 0).sum(axis=1)\n",
    "\n",
    "print(\"\\nCandidate statistics:\")\n",
    "print(f\"  Candidates per user: {len(all_candidates) / all_candidates['customer_id'].nunique():.2f}\")\n",
    "print(f\"  Avg strategies per candidate: {all_candidates['n_strategies'].mean():.2f}\")\n",
    "print(\"\\n  Candidates by number of strategies:\")\n",
    "for n in sorted(all_candidates['n_strategies'].unique()):\n",
    "    count = (all_candidates['n_strategies'] == n).sum()\n",
    "    pct = count / len(all_candidates) * 100\n",
    "    print(f\"    {n} strategies: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Save the merged candidates\n",
    "print(\"\\nSaving merged candidates...\")\n",
    "all_candidates.to_parquet(config.OUTPUT_PATH / 'all_candidates_merged.parquet', index=False)\n",
    "print(f\"‚úì Saved to all_candidates_merged.parquet ({len(all_candidates):,} rows)\")\n",
    "\n",
    "# Clean up to save memory\n",
    "print(\"\\nCleaning up temporary dataframes...\")\n",
    "del repurchase_candidates, popularity_candidates, copurchase_candidates\n",
    "del userknn_candidates, category_candidates\n",
    "if has_text_candidates:\n",
    "    del text_candidates\n",
    "gc.collect()\n",
    "print(\"‚úì Memory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9375a",
   "metadata": {},
   "source": [
    "### Stage 3: Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "06f8f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "312e9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    print(f\"  üíæ Memory: {mem:.2f} GB\")\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Aggressive garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3f1e40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2')\n",
    "    \n",
    "    # Processing\n",
    "    CHUNK_SIZE = 50000  # Process candidates in chunks\n",
    "    \n",
    "    # Feature engineering windows\n",
    "    RECENT_DAYS = 7  # Last week\n",
    "    MEDIUM_DAYS = 30  # Last month\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec87e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading preprocessed data...\n",
      "‚úì Loaded all_candidates_merged.parquet\n",
      "‚úì Train transactions: 412,156\n",
      "‚úì Articles: 16,616\n",
      "‚úì Customers: 49,576\n",
      "‚úì Candidates: 4,044,442\n",
      "  üíæ Memory: 1.56 GB\n",
      "‚úì Max date: 2020-09-14\n",
      "‚úì Item popularity scores: 7,115\n",
      "‚úì Available recall scores in candidates: ['repurchase_score', 'popularity_score', 'copurchase_score', 'userknn_score', 'category_score', 'text_similarity_score']\n",
      "‚úì Co-purchase scores available in candidates\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "print_section(\"LOADING DATA\")\n",
    "\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Check which candidate file exists\n",
    "if (config.DATA_PATH / 'all_candidates_merged.parquet').exists():\n",
    "    candidates = pd.read_parquet(config.DATA_PATH / 'all_candidates_merged.parquet')\n",
    "    print(f\"‚úì Loaded all_candidates_merged.parquet\")\n",
    "elif (config.DATA_PATH / 'recall_candidates.parquet').exists():\n",
    "    candidates = pd.read_parquet(config.DATA_PATH / 'recall_candidates.parquet')\n",
    "    print(f\"‚úì Loaded recall_candidates.parquet\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find candidates file (all_candidates_merged.parquet or recall_candidates.parquet)\")\n",
    "\n",
    "train_transactions = pd.read_parquet(config.DATA_PATH / 'train_transactions.parquet')\n",
    "articles = pd.read_parquet(config.DATA_PATH / 'articles.parquet')\n",
    "customers = pd.read_parquet(config.DATA_PATH / 'customers.parquet')\n",
    "\n",
    "print(f\"‚úì Train transactions: {len(train_transactions):,}\")\n",
    "print(f\"‚úì Articles: {len(articles):,}\")\n",
    "print(f\"‚úì Customers: {len(customers):,}\")\n",
    "print(f\"‚úì Candidates: {len(candidates):,}\")\n",
    "print_memory()\n",
    "\n",
    "# Get max date\n",
    "max_date = train_transactions['t_dat'].max()\n",
    "print(f\"‚úì Max date: {max_date.date()}\")\n",
    "\n",
    "# Load item popularity from recall stage\n",
    "item_popularity = pd.read_parquet(config.DATA_PATH / 'item_popularity.parquet')\n",
    "print(f\"‚úì Item popularity scores: {len(item_popularity):,}\")\n",
    "\n",
    "# Check what score columns are available in candidates\n",
    "available_scores = [col for col in candidates.columns if 'score' in col.lower()]\n",
    "print(f\"‚úì Available recall scores in candidates: {available_scores}\")\n",
    "\n",
    "# Item-to-item CF is already in the candidates as 'copurchase_score'\n",
    "# We don't need to load a separate pkl file\n",
    "has_copurchase_score = 'copurchase_score' in candidates.columns\n",
    "if has_copurchase_score:\n",
    "    print(f\"‚úì Co-purchase scores available in candidates\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Co-purchase scores not found in candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdaa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PART 1: USER FEATURES\n",
      "================================================================================\n",
      "\n",
      "‚ö° Found existing user_features.parquet, loading...\n",
      "‚úì Loaded 18 user features from disk\n",
      "  üíæ Memory: 1.42 GB\n"
     ]
    }
   ],
   "source": [
    "# PART 1: USER FEATURES (20-25 features)\n",
    "\n",
    "print_section(\"PART 1: USER FEATURES\")\n",
    "\n",
    "# Check if user features already exist\n",
    "if (config.OUTPUT_PATH / 'user_features.parquet').exists():\n",
    "    print(\"‚ö° Found existing user_features.parquet, loading...\")\n",
    "    user_stats = pd.read_parquet(config.OUTPUT_PATH / 'user_features.parquet')\n",
    "    print(f\"‚úì Loaded {len(user_stats.columns)-1} user features from disk\")\n",
    "    print_memory()\n",
    "else:\n",
    "    print(\"Computing user-level features...\")\n",
    "\n",
    "    # Basic user statistics\n",
    "    print(\"  [1/5] Basic purchase statistics...\")\n",
    "    user_stats = train_transactions.groupby('customer_id').agg({\n",
    "        'article_id': 'count',  # Total purchases\n",
    "        'price': ['mean', 'std', 'min', 'max'],  # Price statistics\n",
    "        't_dat': ['min', 'max']  # First and last purchase dates\n",
    "    }).reset_index()\n",
    "\n",
    "    user_stats.columns = ['customer_id', 'n_purchases', 'avg_price', 'std_price', \n",
    "                          'min_price', 'max_price', 'first_purchase_date', 'last_purchase_date']\n",
    "\n",
    "    # Calculate days since first/last purchase\n",
    "    user_stats['days_since_first_purchase'] = (\n",
    "        max_date - user_stats['first_purchase_date']\n",
    "    ).dt.days.astype(np.int16)\n",
    "\n",
    "    user_stats['days_since_last_purchase'] = (\n",
    "        max_date - user_stats['last_purchase_date']\n",
    "    ).dt.days.astype(np.int16)\n",
    "\n",
    "    # Purchase frequency\n",
    "    user_stats['purchase_frequency'] = (\n",
    "        user_stats['n_purchases'] / (user_stats['days_since_first_purchase'] + 1)\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Drop date columns (not needed anymore)\n",
    "    user_stats = user_stats.drop(['first_purchase_date', 'last_purchase_date'], axis=1)\n",
    "\n",
    "    print(f\"  - Created {len(user_stats.columns)-1} basic features\")\n",
    "\n",
    "    # Recent activity features\n",
    "    print(\"  [2/5] Recent activity features...\")\n",
    "    recent_cutoff = max_date - timedelta(days=config.RECENT_DAYS)\n",
    "    recent_transactions = train_transactions[train_transactions['t_dat'] >= recent_cutoff]\n",
    "\n",
    "    user_recent_stats = recent_transactions.groupby('customer_id').agg({\n",
    "        'article_id': 'count',\n",
    "        'price': 'mean'\n",
    "    }).reset_index()\n",
    "    user_recent_stats.columns = ['customer_id', 'n_purchases_last_week', 'avg_price_last_week']\n",
    "\n",
    "    # Merge with main stats\n",
    "    user_stats = user_stats.merge(user_recent_stats, on='customer_id', how='left')\n",
    "    user_stats['n_purchases_last_week'] = user_stats['n_purchases_last_week'].fillna(0).astype(np.int16)\n",
    "    user_stats['avg_price_last_week'] = user_stats['avg_price_last_week'].fillna(0).astype(np.float32)\n",
    "\n",
    "    # Is user active recently\n",
    "    user_stats['is_active_last_week'] = (user_stats['n_purchases_last_week'] > 0).astype(np.int8)\n",
    "\n",
    "    del recent_transactions, user_recent_stats\n",
    "    force_garbage_collection()\n",
    "\n",
    "    print(f\"  - Total features so far: {len(user_stats.columns)-1}\")\n",
    "\n",
    "    # Diversity features\n",
    "    print(\"  [3/5] Diversity features...\")\n",
    "    user_diversity = train_transactions.groupby('customer_id').agg({\n",
    "        'article_id': 'nunique',\n",
    "    }).reset_index()\n",
    "    user_diversity.columns = ['customer_id', 'n_unique_articles']\n",
    "\n",
    "    # Category diversity\n",
    "    user_cat_diversity = (\n",
    "        train_transactions\n",
    "        .merge(articles[['article_id', 'product_type_no']], on='article_id')\n",
    "        .groupby('customer_id')['product_type_no']\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "    )\n",
    "    user_cat_diversity.columns = ['customer_id', 'n_unique_categories']\n",
    "\n",
    "    user_stats = user_stats.merge(user_diversity, on='customer_id', how='left')\n",
    "    user_stats = user_stats.merge(user_cat_diversity, on='customer_id', how='left')\n",
    "\n",
    "    user_stats['exploration_rate'] = (\n",
    "        user_stats['n_unique_articles'] / user_stats['n_purchases']\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    del user_diversity, user_cat_diversity\n",
    "    force_garbage_collection()\n",
    "\n",
    "    print(f\"  - Total features so far: {len(user_stats.columns)-1}\")\n",
    "\n",
    "    # Customer demographic features\n",
    "    print(\"  [4/5] Demographic features...\")\n",
    "    customer_features = customers[['customer_id', 'age', 'FN', 'Active']].copy()\n",
    "\n",
    "    # Merge with user stats\n",
    "    user_stats = user_stats.merge(customer_features, on='customer_id', how='left')\n",
    "\n",
    "    # Fill missing values\n",
    "    user_stats['age'] = user_stats['age'].fillna(user_stats['age'].median()).astype(np.float32)\n",
    "    user_stats['FN'] = user_stats['FN'].fillna(0).astype(np.float32)\n",
    "    user_stats['Active'] = user_stats['Active'].fillna(0).astype(np.float32)\n",
    "\n",
    "    del customer_features\n",
    "    force_garbage_collection()\n",
    "\n",
    "    print(f\"  - Total features so far: {len(user_stats.columns)-1}\")\n",
    "\n",
    "    # Purchase trend\n",
    "    print(\"  [5/5] Purchase trend features...\")\n",
    "    # Split into two periods and compare\n",
    "    mid_date = max_date - timedelta(days=config.MEDIUM_DAYS // 2)\n",
    "    old_cutoff = max_date - timedelta(days=config.MEDIUM_DAYS)\n",
    "\n",
    "    recent_period = train_transactions[train_transactions['t_dat'] >= mid_date]\n",
    "    old_period = train_transactions[\n",
    "        (train_transactions['t_dat'] >= old_cutoff) & (train_transactions['t_dat'] < mid_date)\n",
    "    ]\n",
    "\n",
    "    user_recent_count = recent_period.groupby('customer_id').size().reset_index(name='purchases_recent_period')\n",
    "    user_old_count = old_period.groupby('customer_id').size().reset_index(name='purchases_old_period')\n",
    "\n",
    "    user_trend = user_recent_count.merge(user_old_count, on='customer_id', how='outer').fillna(0)\n",
    "    user_trend['purchase_trend'] = (\n",
    "        (user_trend['purchases_recent_period'] - user_trend['purchases_old_period']) / \n",
    "        (user_trend['purchases_old_period'] + 1)\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    user_stats = user_stats.merge(\n",
    "        user_trend[['customer_id', 'purchase_trend']], \n",
    "        on='customer_id', \n",
    "        how='left'\n",
    "    )\n",
    "    user_stats['purchase_trend'] = user_stats['purchase_trend'].fillna(0).astype(np.float32)\n",
    "\n",
    "    del recent_period, old_period, user_recent_count, user_old_count, user_trend\n",
    "    force_garbage_collection()\n",
    "\n",
    "    # Convert to optimal dtypes\n",
    "    for col in user_stats.columns:\n",
    "        if col != 'customer_id':\n",
    "            if user_stats[col].dtype == 'float64':\n",
    "                user_stats[col] = user_stats[col].astype(np.float32)\n",
    "            elif user_stats[col].dtype == 'int64':\n",
    "                user_stats[col] = user_stats[col].astype(np.int32)\n",
    "\n",
    "    print(f\"‚úì Created {len(user_stats.columns)-1} user features\")\n",
    "    print_memory()\n",
    "\n",
    "    # Save user features for reuse\n",
    "    print(\"\\nSaving user features...\")\n",
    "    user_stats.to_parquet(config.OUTPUT_PATH / 'user_features.parquet', index=False)\n",
    "    print(f\"‚úì Saved user_features.parquet ({len(user_stats):,} rows, {len(user_stats.columns)-1} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9835b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PART 2: ITEM FEATURES\n",
      "================================================================================\n",
      "\n",
      "‚ö° Found existing item_features.parquet, loading...\n",
      "‚úì Loaded 19 item features from disk\n",
      "  üíæ Memory: 1.42 GB\n",
      "  [1/4] Basic item statistics...\n",
      "  - Created 7 basic features\n",
      "  [2/4] Recent popularity features...\n",
      "  - Total features so far: 9\n",
      "  [3/4] Sales trend features...\n",
      "  - Total features so far: 10\n",
      "  [4/4] Article metadata features...\n",
      "‚úì Created 19 item features\n",
      "  üíæ Memory: 2.07 GB\n",
      "\n",
      "Saving item features...\n",
      "‚úì Saved item_features.parquet (15,932 rows, 19 features)\n"
     ]
    }
   ],
   "source": [
    "# PART 2: ITEM FEATURES (20-25 features)\n",
    "\n",
    "print_section(\"PART 2: ITEM FEATURES\")\n",
    "\n",
    "# Check if item features already exist\n",
    "if (config.OUTPUT_PATH / 'item_features.parquet').exists():\n",
    "    print(\"‚ö° Found existing item_features.parquet, loading...\")\n",
    "    item_stats = pd.read_parquet(config.OUTPUT_PATH / 'item_features.parquet')\n",
    "    print(f\"‚úì Loaded {len(item_stats.columns)-1} item features from disk\")\n",
    "    print_memory()\n",
    "else:\n",
    "    print(\"Computing item-level features...\")\n",
    "\n",
    "# Basic item statistics\n",
    "print(\"  [1/4] Basic item statistics...\")\n",
    "item_stats = train_transactions.groupby('article_id').agg({\n",
    "    'customer_id': 'nunique',  # Number of unique buyers\n",
    "    'price': ['mean', 'std'],\n",
    "    't_dat': ['min', 'max', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "item_stats.columns = ['article_id', 'n_unique_buyers', 'avg_price', 'std_price',\n",
    "                      'first_sale_date', 'last_sale_date', 'total_sales']\n",
    "\n",
    "# Days since first/last sale\n",
    "item_stats['days_since_first_sale'] = (\n",
    "    max_date - item_stats['first_sale_date']\n",
    ").dt.days.astype(np.int16)\n",
    "\n",
    "item_stats['days_since_last_sale'] = (\n",
    "    max_date - item_stats['last_sale_date']\n",
    ").dt.days.astype(np.int16)\n",
    "\n",
    "# Sales frequency\n",
    "item_stats['sales_frequency'] = (\n",
    "    item_stats['total_sales'] / (item_stats['days_since_first_sale'] + 1)\n",
    ").astype(np.float32)\n",
    "\n",
    "item_stats = item_stats.drop(['first_sale_date', 'last_sale_date'], axis=1)\n",
    "\n",
    "print(f\"  - Created {len(item_stats.columns)-1} basic features\")\n",
    "\n",
    "# Recent popularity\n",
    "print(\"  [2/4] Recent popularity features...\")\n",
    "recent_cutoff = max_date - timedelta(days=config.RECENT_DAYS)\n",
    "recent_sales = train_transactions[train_transactions['t_dat'] >= recent_cutoff]\n",
    "\n",
    "item_recent_stats = recent_sales.groupby('article_id').agg({\n",
    "    'customer_id': ['count', 'nunique']\n",
    "}).reset_index()\n",
    "item_recent_stats.columns = ['article_id', 'sales_last_week', 'buyers_last_week']\n",
    "\n",
    "item_stats = item_stats.merge(item_recent_stats, on='article_id', how='left')\n",
    "item_stats['sales_last_week'] = item_stats['sales_last_week'].fillna(0).astype(np.int16)\n",
    "item_stats['buyers_last_week'] = item_stats['buyers_last_week'].fillna(0).astype(np.int16)\n",
    "\n",
    "del recent_sales, item_recent_stats\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"  - Total features so far: {len(item_stats.columns)-1}\")\n",
    "\n",
    "# Sales trend\n",
    "print(\"  [3/4] Sales trend features...\")\n",
    "mid_date = max_date - timedelta(days=config.MEDIUM_DAYS // 2)\n",
    "old_cutoff = max_date - timedelta(days=config.MEDIUM_DAYS)\n",
    "\n",
    "recent_period = train_transactions[train_transactions['t_dat'] >= mid_date]\n",
    "old_period = train_transactions[\n",
    "    (train_transactions['t_dat'] >= old_cutoff) & (train_transactions['t_dat'] < mid_date)\n",
    "]\n",
    "\n",
    "item_recent_count = recent_period.groupby('article_id').size().reset_index(name='sales_recent_period')\n",
    "item_old_count = old_period.groupby('article_id').size().reset_index(name='sales_old_period')\n",
    "\n",
    "item_trend = item_recent_count.merge(item_old_count, on='article_id', how='outer').fillna(0)\n",
    "item_trend['sales_trend'] = (\n",
    "    (item_trend['sales_recent_period'] - item_trend['sales_old_period']) / \n",
    "    (item_trend['sales_old_period'] + 1)\n",
    ").astype(np.float32)\n",
    "\n",
    "item_stats = item_stats.merge(\n",
    "    item_trend[['article_id', 'sales_trend']], \n",
    "    on='article_id', \n",
    "    how='left'\n",
    ")\n",
    "item_stats['sales_trend'] = item_stats['sales_trend'].fillna(0).astype(np.float32)\n",
    "\n",
    "del recent_period, old_period, item_recent_count, item_old_count, item_trend\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"  - Total features so far: {len(item_stats.columns)-1}\")\n",
    "\n",
    "# Merge with article metadata\n",
    "print(\"  [4/4] Article metadata features...\")\n",
    "article_features = articles[[\n",
    "    'article_id', 'product_type_no', 'graphical_appearance_no',\n",
    "    'colour_group_code', 'perceived_colour_value_id', \n",
    "    'department_no', 'index_group_no', 'section_no', 'garment_group_no'\n",
    "]].copy()\n",
    "\n",
    "item_stats = item_stats.merge(article_features, on='article_id', how='left')\n",
    "\n",
    "del article_features\n",
    "force_garbage_collection()\n",
    "\n",
    "# Add popularity scores from recall stage\n",
    "item_stats = item_stats.merge(\n",
    "    item_popularity[['article_id', 'popularity_score']], \n",
    "    on='article_id', \n",
    "    how='left'\n",
    ")\n",
    "item_stats['popularity_score'] = item_stats['popularity_score'].fillna(0).astype(np.float32)\n",
    "\n",
    "# Convert to optimal dtypes\n",
    "for col in item_stats.columns:\n",
    "    if col != 'article_id':\n",
    "        if item_stats[col].dtype == 'float64':\n",
    "            item_stats[col] = item_stats[col].astype(np.float32)\n",
    "        elif item_stats[col].dtype == 'int64':\n",
    "            item_stats[col] = item_stats[col].astype(np.int32)\n",
    "\n",
    "print(f\"‚úì Created {len(item_stats.columns)-1} item features\")\n",
    "print_memory()\n",
    "\n",
    "# Save item features for reuse\n",
    "print(\"\\nSaving item features...\")\n",
    "item_stats.to_parquet(config.OUTPUT_PATH / 'item_features.parquet', index=False)\n",
    "print(f\"‚úì Saved item_features.parquet ({len(item_stats):,} rows, {len(item_stats.columns)-1} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PART 3: USER-ITEM INTERACTION FEATURES\n",
      "================================================================================\n",
      "\n",
      "Computing interaction features in chunks...\n",
      "  [1/3] Building user purchase history...\n",
      "  [1.5/3] Saving user purchase history...\n",
      "  ‚úì Saved user_purchase_history.parquet\n",
      "Saving additional user statistics...\n",
      "  ‚úì Saved user_top_categories.parquet\n",
      "  ‚úì Saved user_price_stats.parquet\n",
      "  [2/3] Processing candidates in chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499cd1bb40394ee9b528b2cf2dc87d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feature chunks:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [3/3] Combining feature chunks...\n",
      "‚úì Created interaction features for 4,044,442 candidates\n",
      "  üíæ Memory: 2.43 GB\n",
      "\n",
      "================================================================================\n",
      "  MERGING ALL FEATURES\n",
      "================================================================================\n",
      "\n",
      "Merging user, item, and interaction features...\n",
      "  ‚úì Merged user features\n",
      "  ‚úì Merged item features\n",
      "Filling missing values...\n",
      "\n",
      "‚úì Total features: 55 (excluding customer_id, article_id)\n",
      "‚úì Total candidate-feature pairs: 4,044,442\n",
      "  üíæ Memory: 1.84 GB\n"
     ]
    }
   ],
   "source": [
    "# PART 3: USER-ITEM INTERACTION FEATURES (CHUNKED)\n",
    "\n",
    "print_section(\"PART 3: USER-ITEM INTERACTION FEATURES\")\n",
    "\n",
    "print(\"Computing interaction features in chunks...\")\n",
    "\n",
    "# Precompute user purchase history for fast lookup\n",
    "print(\"  [1/3] Building user purchase history...\")\n",
    "user_purchases = (\n",
    "    train_transactions\n",
    "    .groupby('customer_id')['article_id']\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "user_purchase_list = (\n",
    "    train_transactions\n",
    "    .sort_values('t_dat', ascending=False)\n",
    "    .groupby('customer_id')['article_id']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Save user purchase history for reuse\n",
    "print(\"  [1.5/3] Saving user purchase history...\")\n",
    "user_purchase_df = pd.DataFrame([\n",
    "    {'customer_id': user, 'purchased_articles': list(items)}\n",
    "    for user, items in user_purchases.items()\n",
    "])\n",
    "user_purchase_df.to_parquet(config.OUTPUT_PATH / 'user_purchase_history.parquet', index=False)\n",
    "print(f\"  ‚úì Saved user_purchase_history.parquet\")\n",
    "\n",
    "del user_purchase_df\n",
    "force_garbage_collection()\n",
    "\n",
    "# User category preferences\n",
    "user_categories = (\n",
    "    train_transactions\n",
    "    .merge(articles[['article_id', 'product_type_no']], on='article_id')\n",
    "    .groupby(['customer_id', 'product_type_no'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "user_top_category = (\n",
    "    user_categories\n",
    "    .sort_values(['customer_id', 'count'], ascending=[True, False])\n",
    "    .groupby('customer_id')\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    [['customer_id', 'product_type_no']]\n",
    "    .rename(columns={'product_type_no': 'top_category'})\n",
    ")\n",
    "\n",
    "# User price preferences\n",
    "user_price_stats = train_transactions.groupby('customer_id')['price'].agg(['mean', 'std']).reset_index()\n",
    "user_price_stats.columns = ['customer_id', 'user_avg_price', 'user_std_price']\n",
    "\n",
    "# Save additional user stats\n",
    "print(\"Saving additional user statistics...\")\n",
    "user_top_category.to_parquet(config.OUTPUT_PATH / 'user_top_categories.parquet', index=False)\n",
    "user_price_stats.to_parquet(config.OUTPUT_PATH / 'user_price_stats.parquet', index=False)\n",
    "print(f\"  ‚úì Saved user_top_categories.parquet\")\n",
    "print(f\"  ‚úì Saved user_price_stats.parquet\")\n",
    "\n",
    "print(\"  [2/3] Processing candidates in chunks...\")\n",
    "\n",
    "# Split candidates into chunks\n",
    "n_chunks = max(1, len(candidates) // config.CHUNK_SIZE)\n",
    "candidate_chunks = np.array_split(candidates, n_chunks)\n",
    "\n",
    "feature_chunks = []\n",
    "\n",
    "for chunk_idx, chunk in enumerate(tqdm(candidate_chunks, desc=\"Feature chunks\")):\n",
    "    # Start with the chunk\n",
    "    chunk_features = chunk.copy()\n",
    "    \n",
    "    # Has user purchased this exact item before?\n",
    "    chunk_features['has_purchased_item'] = chunk_features.apply(\n",
    "        lambda row: 1 if row['article_id'] in user_purchases.get(row['customer_id'], set()) else 0,\n",
    "        axis=1\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    # If purchased before, get days since last purchase\n",
    "    def days_since_purchase(row):\n",
    "        user_items = user_purchase_list.get(row['customer_id'], [])\n",
    "        if row['article_id'] in user_items:\n",
    "            # Get position of first occurrence (most recent due to sort)\n",
    "            try:\n",
    "                idx = user_items.index(row['article_id'])\n",
    "                # Approximate days (assuming 1 purchase per day on average)\n",
    "                return min(idx, 365)\n",
    "            except:\n",
    "                return 365\n",
    "        return 365\n",
    "    \n",
    "    chunk_features['days_since_item_purchase'] = chunk_features.apply(\n",
    "        days_since_purchase, axis=1\n",
    "    ).astype(np.int16)\n",
    "    \n",
    "    # Merge with item stats to get item metadata\n",
    "    chunk_features = chunk_features.merge(\n",
    "        item_stats[['article_id', 'product_type_no', 'avg_price', 'popularity_score']], \n",
    "        on='article_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Has user purchased items from this category?\n",
    "    chunk_features = chunk_features.merge(user_top_category, on='customer_id', how='left')\n",
    "    chunk_features['category_match'] = (\n",
    "        chunk_features['product_type_no'] == chunk_features['top_category']\n",
    "    ).astype(np.int8)\n",
    "    chunk_features = chunk_features.drop(['product_type_no', 'top_category'], axis=1)\n",
    "    \n",
    "    # Price match features\n",
    "    chunk_features = chunk_features.merge(user_price_stats, on='customer_id', how='left')\n",
    "    chunk_features['price_vs_user_avg'] = (\n",
    "        (chunk_features['avg_price'] - chunk_features['user_avg_price']) / \n",
    "        (chunk_features['user_std_price'] + 0.01)\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    chunk_features['is_cheaper_than_usual'] = (\n",
    "        chunk_features['avg_price'] < chunk_features['user_avg_price']\n",
    "    ).astype(np.int8)\n",
    "    \n",
    "    chunk_features = chunk_features.drop(['user_avg_price', 'user_std_price', 'avg_price'], axis=1)\n",
    "    \n",
    "    # Co-purchase score is already in candidates, but create derived features\n",
    "    if 'copurchase_score' in chunk_features.columns:\n",
    "        # Normalize copurchase score by user's max copurchase score\n",
    "        user_max_copurchase = chunk_features.groupby('customer_id')['copurchase_score'].transform('max')\n",
    "        chunk_features['copurchase_score_normalized'] = (\n",
    "            chunk_features['copurchase_score'] / (user_max_copurchase + 0.001)\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Binary: has any copurchase signal\n",
    "        chunk_features['has_copurchase_signal'] = (\n",
    "            chunk_features['copurchase_score'] > 0\n",
    "        ).astype(np.int8)\n",
    "    \n",
    "    # Recall strategy coverage (how many strategies recommended this item)\n",
    "    # Already in candidates as 'n_strategies'\n",
    "    \n",
    "    # Rank features (rank within each recall strategy)\n",
    "    for score_col in ['repurchase_score', 'popularity_score', 'copurchase_score', \n",
    "                      'userknn_score', 'category_score']:\n",
    "        if score_col in chunk_features.columns:\n",
    "            chunk_features[f'{score_col}_rank'] = (\n",
    "                chunk_features.groupby('customer_id')[score_col]\n",
    "                .rank(method='dense', ascending=False)\n",
    "                .astype(np.int16)\n",
    "            )\n",
    "    \n",
    "    # Overall candidate rank (by combined score if available)\n",
    "    if 'n_strategies' in chunk_features.columns:\n",
    "        chunk_features['overall_rank'] = (\n",
    "            chunk_features.groupby('customer_id')['n_strategies']\n",
    "            .rank(method='dense', ascending=False)\n",
    "            .astype(np.int16)\n",
    "        )\n",
    "    \n",
    "    # Clean up\n",
    "    chunk_features = chunk_features.fillna(0)\n",
    "    \n",
    "    feature_chunks.append(chunk_features)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk_features\n",
    "    if chunk_idx % 10 == 0:\n",
    "        force_garbage_collection()\n",
    "\n",
    "print(\"  [3/3] Combining feature chunks...\")\n",
    "all_features = pd.concat(feature_chunks, ignore_index=True)\n",
    "del feature_chunks\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"‚úì Created interaction features for {len(all_features):,} candidates\")\n",
    "print_memory()\n",
    "\n",
    "# MERGE ALL FEATURES\n",
    "\n",
    "print_section(\"MERGING ALL FEATURES\")\n",
    "\n",
    "print(\"Merging user, item, and interaction features...\")\n",
    "\n",
    "# Merge user features\n",
    "all_features = all_features.merge(user_stats, on='customer_id', how='left')\n",
    "print(f\"  ‚úì Merged user features\")\n",
    "\n",
    "# Merge item features (already partially merged, merge remaining)\n",
    "remaining_item_cols = [col for col in item_stats.columns if col not in all_features.columns]\n",
    "remaining_item_cols.append('article_id')\n",
    "all_features = all_features.merge(item_stats[remaining_item_cols], on='article_id', how='left')\n",
    "print(f\"  ‚úì Merged item features\")\n",
    "\n",
    "# Fill any remaining NaNs (handle categorical columns separately)\n",
    "print(\"Filling missing values...\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = all_features.select_dtypes(include=['category']).columns.tolist()\n",
    "numerical_cols = all_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Fill numerical columns with 0\n",
    "if numerical_cols:\n",
    "    all_features[numerical_cols] = all_features[numerical_cols].fillna(0)\n",
    "\n",
    "# Fill categorical columns with their mode or a default value\n",
    "for col in categorical_cols:\n",
    "    if all_features[col].isna().any():\n",
    "        # Get the most frequent category\n",
    "        mode_value = all_features[col].mode()\n",
    "        if len(mode_value) > 0:\n",
    "            all_features[col] = all_features[col].fillna(mode_value[0])\n",
    "        else:\n",
    "            # If no mode, convert to string and fill with 'unknown'\n",
    "            all_features[col] = all_features[col].astype(str).fillna('unknown')\n",
    "\n",
    "print(f\"\\n‚úì Total features: {len(all_features.columns) - 2} (excluding customer_id, article_id)\")\n",
    "print(f\"‚úì Total candidate-feature pairs: {len(all_features):,}\")\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e1f090f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PART 4: TEXT-BASED SEMANTIC FEATURES\n",
      "================================================================================\n",
      "\n",
      "Found saved text embeddings, integrating text features...\n",
      "\n",
      "================================================================================\n",
      "  STAGE 3 ENHANCEMENT: TEXT-BASED FEATURES\n",
      "================================================================================\n",
      "\n",
      "Loading saved embeddings...\n",
      "  ‚úì Loaded 16,616 article embeddings\n",
      "  ‚úì Loaded 47,543 user embeddings\n",
      "\n",
      "Enhancing features with text semantics...\n",
      "\n",
      "Creating category encoding features...\n",
      "  Processing 7 categorical columns...\n",
      "  ‚úì Created 14 category encoding features\n",
      "\n",
      "Computing semantic diversity features...\n",
      "  ‚úì Computed diversity for 47,543 users\n",
      "\n",
      "  Merging category features...\n",
      "  Merging semantic diversity features...\n",
      "  Computing user-item text similarities in chunks...\n",
      "    Processing chunk 1/80...\n",
      "    Processing chunk 11/80...\n",
      "    Processing chunk 21/80...\n",
      "    Processing chunk 31/80...\n",
      "    Processing chunk 41/80...\n",
      "    Processing chunk 51/80...\n",
      "    Processing chunk 61/80...\n",
      "    Processing chunk 71/80...\n",
      "  ‚úì Text enhancement complete\n",
      "\n",
      "‚úì Enhanced features with text semantics\n",
      "  Total features now: 72\n",
      "  üíæ Memory: 3.28 GB\n"
     ]
    }
   ],
   "source": [
    "print_section(\"PART 4: TEXT-BASED SEMANTIC FEATURES\")\n",
    "\n",
    "# Import the text feature module\n",
    "from text_features import integrate_text_features_stage3\n",
    "\n",
    "# Check if embeddings exist\n",
    "embeddings_path = config.DATA_PATH  # Or wherever you ran Stage 2\n",
    "\n",
    "if (embeddings_path / 'article_embeddings.pkl').exists():\n",
    "    print(\"Found saved text embeddings, integrating text features...\")\n",
    "    \n",
    "    all_features = integrate_text_features_stage3(\n",
    "        all_features=all_features,\n",
    "        articles=articles,\n",
    "        train_transactions=train_transactions,\n",
    "        embeddings_path=embeddings_path\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Enhanced features with text semantics\")\n",
    "    print(f\"  Total features now: {len(all_features.columns) - 2}\")\n",
    "    print_memory()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Text embeddings not found, skipping text features\")\n",
    "    print(\"   Run Stage 2 with text feature integration first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1835012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  SAVING FEATURES\n",
      "================================================================================\n",
      "\n",
      "Saving feature matrix...\n",
      "  ‚úì Text semantic features detected in feature matrix\n",
      "‚úì Saved training_features.parquet (288.07 MB)\n",
      "‚úì Saved feature_names.txt (72 features)\n",
      "‚úì Saved feature_metadata.json\n",
      "\n",
      "================================================================================\n",
      "FEATURE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total features: 72\n",
      "\n",
      "Feature breakdown:\n",
      "  - User features: 23\n",
      "  - Item features: 24\n",
      "  - Interaction features: 10\n",
      "  - Text semantic features: 4 ‚úì\n",
      "\n",
      "  Text features included:\n",
      "    ‚Ä¢ text_similarity_score\n",
      "    ‚Ä¢ semantic_diversity\n",
      "    ‚Ä¢ semantic_range\n",
      "    ‚Ä¢ user_item_text_similarity\n",
      "\n",
      "  Sample user features:\n",
      "    ‚Ä¢ repurchase_score\n",
      "    ‚Ä¢ copurchase_score\n",
      "    ‚Ä¢ userknn_score\n",
      "    ‚Ä¢ has_purchased_item\n",
      "    ‚Ä¢ days_since_item_purchase\n",
      "\n",
      "  Sample item features:\n",
      "    ‚Ä¢ n_unique_articles\n",
      "    ‚Ä¢ total_sales\n",
      "    ‚Ä¢ sales_frequency\n",
      "    ‚Ä¢ sales_last_week\n",
      "    ‚Ä¢ product_type_no\n",
      "\n",
      "  Sample interaction features:\n",
      "    ‚Ä¢ popularity_score_x\n",
      "    ‚Ä¢ category_score\n",
      "    ‚Ä¢ text_similarity_score\n",
      "    ‚Ä¢ n_strategies\n",
      "    ‚Ä¢ popularity_score_y\n",
      "\n",
      "================================================================================\n",
      "SAVED FILES\n",
      "================================================================================\n",
      "\n",
      "Intermediate feature files (for reuse):\n",
      "  ‚úì user_features.parquet - User-level features\n",
      "  ‚úì item_features.parquet - Item-level features\n",
      "  ‚úì user_purchase_history.parquet - User purchase history\n",
      "  ‚úì user_top_categories.parquet - User category preferences\n",
      "  ‚úì user_price_stats.parquet - User price statistics\n",
      "\n",
      "Final output files:\n",
      "  ‚úì training_features.parquet - Complete feature matrix for training\n",
      "  ‚úì feature_names.txt - List of all feature names\n",
      "  ‚úì feature_metadata.json - Feature metadata and info\n",
      "\n",
      "================================================================================\n",
      "‚úÖ STAGE 3 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Validation checks:\n",
      "  ‚úì No NaN values: True\n",
      "  ‚úì Total rows: 4,044,442\n",
      "  ‚úì Total features: 72\n",
      "  ‚úì Memory usage: 1546.70 MB\n",
      "  ‚úì Text features integrated: YES ‚ú®\n",
      "    - 4 text-based features added\n",
      "\n",
      "Next steps:\n",
      "  1. Review training_features.parquet\n",
      "  2. Check feature_metadata.json for complete feature info\n",
      "  3. Proceed to Stage 4: Model Training\n",
      "\n",
      "üí° TIP: Delete intermediate files (user_features.parquet, etc.) to force\n",
      "   recomputation if you change feature engineering logic.\n",
      "\n",
      "üéâ SUCCESS: Text semantic features successfully integrated!\n",
      "   Your model now has access to product descriptions, colors,\n",
      "   departments, and semantic similarity features!\n"
     ]
    }
   ],
   "source": [
    "print_section(\"SAVING FEATURES\")\n",
    "\n",
    "print(\"Saving feature matrix...\")\n",
    "\n",
    "# IMPORTANT: Check if text features were added\n",
    "text_features_added = any('text_similarity' in col or 'semantic' in col \n",
    "                          for col in all_features.columns)\n",
    "\n",
    "if text_features_added:\n",
    "    print(\"  ‚úì Text semantic features detected in feature matrix\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  No text semantic features detected\")\n",
    "\n",
    "# Save the complete feature matrix\n",
    "all_features.to_parquet(config.OUTPUT_PATH / 'training_features.parquet', index=False)\n",
    "\n",
    "file_size = (config.OUTPUT_PATH / 'training_features.parquet').stat().st_size / 1024**2\n",
    "print(f\"‚úì Saved training_features.parquet ({file_size:.2f} MB)\")\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_names = [col for col in all_features.columns \n",
    "                 if col not in ['customer_id', 'article_id']]\n",
    "\n",
    "with open(config.OUTPUT_PATH / 'feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(feature_names))\n",
    "\n",
    "print(f\"‚úì Saved feature_names.txt ({len(feature_names)} features)\")\n",
    "\n",
    "# NEW: Save feature metadata including text features info\n",
    "feature_metadata = {\n",
    "    'total_features': len(feature_names),\n",
    "    'has_text_features': text_features_added,\n",
    "    'feature_list': feature_names,\n",
    "    'timestamp': str(datetime.now())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(config.OUTPUT_PATH / 'feature_metadata.json', 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "print(f\"‚úì Saved feature_metadata.json\")\n",
    "\n",
    "# Print feature summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal features: {len(feature_names)}\")\n",
    "\n",
    "# Categorize features\n",
    "user_features = [f for f in feature_names if any(x in f.lower() for x in [\n",
    "    'user', 'customer', 'purchase', 'age', 'active', 'fn', 'trend'\n",
    "])]\n",
    "\n",
    "item_features = [f for f in feature_names if any(x in f.lower() for x in [\n",
    "    'item', 'article', 'sales', 'product', 'colour', 'color', \n",
    "    'department', 'section', 'garment', 'frequency', 'count'\n",
    "]) and f not in user_features]\n",
    "\n",
    "interaction_features = [f for f in feature_names if any(x in f.lower() for x in [\n",
    "    'score', 'rank', 'strategies', 'match', 'purchased', 'category_match',\n",
    "    'price_vs', 'cheaper'\n",
    "]) and f not in user_features and f not in item_features]\n",
    "\n",
    "# NEW: Identify text features\n",
    "text_features = [f for f in feature_names if any(x in f.lower() for x in [\n",
    "    'text_similarity', 'semantic', 'embedding'\n",
    "])]\n",
    "\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - User features: {len(user_features)}\")\n",
    "print(f\"  - Item features: {len(item_features)}\")\n",
    "print(f\"  - Interaction features: {len(interaction_features)}\")\n",
    "\n",
    "# NEW: Show text features if present\n",
    "if text_features:\n",
    "    print(f\"  - Text semantic features: {len(text_features)} ‚úì\")\n",
    "    print(f\"\\n  Text features included:\")\n",
    "    for feat in text_features:\n",
    "        print(f\"    ‚Ä¢ {feat}\")\n",
    "else:\n",
    "    print(f\"  - Text semantic features: 0 (not integrated)\")\n",
    "\n",
    "# NEW: Show sample of other feature categories\n",
    "print(f\"\\n  Sample user features:\")\n",
    "for feat in user_features[:5]:\n",
    "    print(f\"    ‚Ä¢ {feat}\")\n",
    "\n",
    "print(f\"\\n  Sample item features:\")\n",
    "for feat in item_features[:5]:\n",
    "    print(f\"    ‚Ä¢ {feat}\")\n",
    "\n",
    "print(f\"\\n  Sample interaction features:\")\n",
    "for feat in interaction_features[:5]:\n",
    "    print(f\"    ‚Ä¢ {feat}\")\n",
    "\n",
    "print(\"\\n SAVED FILES \\n\")\n",
    "print(\"\\nIntermediate feature files (for reuse):\")\n",
    "print(\"user_features.parquet - User-level features\")\n",
    "print(\"item_features.parquet - Item-level features\")\n",
    "print(\"user_purchase_history.parquet - User purchase history\")\n",
    "print(\"user_top_categories.parquet - User category preferences\")\n",
    "print(\"user_price_stats.parquet - User price statistics\")\n",
    "\n",
    "# NEW: Check for text-related files\n",
    "if (config.OUTPUT_PATH / 'article_embeddings.pkl').exists():\n",
    "    print(\"article_embeddings.pkl - Article text embeddings\")\n",
    "if (config.OUTPUT_PATH / 'user_embeddings.pkl').exists():\n",
    "    print(\"user_embeddings.pkl - User preference embeddings\")\n",
    "\n",
    "print(\"\\nFinal output files:\")\n",
    "print(\"training_features.parquet - Complete feature matrix for training\")\n",
    "print(\"feature_names.txt - List of all feature names\")\n",
    "print(\"feature_metadata.json - Feature metadata and info\")\n",
    "\n",
    "print(\"\\n STAGE 3 COMPLETE! \\n\")\n",
    "\n",
    "# NEW: Validation checks\n",
    "print(\"\\nValidation checks:\")\n",
    "print(f\"No NaN values: {all_features.isnull().sum().sum() == 0}\")\n",
    "print(f\"Total rows: {len(all_features):,}\")\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"Memory usage: {all_features.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "if text_features_added:\n",
    "    print(f\"Text features integrated: YES ‚ú®\")\n",
    "    print(f\"{len(text_features)} text-based features added\")\n",
    "else:\n",
    "    print(f\"Text features not integrated\")\n",
    "    print(f\"Run Stage 2 with text integration first\")\n",
    "    print(f\"Or check if embeddings exist in {config.DATA_PATH}\")\n",
    "\n",
    "if not text_features_added:\n",
    "    print(\"Consider integrating text features for better performance\")\n",
    "    print(\"(See text_feature_integration guide)\")\n",
    "else:\n",
    "    print(\"Ready for Model Training\")\n",
    "\n",
    "if text_features_added:\n",
    "    print(\"\\n Text semantic features successfully integrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84802a24",
   "metadata": {},
   "source": [
    "### Getting Image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c359e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyTorch available\n",
      "  PyTorch version: 2.9.1\n",
      "‚úì MPS (Metal Performance Shaders) available - Apple Silicon GPU acceleration enabled!\n",
      "‚úì CLIP/FashionCLIP available\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    from torchvision import models, transforms\n",
    "    from PIL import Image\n",
    "    HAS_TORCH = True\n",
    "    print(\"‚úì PyTorch available\")\n",
    "    print(f\"  PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check for MPS (Metal Performance Shaders) support\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"‚úì MPS (Metal Performance Shaders) available - Apple Silicon GPU acceleration enabled!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  MPS not available - will use CPU\")\n",
    "        \n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    print(\"‚ö†Ô∏è  PyTorch not available. Install with: pip install torch torchvision pillow\")\n",
    "\n",
    "try:\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    HAS_CLIP = True\n",
    "    print(\"‚úì CLIP/FashionCLIP available\")\n",
    "except ImportError:\n",
    "    HAS_CLIP = False\n",
    "    print(\"‚ö†Ô∏è  CLIP/FashionCLIP not available. Install with: pip install transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY MONITORING\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    print(f\"  üíæ Memory: {mem:.2f} GB\")\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Aggressive garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()  # Clear MPS cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "class Config:\n",
    "    # Paths - UPDATE THESE FOR YOUR LOCAL SETUP\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')  # Where your parquet files are\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')  # Where to save outputs\n",
    "    IMAGE_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/h-and-m-personalized-fashion-recommendations/images')  # H&M image directory\n",
    "    \n",
    "    # Image embedding options\n",
    "    EMBEDDING_METHOD = 'fashion-clip'  # Options: 'fashion-clip', 'resnet50', 'clip', 'efficientnet'\n",
    "    EMBEDDING_DIM = 512  # Output dimension (will be projected from original)\n",
    "    \n",
    "    # Processing - Optimized for Apple Silicon\n",
    "    BATCH_SIZE = 64  # M4 can handle larger batches efficiently\n",
    "    IMAGE_SIZE = 224  # Input size for models\n",
    "    \n",
    "    # Memory optimization\n",
    "    PROCESS_SUBSET = False  # Set True to process only subset (for testing)\n",
    "    SUBSET_SIZE = 10000  # Number of images to process if PROCESS_SUBSET=True\n",
    "    USE_FP16 = False  # MPS doesn't fully support FP16 yet, keep False\n",
    "    \n",
    "    # Apple Silicon specific\n",
    "    USE_MPS = True  # Enable MPS acceleration\n",
    "    NUM_WORKERS = 4  # For data loading (M4 has excellent multi-core)\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "config.OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device for Apple Silicon\"\"\"\n",
    "    if config.USE_MPS and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE EMBEDDING EXTRACTION\n",
    "\n",
    "class ImageEmbeddingExtractor:\n",
    "    \"\"\"Extract embeddings from product images using pre-trained models\"\"\"\n",
    "    \n",
    "    def __init__(self, method='fashion-clip', device=None):\n",
    "        self.method = method\n",
    "        self.device = device if device else get_device()\n",
    "        self.model = None\n",
    "        self.transform = None\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        if method == 'fashion-clip':\n",
    "            self._load_fashion_clip()\n",
    "        elif method == 'resnet50':\n",
    "            self._load_resnet()\n",
    "        elif method == 'clip':\n",
    "            self._load_clip()\n",
    "        elif method == 'efficientnet':\n",
    "            self._load_efficientnet()\n",
    "    \n",
    "    def _load_fashion_clip(self):\n",
    "        \"\"\"Load pre-trained FashionCLIP - optimized for fashion domain\"\"\"\n",
    "        print(\"Loading FashionCLIP\")\n",
    "        \n",
    "        # Load FashionCLIP 2.0 - uses better base model\n",
    "        self.model = CLIPModel.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.output_dim = 512  # FashionCLIP outputs 512-dim embeddings\n",
    "        print(f\"‚úì FashionCLIP 2.0 loaded on {self.device}, output dim: {self.output_dim}\")\n",
    "        print(\"  Model details: ViT-B/32 architecture, trained on Farfetch dataset\")\n",
    "        print(\"  Benefits: Better fashion understanding, semantic similarity, zero-shot capabilities\")\n",
    "    \n",
    "    def _load_resnet(self):\n",
    "        \"\"\"Load pre-trained ResNet50 - optimized for Apple Silicon\"\"\"\n",
    "        print(\"Loading ResNet50 (pre-trained on ImageNet)...\")\n",
    "        \n",
    "        # Load model with updated weights parameter (new PyTorch API)\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove final classification layer\n",
    "        self.model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Move to MPS device\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Image preprocessing - using updated normalize values from weights\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(config.IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.output_dim = 2048\n",
    "        print(f\"‚úì ResNet50 loaded on {self.device}, output dim: {self.output_dim}\")\n",
    "    \n",
    "    def _load_efficientnet(self):\n",
    "        \"\"\"Load pre-trained EfficientNet-B0 - optimized for Apple Silicon\"\"\"\n",
    "        print(\"Loading EfficientNet-B0 (pre-trained on ImageNet)...\")\n",
    "        \n",
    "        # Load with updated weights parameter\n",
    "        model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove final classification layer\n",
    "        self.model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(config.IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.output_dim = 1280\n",
    "        print(f\"‚úì EfficientNet-B0 loaded on {self.device}, output dim: {self.output_dim}\")\n",
    "    \n",
    "    def _load_clip(self):\n",
    "        \"\"\"Load CLIP model - optimized for Apple Silicon\"\"\"\n",
    "        print(\"Loading CLIP (vision-language model)...\")\n",
    "        \n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        self.output_dim = 512\n",
    "        print(f\"‚úì CLIP loaded on {self.device}, output dim: {self.output_dim}\")\n",
    "    \n",
    "    def extract_single(self, image_path):\n",
    "        \"\"\"Extract embedding from a single image\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            if self.method in ['clip', 'fashion-clip']:\n",
    "                inputs = self.processor(images=img, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    image_features = self.model.get_image_features(**inputs)\n",
    "                \n",
    "                embedding = image_features.squeeze().cpu().numpy()\n",
    "            else:\n",
    "                img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    embedding = self.model(img_tensor).squeeze().cpu().numpy()\n",
    "            \n",
    "            return embedding\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Return zero vector if image processing fails\n",
    "            print(f\"‚ö†Ô∏è  Error processing {image_path}: {e}\")\n",
    "            return np.zeros(self.output_dim, dtype=np.float32)\n",
    "    \n",
    "    def extract_batch(self, image_paths):\n",
    "        \"\"\"Extract embeddings from a batch of images - optimized for MPS\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        # For CLIP-based models (fashion-clip and clip), process individually\n",
    "        # because batch processing with processor is more complex\n",
    "        if self.method in ['clip', 'fashion-clip']:\n",
    "            for img_path in image_paths:\n",
    "                emb = self.extract_single(img_path)\n",
    "                embeddings.append(emb)\n",
    "            return np.array(embeddings)\n",
    "        \n",
    "        # For CNN models (ResNet, EfficientNet), process as batch\n",
    "        batch_tensors = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        # Load and preprocess all images in batch\n",
    "        for idx, img_path in enumerate(image_paths):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = self.transform(img)\n",
    "                batch_tensors.append(img_tensor)\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error loading {img_path}: {e}\")\n",
    "                embeddings.append(np.zeros(self.output_dim, dtype=np.float32))\n",
    "        \n",
    "        # Process batch\n",
    "        if batch_tensors:\n",
    "            batch = torch.stack(batch_tensors).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_embeddings = self.model(batch).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Handle single vs multiple embeddings\n",
    "            if len(batch_tensors) == 1:\n",
    "                batch_embeddings = batch_embeddings.reshape(1, -1)\n",
    "            \n",
    "            # Insert embeddings at correct positions\n",
    "            result_embeddings = []\n",
    "            batch_idx = 0\n",
    "            for idx in range(len(image_paths)):\n",
    "                if idx in valid_indices:\n",
    "                    result_embeddings.append(batch_embeddings[batch_idx])\n",
    "                    batch_idx += 1\n",
    "                else:\n",
    "                    result_embeddings.append(np.zeros(self.output_dim, dtype=np.float32))\n",
    "            \n",
    "            return np.array(result_embeddings)\n",
    "        \n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48909672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STAGE 1.5: IMAGE EMBEDDING EXTRACTION (APPLE SILICON OPTIMIZED)\n",
      "================================================================================\n",
      "\n",
      "Image embeddings not found. Starting extraction...\n",
      "\n",
      "Loading articles metadata...\n",
      "‚úì Loaded 16,616 articles\n",
      "\n",
      "Checking image directory: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/h-and-m-personalized-fashion-recommendations/images\n",
      "Scanning for available images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4f96e6851a44508fe1bc1d5f336776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking images:   0%|          | 0/16616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Found 16,593 images\n",
      "  Missing 23 images (0.1%)\n",
      "\n",
      "Initializing fashion-clip model...\n",
      "Using device: mps\n",
      "Loading FashionCLIP (fine-tuned on 800K+ fashion products)...\n",
      "  This model is specifically trained for fashion and will give MUCH better embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d85895de-c9e4-4046-bd79-6ab3147b5848)')' thrown while requesting HEAD https://huggingface.co/patrickjohncyh/fashion-clip/resolve/main/processor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì FashionCLIP 2.0 loaded on mps, output dim: 512\n",
      "  Model details: ViT-B/32 architecture, trained on Farfetch dataset\n",
      "  Benefits: Better fashion understanding, semantic similarity, zero-shot capabilities\n",
      "  üíæ Memory: 0.49 GB\n",
      "\n",
      "Extracting embeddings for 16,593 images...\n",
      "  Batch size: 64\n",
      "  Device: mps\n",
      "  Estimated time: 25.9 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5874f5c01445f5be1cafcb555e54c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting embeddings:   0%|          | 0/16593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Extracted 16,593 embeddings\n",
      "  üíæ Memory: 1.29 GB\n",
      "\n",
      "Creating embeddings DataFrame...\n",
      "\n",
      "Saving image embeddings...\n",
      "‚úì Saved image_embeddings.parquet (32.66 MB)\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXTRACTION PIPELINE\n",
    "\n",
    "print_section(\"STAGE 1.5: IMAGE EMBEDDING EXTRACTION (APPLE SILICON OPTIMIZED)\")\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if (config.OUTPUT_PATH / 'image_embeddings_3.parquet').exists():\n",
    "    print(\"‚ö° Found existing image_embeddings_3.parquet!\")\n",
    "    print(\"   Delete this file if you want to re-extract embeddings.\")\n",
    "    \n",
    "    # Load existing embeddings\n",
    "    image_embeddings_df = pd.read_parquet(config.OUTPUT_PATH / 'image_embeddings_3.parquet')\n",
    "    print(f\"‚úì Loaded {len(image_embeddings_df):,} image embeddings from disk\")\n",
    "    \n",
    "else:\n",
    "    print(\"Image embeddings not found. Starting extraction...\")\n",
    "    \n",
    "    # Check prerequisites\n",
    "    if not HAS_TORCH:\n",
    "        raise ImportError(\"PyTorch is required. Install with: pip install torch torchvision pillow\")\n",
    "    \n",
    "    # Load articles\n",
    "    print(\"\\nLoading articles metadata...\")\n",
    "    articles = pd.read_parquet(config.DATA_PATH / 'articles.parquet')\n",
    "    print(f\"‚úì Loaded {len(articles):,} articles\")\n",
    "    \n",
    "    # Determine which articles to process\n",
    "    if config.PROCESS_SUBSET:\n",
    "        articles = articles.head(config.SUBSET_SIZE)\n",
    "        print(f\"‚ö†Ô∏è  Processing subset of {len(articles):,} articles (PROCESS_SUBSET=True)\")\n",
    "    \n",
    "    article_ids = articles['article_id'].tolist()\n",
    "    \n",
    "    # Check image directory structure\n",
    "    print(f\"\\nChecking image directory: {config.IMAGE_PATH}\")\n",
    "    if not config.IMAGE_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Image directory not found: {config.IMAGE_PATH}\")\n",
    "    \n",
    "    # H&M images are organized as: images/0XX/0XXXXXXXX.jpg\n",
    "    # Where first 3 digits determine subfolder\n",
    "    \n",
    "    # Find available images\n",
    "    print(\"Scanning for available images...\")\n",
    "    available_images = {}\n",
    "    missing_count = 0\n",
    "    \n",
    "    for article_id in tqdm(article_ids, desc=\"Checking images\"):\n",
    "        # Try different possible paths\n",
    "        article_str = str(article_id).zfill(10)  # Pad to 10 digits\n",
    "        subfolder = article_str[:3]\n",
    "        \n",
    "        # Possible image paths\n",
    "        possible_paths = [\n",
    "            config.IMAGE_PATH / subfolder / f\"{article_str}.jpg\",\n",
    "            config.IMAGE_PATH / f\"{article_str}.jpg\",\n",
    "            config.IMAGE_PATH / f\"{article_id}.jpg\",\n",
    "        ]\n",
    "        \n",
    "        image_found = False\n",
    "        for img_path in possible_paths:\n",
    "            if img_path.exists():\n",
    "                available_images[article_id] = img_path\n",
    "                image_found = True\n",
    "                break\n",
    "        \n",
    "        if not image_found:\n",
    "            missing_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úì Found {len(available_images):,} images\")\n",
    "    print(f\"  Missing {missing_count:,} images ({100*missing_count/len(article_ids):.1f}%)\")\n",
    "    \n",
    "    # Initialize extractor\n",
    "    print(f\"\\nInitializing {config.EMBEDDING_METHOD} model...\")\n",
    "    extractor = ImageEmbeddingExtractor(method=config.EMBEDDING_METHOD)\n",
    "    print_memory()\n",
    "    \n",
    "    # Extract embeddings\n",
    "    print(f\"\\nExtracting embeddings for {len(available_images):,} images...\")\n",
    "    print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"  Device: {extractor.device}\")\n",
    "    estimated_time = len(available_images) / (config.BATCH_SIZE * 10)  # ~10 batches/sec on M4\n",
    "    print(f\"  Estimated time: {estimated_time:.1f} minutes\")\n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    batch_article_ids = []\n",
    "    batch_image_paths = []\n",
    "    \n",
    "    for article_id, img_path in tqdm(available_images.items(), desc=\"Extracting embeddings\"):\n",
    "        batch_article_ids.append(article_id)\n",
    "        batch_image_paths.append(img_path)\n",
    "        \n",
    "        # Process batch\n",
    "        if len(batch_article_ids) >= config.BATCH_SIZE:\n",
    "            batch_embeddings = extractor.extract_batch(batch_image_paths)\n",
    "            \n",
    "            for aid, emb in zip(batch_article_ids, batch_embeddings):\n",
    "                embeddings_dict[aid] = emb\n",
    "            \n",
    "            # Clear batch\n",
    "            batch_article_ids = []\n",
    "            batch_image_paths = []\n",
    "            \n",
    "            # Periodic garbage collection\n",
    "            if len(embeddings_dict) % (config.BATCH_SIZE * 10) == 0:\n",
    "                force_garbage_collection()\n",
    "    \n",
    "    # Process remaining batch\n",
    "    if batch_article_ids:\n",
    "        batch_embeddings = extractor.extract_batch(batch_image_paths)\n",
    "        for aid, emb in zip(batch_article_ids, batch_embeddings):\n",
    "            embeddings_dict[aid] = emb\n",
    "    \n",
    "    print(f\"\\n‚úì Extracted {len(embeddings_dict):,} embeddings\")\n",
    "    print_memory()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    print(\"\\nCreating embeddings DataFrame...\")\n",
    "    \n",
    "    # For missing images, use mean embedding or zero vector\n",
    "    mean_embedding = np.mean(list(embeddings_dict.values()), axis=0) if embeddings_dict else np.zeros(extractor.output_dim)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    for article_id in article_ids:\n",
    "        if article_id in embeddings_dict:\n",
    "            emb = embeddings_dict[article_id]\n",
    "        else:\n",
    "            emb = mean_embedding  # Use mean for missing images\n",
    "        \n",
    "        all_embeddings.append(emb)\n",
    "    \n",
    "    # Create DataFrame with article_id and embedding columns\n",
    "    image_embeddings_df = pd.DataFrame({\n",
    "        'article_id': article_ids\n",
    "    })\n",
    "    \n",
    "    # Add embedding dimensions as separate columns\n",
    "    embedding_matrix = np.array(all_embeddings)\n",
    "    \n",
    "    # Project to target dimension if needed\n",
    "    if embedding_matrix.shape[1] != config.EMBEDDING_DIM:\n",
    "        print(f\"\\nProjecting embeddings from {embedding_matrix.shape[1]} to {config.EMBEDDING_DIM} dimensions...\")\n",
    "        \n",
    "        pca = PCA(n_components=config.EMBEDDING_DIM, random_state=config.RANDOM_STATE)\n",
    "        embedding_matrix = pca.fit_transform(embedding_matrix)\n",
    "        \n",
    "        print(f\"  Explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    # Add embedding columns\n",
    "    for i in range(embedding_matrix.shape[1]):\n",
    "        image_embeddings_df[f'image_emb_{i}'] = embedding_matrix[:, i].astype(np.float32)\n",
    "    \n",
    "    # Save embeddings\n",
    "    print(\"\\nSaving image embeddings...\")\n",
    "    image_embeddings_df.to_parquet(config.OUTPUT_PATH / 'image_embeddings_3.parquet', index=False)\n",
    "    \n",
    "    file_size = (config.OUTPUT_PATH / 'image_embeddings_3.parquet').stat().st_size / 1024**2\n",
    "    print(f\"‚úì Saved image_embeddings.parquet ({file_size:.2f} MB)\")\n",
    "    \n",
    "    # Clean up\n",
    "    del extractor, embeddings_dict, embedding_matrix\n",
    "    force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb82906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  INTEGRATING IMAGE EMBEDDINGS INTO TRAINING FEATURES\n",
      "================================================================================\n",
      "\n",
      "Loading training_features.parquet...\n",
      "‚úì Loaded training features: 4,044,442 rows\n",
      "  Current features: 72\n",
      "  üíæ Memory: 2.13 GB\n",
      "\n",
      "Merging image embeddings with training features...\n",
      "  Image embeddings: 16,616 articles\n",
      "‚úì Merged successfully\n",
      "  New features: 584\n",
      "\n",
      "Optimizing data types...\n",
      "  üíæ Memory: 0.60 GB\n",
      "\n",
      "Saving updated training_features.parquet...\n",
      "‚úì Saved training_features.parquet (5126.99 MB)\n",
      "‚úì Updated feature_names.txt (584 features)\n",
      "\n",
      "================================================================================\n",
      "  IMAGE EMBEDDING INTEGRATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "  üì∏ Image embeddings extracted: 16,616\n",
      "  üìä Image embedding dimensions: 512\n",
      "  üéØ Total features: 584\n",
      "     - User features: 531\n",
      "     - Item features: 12\n",
      "     - Interaction features: 16\n",
      "     - Image features: 512\n",
      "\n",
      "Files created/updated:\n",
      "  ‚úì image_embeddings.parquet - Image embeddings for all articles\n",
      "  ‚úì training_features.parquet - Updated with image embeddings\n",
      "  ‚úì feature_names.txt - Updated feature list\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Ready for Stage 4: Model Training with Image Features!\n",
      "================================================================================\n",
      "\n",
      "üí° Apple Silicon Optimization Tips:\n",
      "  - MPS acceleration enabled for GPU-like performance on M4\n",
      "  - Batch size optimized for Apple Silicon (64)\n",
      "  - Memory management tuned for unified memory architecture\n",
      "  - Image embeddings are cached and won't be re-extracted on next run\n",
      "  - To re-extract, delete image_embeddings.parquet\n",
      "  - Expected speed: ~10-15x faster than CPU on M4\n",
      "\n",
      "üéØ FashionCLIP Advantages for H&M Competition:\n",
      "  ‚úì Trained on 800K+ fashion products (not general images)\n",
      "  ‚úì Understands fashion-specific concepts (patterns, styles, materials)\n",
      "  ‚úì Better semantic similarity for similar items\n",
      "  ‚úì Can handle product attributes H&M uses (stripes, color-blocked, etc.)\n",
      "  ‚úì Optimized for standard product images (like H&M's dataset)\n",
      "  ‚úì Can be used for zero-shot classification of fashion categories\n"
     ]
    }
   ],
   "source": [
    "# INTEGRATE INTO TRAINING FEATURES\n",
    "\n",
    "print_section(\"INTEGRATING IMAGE EMBEDDINGS INTO TRAINING FEATURES\")\n",
    "\n",
    "# Check for different possible training feature files\n",
    "training_file = None\n",
    "possible_files = [\n",
    "    'training_features.parquet'\n",
    "]\n",
    "\n",
    "for filename in possible_files:\n",
    "    if (Path(\"/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2/\" + filename)).exists():\n",
    "        training_file = filename\n",
    "        break\n",
    "try:\n",
    "    image_embeddings_df = pd.read_parquet(config.OUTPUT_PATH / 'image_embeddings_3.parquet')\n",
    "except Exception as e:\n",
    "    print(f\"unable to load image embeddings\")\n",
    "\n",
    "if training_file is None:\n",
    "    print(\"No training features file found!\")\n",
    "    print(f\"\\n Image embeddings saved to: {config.OUTPUT_PATH / 'image_embeddings.parquet'}\")\n",
    "else:\n",
    "    # Load training features\n",
    "    print(f\"Loading {training_file}...\")\n",
    "    training_features = pd.read_parquet(\"/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2/\" + training_file)\n",
    "    print(f\"‚úì Loaded training features: {len(training_features):,} rows\")\n",
    "    print(f\"  Current features: {len(training_features.columns) - 2}\")  # Exclude customer_id, article_id\n",
    "    print_memory()\n",
    "\n",
    "    # Merge image embeddings\n",
    "    print(\"\\nMerging image embeddings with training features...\")\n",
    "    print(f\"  Image embeddings: {len(image_embeddings_df):,} articles\")\n",
    "\n",
    "    # Merge on article_id\n",
    "    training_features = training_features.merge(\n",
    "        image_embeddings_df,\n",
    "        on='article_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    print(f\"Merged successfully\")\n",
    "    print(f\"New features: {len(training_features.columns) - 2}\")\n",
    "\n",
    "    # Check for missing embeddings\n",
    "    image_cols = [col for col in training_features.columns if col.startswith('image_emb_')]\n",
    "    missing_embeddings = training_features[image_cols].isna().any(axis=1).sum()\n",
    "\n",
    "    if missing_embeddings > 0:\n",
    "        print(f\"\\n Found {missing_embeddings:,} rows with missing image embeddings\")\n",
    "        print(\"Filling with mean values...\")\n",
    "        \n",
    "        # Fill with mean\n",
    "        for col in image_cols:\n",
    "            mean_val = training_features[col].mean()\n",
    "            training_features[col] = training_features[col].fillna(mean_val)\n",
    "        \n",
    "        print(\"‚úì Filled missing values\")\n",
    "\n",
    "    # Convert to float32 to save memory\n",
    "    print(\"\\nOptimizing data types...\")\n",
    "    for col in image_cols:\n",
    "        training_features[col] = training_features[col].astype(np.float32)\n",
    "\n",
    "    print_memory()\n",
    "\n",
    "    # Save updated training features\n",
    "    print(f\"\\nSaving updated {training_file}...\")\n",
    "    training_features.to_parquet(\n",
    "        config.OUTPUT_PATH / training_file,\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    file_size = (config.OUTPUT_PATH / training_file).stat().st_size / 1024**2\n",
    "    print(f\"‚úì Saved {training_file} ({file_size:.2f} MB)\")\n",
    "\n",
    "    # Update feature names\n",
    "    feature_names = [col for col in training_features.columns if col not in ['customer_id', 'article_id']]\n",
    "\n",
    "    with open(config.OUTPUT_PATH / 'feature_names.txt', 'w') as f:\n",
    "        f.write('\\n'.join(feature_names))\n",
    "\n",
    "    print(f\"‚úì Updated feature_names.txt ({len(feature_names)} features)\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # SUMMARY\n",
    "    # ============================================================================\n",
    "\n",
    "    print_section(\"IMAGE EMBEDDING INTEGRATION COMPLETE!\")\n",
    "\n",
    "    print(\"Summary:\")\n",
    "    print(f\"Image embeddings extracted: {len(image_embeddings_df):,}\")\n",
    "    print(f\"Image embedding dimensions: {config.EMBEDDING_DIM}\")\n",
    "    print(f\"Total features: {len(feature_names)}\")\n",
    "    print(f\"User features: {len([f for f in feature_names if any(x in f for x in ['user', 'purchase', 'age'])])}\")\n",
    "    print(f\"Item features: {len([f for f in feature_names if any(x in f for x in ['sales', 'product', 'department'])])}\")\n",
    "    print(f\"Interaction features: {len([f for f in feature_names if any(x in f for x in ['match', 'rank', 'score', 'similarity'])])}\")\n",
    "    print(f\"Image features: {len(image_cols)}\")\n",
    "\n",
    "    print(\"\\nFiles created/updated:\")\n",
    "    print(f\"image_embeddings.parquet - Image embeddings for all articles\")\n",
    "    print(f\"{training_file} - Updated with image embeddings\")\n",
    "    print(f\"feature_names.txt - Updated feature list\")\n",
    "\n",
    "print(\" Ready for Stage 4: Model Training with Image Features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de80288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2/training_features.parquet...\n",
      "\n",
      "Shape: (4044442, 74)\n",
      "\n",
      "Columns: ['customer_id', 'article_id', 'repurchase_score', 'popularity_score_x', 'copurchase_score', 'userknn_score', 'category_score', 'text_similarity_score', 'n_strategies', 'has_purchased_item', 'days_since_item_purchase', 'popularity_score_y', 'category_match', 'price_vs_user_avg', 'is_cheaper_than_usual', 'copurchase_score_normalized', 'has_copurchase_signal', 'repurchase_score_rank', 'copurchase_score_rank', 'userknn_score_rank', 'category_score_rank', 'overall_rank', 'n_purchases', 'avg_price', 'std_price', 'min_price', 'max_price', 'days_since_first_purchase', 'days_since_last_purchase', 'purchase_frequency', 'n_purchases_last_week', 'avg_price_last_week', 'is_active_last_week', 'n_unique_articles', 'n_unique_categories', 'exploration_rate', 'age', 'FN', 'Active', 'purchase_trend', 'n_unique_buyers', 'total_sales', 'days_since_first_sale', 'days_since_last_sale', 'sales_frequency', 'sales_last_week', 'buyers_last_week', 'sales_trend', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id', 'department_no', 'index_group_no', 'section_no', 'garment_group_no', 'popularity_score', 'product_type_name_frequency', 'product_type_name_count', 'product_group_name_frequency', 'product_group_name_count', 'colour_group_name_frequency', 'colour_group_name_count', 'department_name_frequency', 'department_name_count', 'index_group_name_frequency', 'index_group_name_count', 'section_name_frequency', 'section_name_count', 'garment_group_name_frequency', 'garment_group_name_count', 'semantic_diversity', 'semantic_range', 'user_item_text_similarity']\n",
      "\n",
      "First 5 rows:\n",
      "100    0.0\n",
      "101    0.0\n",
      "102    0.0\n",
      "103    0.0\n",
      "104    0.0\n",
      "Name: userknn_score, dtype: float64\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4044442 entries, 0 to 4044441\n",
      "Data columns (total 74 columns):\n",
      " #   Column                        Dtype   \n",
      "---  ------                        -----   \n",
      " 0   customer_id                   object  \n",
      " 1   article_id                    int64   \n",
      " 2   repurchase_score              float32 \n",
      " 3   popularity_score_x            float32 \n",
      " 4   copurchase_score              float64 \n",
      " 5   userknn_score                 float64 \n",
      " 6   category_score                float64 \n",
      " 7   text_similarity_score         float32 \n",
      " 8   n_strategies                  int64   \n",
      " 9   has_purchased_item            int8    \n",
      " 10  days_since_item_purchase      int16   \n",
      " 11  popularity_score_y            float32 \n",
      " 12  category_match                int8    \n",
      " 13  price_vs_user_avg             float32 \n",
      " 14  is_cheaper_than_usual         int8    \n",
      " 15  copurchase_score_normalized   float32 \n",
      " 16  has_copurchase_signal         int8    \n",
      " 17  repurchase_score_rank         int16   \n",
      " 18  copurchase_score_rank         int16   \n",
      " 19  userknn_score_rank            int16   \n",
      " 20  category_score_rank           int16   \n",
      " 21  overall_rank                  int16   \n",
      " 22  n_purchases                   int32   \n",
      " 23  avg_price                     float32 \n",
      " 24  std_price                     float32 \n",
      " 25  min_price                     float32 \n",
      " 26  max_price                     float32 \n",
      " 27  days_since_first_purchase     int16   \n",
      " 28  days_since_last_purchase      int16   \n",
      " 29  purchase_frequency            float32 \n",
      " 30  n_purchases_last_week         int16   \n",
      " 31  avg_price_last_week           float32 \n",
      " 32  is_active_last_week           int8    \n",
      " 33  n_unique_articles             int32   \n",
      " 34  n_unique_categories           int32   \n",
      " 35  exploration_rate              float32 \n",
      " 36  age                           float32 \n",
      " 37  FN                            float32 \n",
      " 38  Active                        float32 \n",
      " 39  purchase_trend                float32 \n",
      " 40  n_unique_buyers               float64 \n",
      " 41  total_sales                   float64 \n",
      " 42  days_since_first_sale         float64 \n",
      " 43  days_since_last_sale          float64 \n",
      " 44  sales_frequency               float32 \n",
      " 45  sales_last_week               float64 \n",
      " 46  buyers_last_week              float64 \n",
      " 47  sales_trend                   float32 \n",
      " 48  product_type_no               category\n",
      " 49  graphical_appearance_no       category\n",
      " 50  colour_group_code             category\n",
      " 51  perceived_colour_value_id     category\n",
      " 52  department_no                 category\n",
      " 53  index_group_no                category\n",
      " 54  section_no                    category\n",
      " 55  garment_group_no              category\n",
      " 56  popularity_score              float32 \n",
      " 57  product_type_name_frequency   float32 \n",
      " 58  product_type_name_count       int32   \n",
      " 59  product_group_name_frequency  float32 \n",
      " 60  product_group_name_count      int32   \n",
      " 61  colour_group_name_frequency   float32 \n",
      " 62  colour_group_name_count       int32   \n",
      " 63  department_name_frequency     float32 \n",
      " 64  department_name_count         int32   \n",
      " 65  index_group_name_frequency    float32 \n",
      " 66  index_group_name_count        int32   \n",
      " 67  section_name_frequency        float32 \n",
      " 68  section_name_count            int32   \n",
      " 69  garment_group_name_frequency  float32 \n",
      " 70  garment_group_name_count      int32   \n",
      " 71  semantic_diversity            float32 \n",
      " 72  semantic_range                float32 \n",
      " 73  user_item_text_similarity     float32 \n",
      "dtypes: category(8), float32(30), float64(9), int16(9), int32(10), int64(2), int8(5), object(1)\n",
      "memory usage: 1.1+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your file\n",
    "file_path = '/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_features_2/training_features.parquet'\n",
    "\n",
    "try:\n",
    "    # Load the parquet file\n",
    "    print(f\"Loading {file_path}...\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # 1. Show dimensions (rows, columns)\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "\n",
    "    # 2. Show column names\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "    # 3. Show the first 5 rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df[100:105][\"userknn_score\"])\n",
    "\n",
    "    # 4. (Optional) Show data types and memory usage\n",
    "    print(\"\\nInfo:\")\n",
    "    print(df.info())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d7dca",
   "metadata": {},
   "source": [
    "### Stage 4: Ensemble Training with Advanced Collaborative Filtering\n",
    "\n",
    "This stage implements a comprehensive ensemble approach combining advanced collaborative filtering methods with existing models to maximize MAP@12 performance.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Advanced CF Methods:**\n",
    "   - **SVD++**: Improved SVD with implicit feedback handling\n",
    "   - **ALS (Alternating Least Squares)**: Optimized for sparse matrices\n",
    "   - **NMF (Non-negative Matrix Factorization)**: Additive parts-based representation\n",
    "   - **Enhanced User/Item CF**: Improved similarity metrics\n",
    "\n",
    "2. **Ensemble Strategies:**\n",
    "   - **Weighted Average**: Optimized weights on validation set\n",
    "   - **Stacked Ensemble**: CF predictions as features for LightGBM/Neural Tower\n",
    "   - **Rank-Based Ensemble**: Borda count and reciprocal rank fusion\n",
    "   - **Two-Stage Ensemble**: CF for candidate generation, advanced models for reranking\n",
    "\n",
    "3. **Integration:**\n",
    "   - Combines CF methods with LightGBM and Neural Tower models\n",
    "   - Optimizes ensemble weights using validation MAP@12\n",
    "   - Provides comprehensive evaluation and comparison\n",
    "\n",
    "**Expected Improvement:**\n",
    "- Baseline CF: ~0.78 MAP@12\n",
    "- With Advanced CF: ~0.79-0.80 MAP@12\n",
    "- With Optimized Ensemble: ~0.82-0.85 MAP@12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb78c864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  NumPy 2.x detected. scikit-surprise requires NumPy < 2.0\n",
      "   To fix: pip install 'numpy<2' scikit-surprise\n",
      "   Will use enhanced SVD instead of SVD++\n",
      "‚úì Ensemble Configuration loaded\n",
      "  Model path: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models\n",
      "  Data path: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2\n",
      "  Surprise available: False\n",
      "  Implicit available: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND CONFIGURATION FOR ENSEMBLE TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from scipy.optimize import minimize\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import advanced CF libraries\n",
    "# Note: surprise (scikit-surprise) requires NumPy < 2.0\n",
    "# If you get NumPy compatibility errors, run: pip install \"numpy<2\" scikit-surprise\n",
    "HAS_SURPRISE = False\n",
    "try:\n",
    "    # Check NumPy version first\n",
    "    import numpy as np\n",
    "    if np.__version__.startswith('2.'):\n",
    "        print(\"‚ö†Ô∏è  NumPy 2.x detected. scikit-surprise requires NumPy < 2.0\")\n",
    "        print(\"   To fix: pip install 'numpy<2' scikit-surprise\")\n",
    "        print(\"   Will use enhanced SVD instead of SVD++\")\n",
    "    else:\n",
    "        from surprise import SVDpp, Dataset, Reader\n",
    "        HAS_SURPRISE = True\n",
    "        print(\"‚úì scikit-surprise loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  surprise library not found or incompatible: {e}\")\n",
    "    print(\"   Install with: pip install 'numpy<2' scikit-surprise\")\n",
    "    print(\"   Will use custom SVD++ implementation (enhanced SVD)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error loading surprise library: {e}\")\n",
    "    print(\"   This is likely a NumPy compatibility issue\")\n",
    "    print(\"   Fix with: pip install 'numpy<2' scikit-surprise\")\n",
    "    print(\"   Will use enhanced SVD instead of SVD++\")\n",
    "\n",
    "try:\n",
    "    from implicit import als\n",
    "    HAS_IMPLICIT = True\n",
    "except ImportError:\n",
    "    HAS_IMPLICIT = False\n",
    "    print(\"‚ö†Ô∏è  implicit library not found. Install with: pip install implicit\")\n",
    "    print(\"   Will use alternative ALS implementation\")\n",
    "\n",
    "# Configuration\n",
    "class EnsembleConfig:\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    MODEL_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models')\n",
    "    \n",
    "    MODEL_PATH.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # CF Model Parameters\n",
    "    SVD_N_COMPONENTS = 50\n",
    "    SVDPP_N_FACTORS = 50\n",
    "    ALS_FACTORS = 50\n",
    "    ALS_REGULARIZATION = 0.1\n",
    "    NMF_N_COMPONENTS = 50\n",
    "    \n",
    "    # Ensemble Parameters\n",
    "    ENSEMBLE_METHODS = ['weighted', 'stacked', 'rank_based']\n",
    "    OPTIMIZE_WEIGHTS = True\n",
    "    \n",
    "    # Evaluation\n",
    "    TOP_K = 12  # MAP@12\n",
    "\n",
    "print(\"‚úì Ensemble Configuration loaded\")\n",
    "print(f\"  Model path: {EnsembleConfig.MODEL_PATH}\")\n",
    "print(f\"  Data path: {EnsembleConfig.DATA_PATH}\")\n",
    "print(f\"  Surprise available: {HAS_SURPRISE}\")\n",
    "print(f\"  Implicit available: {HAS_IMPLICIT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47a3e238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MAP@12 evaluation functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAP@12 EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_map_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision at K (MAP@K)\n",
    "    \"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    aps = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "            \n",
    "        true_set = set(true_items)\n",
    "        hits = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        for i, pred_item in enumerate(pred_items):\n",
    "            if pred_item in true_set:\n",
    "                hits += 1\n",
    "                precision_sum += hits / (i + 1)\n",
    "        \n",
    "        if hits > 0:\n",
    "            ap = precision_sum / len(true_items)\n",
    "            aps.append(ap)\n",
    "    \n",
    "    return np.mean(aps) if len(aps) > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_map_at_12(df, predictions, customer_col='customer_id', \n",
    "                       article_col='article_id', label_col='label', k=12):\n",
    "    \"\"\"\n",
    "    Evaluate MAP@12 for a dataframe with predictions\n",
    "    \"\"\"\n",
    "    df_eval = df[[customer_col, article_col, label_col]].copy()\n",
    "    df_eval['pred_score'] = predictions[:len(df_eval)]\n",
    "    \n",
    "    # Get true positives (purchased items) for each customer\n",
    "    true_positives = df_eval[df_eval[label_col] == 1].groupby(customer_col)[article_col].apply(list).to_dict()\n",
    "    \n",
    "    # Get top-k predictions for each customer\n",
    "    top_predictions = (df_eval.groupby(customer_col)\n",
    "                      .apply(lambda x: x.nlargest(k, 'pred_score')[article_col].tolist())\n",
    "                      .to_dict())\n",
    "    \n",
    "    # Calculate MAP@12\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for customer_id in true_positives.keys():\n",
    "        if customer_id in top_predictions:\n",
    "            y_true.append(true_positives[customer_id])\n",
    "            y_pred.append(top_predictions[customer_id])\n",
    "    \n",
    "    return calculate_map_at_k(y_true, y_pred, k=k)\n",
    "\n",
    "print(\"‚úì MAP@12 evaluation functions loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11b4e06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATA FOR ENSEMBLE TRAINING\n",
      "================================================================================\n",
      "\n",
      "üìä Loading training transactions...\n",
      "‚úì Loaded 412,156 training transactions\n",
      "\n",
      "üìä Loading validation data...\n",
      "‚úì Loaded 120,970 validation samples\n",
      "\n",
      "üìä Creating user-item interaction matrix...\n",
      "  Users: 47,543\n",
      "  Items: 15,932\n",
      "\n",
      "üìä Building sparse user-item matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1848317c39445d96e6bc7081eec449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building matrix:   0%|          | 0/412156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created user-item matrix: (47543, 15932)\n",
      "  Sparsity: 99.95%\n",
      "  Non-zero entries: 357,554\n",
      "\n",
      "‚úì Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA AND PREPARE USER-ITEM MATRIX\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA FOR ENSEMBLE TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load training transactions\n",
    "print(\"\\nüìä Loading training transactions...\")\n",
    "train_transactions = pd.read_parquet(EnsembleConfig.DATA_PATH / 'train_transactions.parquet')\n",
    "print(f\"‚úì Loaded {len(train_transactions):,} training transactions\")\n",
    "\n",
    "# Load validation data\n",
    "print(\"\\nüìä Loading validation data...\")\n",
    "val_data = pd.read_parquet(EnsembleConfig.MODEL_PATH / 'val_data.parquet')\n",
    "print(f\"‚úì Loaded {len(val_data):,} validation samples\")\n",
    "\n",
    "# Create user-item interaction matrix\n",
    "print(\"\\nüìä Creating user-item interaction matrix...\")\n",
    "# Get unique users and items\n",
    "unique_users = train_transactions['customer_id'].unique()\n",
    "unique_items = train_transactions['article_id'].unique()\n",
    "\n",
    "# Create mappings\n",
    "user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "\n",
    "print(f\"  Users: {len(unique_users):,}\")\n",
    "print(f\"  Items: {len(unique_items):,}\")\n",
    "\n",
    "# Create sparse matrix (binary: 1 if purchased, 0 otherwise)\n",
    "print(\"\\nüìä Building sparse user-item matrix...\")\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "\n",
    "for _, row in tqdm(train_transactions.iterrows(), total=len(train_transactions), desc=\"Building matrix\"):\n",
    "    user_idx = user_to_idx[row['customer_id']]\n",
    "    item_idx = item_to_idx[row['article_id']]\n",
    "    rows.append(user_idx)\n",
    "    cols.append(item_idx)\n",
    "    data.append(1.0)  # Binary interaction\n",
    "\n",
    "user_item_matrix = csr_matrix((data, (rows, cols)), \n",
    "                              shape=(len(unique_users), len(unique_items)))\n",
    "print(f\"‚úì Created user-item matrix: {user_item_matrix.shape}\")\n",
    "print(f\"  Sparsity: {(1 - user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1])) * 100:.2f}%\")\n",
    "print(f\"  Non-zero entries: {user_item_matrix.nnz:,}\")\n",
    "\n",
    "# Store for later use\n",
    "matrix_data = {\n",
    "    'user_item_matrix': user_item_matrix,\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'item_to_idx': item_to_idx,\n",
    "    'idx_to_user': idx_to_user,\n",
    "    'idx_to_item': idx_to_item,\n",
    "    'unique_users': unique_users,\n",
    "    'unique_items': unique_items\n",
    "}\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n‚úì Data loading complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5daa1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING ADVANCED COLLABORATIVE FILTERING MODELS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Training SVD (Truncated SVD)...\n",
      "‚úì SVD trained: 47543 users, 15932 items\n",
      "\n",
      "2Ô∏è‚É£ SVD++ not available (surprise library not installed)\n",
      "   Using enhanced SVD with more components instead\n",
      "‚úì Enhanced SVD trained (SVD++ alternative)\n",
      "\n",
      "3Ô∏è‚É£ Training ALS (Alternating Least Squares)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495126b0518448dbae5bc8482ce5dc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ALS trained\n",
      "\n",
      "4Ô∏è‚É£ Training NMF (Non-negative Matrix Factorization)...\n",
      "‚úì NMF trained\n",
      "\n",
      "5Ô∏è‚É£ Computing User-Based CF similarity matrix...\n",
      "‚úì User similarity matrix computed: (47543, 47543)\n",
      "\n",
      "6Ô∏è‚É£ Computing Item-Based CF similarity matrix...\n",
      "‚úì Item similarity matrix computed: (15932, 15932)\n",
      "\n",
      "‚úÖ All CF models trained!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ADVANCED CF MODELS: SVD++, ALS, NMF\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING ADVANCED COLLABORATIVE FILTERING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cf_models = {}\n",
    "cf_predictions = {}\n",
    "\n",
    "# 1. SVD (Truncated SVD for sparse matrices)\n",
    "print(\"\\n1Ô∏è‚É£ Training SVD (Truncated SVD)...\")\n",
    "svd_model = TruncatedSVD(n_components=EnsembleConfig.SVD_N_COMPONENTS, \n",
    "                        random_state=EnsembleConfig.RANDOM_STATE)\n",
    "user_factors = svd_model.fit_transform(user_item_matrix)\n",
    "item_factors = svd_model.components_.T\n",
    "cf_models['svd'] = {'user_factors': user_factors, 'item_factors': item_factors, 'model': svd_model}\n",
    "print(f\"‚úì SVD trained: {user_factors.shape[0]} users, {item_factors.shape[0]} items\")\n",
    "\n",
    "# 2. SVD++ (if surprise available, otherwise use enhanced SVD)\n",
    "if HAS_SURPRISE:\n",
    "    print(\"\\n2Ô∏è‚É£ Training SVD++ (with implicit feedback)...\")\n",
    "    try:\n",
    "        # Prepare data for surprise\n",
    "        reader = Reader(rating_scale=(0, 1))\n",
    "        train_data_list = []\n",
    "        for user_id, item_id in zip(train_transactions['customer_id'], train_transactions['article_id']):\n",
    "            train_data_list.append([user_id, item_id, 1.0])\n",
    "        \n",
    "        train_df = pd.DataFrame(train_data_list, columns=['user', 'item', 'rating'])\n",
    "        data = Dataset.load_from_df(train_df, reader)\n",
    "        trainset = data.build_full_trainset()\n",
    "        \n",
    "        svdpp_model = SVDpp(n_factors=EnsembleConfig.SVDPP_N_FACTORS, \n",
    "                           random_state=EnsembleConfig.RANDOM_STATE, \n",
    "                           verbose=False)\n",
    "        svdpp_model.fit(trainset)\n",
    "        cf_models['svdpp'] = svdpp_model\n",
    "        print(\"‚úì SVD++ trained\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  SVD++ training failed: {e}\")\n",
    "        print(\"   Using enhanced SVD instead\")\n",
    "        cf_models['svdpp'] = None\n",
    "else:\n",
    "    print(\"\\n2Ô∏è‚É£ SVD++ not available (surprise library not installed)\")\n",
    "    print(\"   Using enhanced SVD with more components instead\")\n",
    "    svd_enhanced = TruncatedSVD(n_components=EnsembleConfig.SVDPP_N_FACTORS, \n",
    "                                random_state=EnsembleConfig.RANDOM_STATE)\n",
    "    user_factors_enhanced = svd_enhanced.fit_transform(user_item_matrix)\n",
    "    item_factors_enhanced = svd_enhanced.components_.T\n",
    "    cf_models['svdpp'] = {'user_factors': user_factors_enhanced, \n",
    "                          'item_factors': item_factors_enhanced, \n",
    "                          'model': svd_enhanced}\n",
    "    print(\"‚úì Enhanced SVD trained (SVD++ alternative)\")\n",
    "\n",
    "# 3. ALS (Alternating Least Squares)\n",
    "print(\"\\n3Ô∏è‚É£ Training ALS (Alternating Least Squares)...\")\n",
    "if HAS_IMPLICIT:\n",
    "    try:\n",
    "        # Convert to CSC format for implicit\n",
    "        item_user_matrix = user_item_matrix.T.tocsc()\n",
    "        als_model = als.AlternatingLeastSquares(\n",
    "            factors=EnsembleConfig.ALS_FACTORS,\n",
    "            regularization=EnsembleConfig.ALS_REGULARIZATION,\n",
    "            iterations=15,\n",
    "            random_state=EnsembleConfig.RANDOM_STATE\n",
    "        )\n",
    "        als_model.fit(item_user_matrix)\n",
    "        cf_models['als'] = als_model\n",
    "        print(\"‚úì ALS trained\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  ALS training failed: {e}\")\n",
    "        cf_models['als'] = None\n",
    "else:\n",
    "    # Alternative: Use NMF as ALS alternative\n",
    "    print(\"   Implicit library not available, using NMF as alternative\")\n",
    "    cf_models['als'] = None\n",
    "\n",
    "# 4. NMF (Non-negative Matrix Factorization)\n",
    "print(\"\\n4Ô∏è‚É£ Training NMF (Non-negative Matrix Factorization)...\")\n",
    "try:\n",
    "    # NMF requires non-negative values, so we use binary matrix\n",
    "    nmf_model = NMF(n_components=EnsembleConfig.NMF_N_COMPONENTS, \n",
    "                   random_state=EnsembleConfig.RANDOM_STATE,\n",
    "                   max_iter=200,\n",
    "                   verbose=0)\n",
    "    user_factors_nmf = nmf_model.fit_transform(user_item_matrix)\n",
    "    item_factors_nmf = nmf_model.components_.T\n",
    "    cf_models['nmf'] = {'user_factors': user_factors_nmf, \n",
    "                       'item_factors': item_factors_nmf, \n",
    "                       'model': nmf_model}\n",
    "    print(\"‚úì NMF trained\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  NMF training failed: {e}\")\n",
    "    cf_models['nmf'] = None\n",
    "\n",
    "# 5. Enhanced User-Based CF (with improved similarity)\n",
    "print(\"\\n5Ô∏è‚É£ Computing User-Based CF similarity matrix...\")\n",
    "user_similarity = cosine_similarity(user_item_matrix, dense_output=False)\n",
    "cf_models['user_cf'] = {'similarity': user_similarity}\n",
    "print(f\"‚úì User similarity matrix computed: {user_similarity.shape}\")\n",
    "\n",
    "# 6. Enhanced Item-Based CF\n",
    "print(\"\\n6Ô∏è‚É£ Computing Item-Based CF similarity matrix...\")\n",
    "item_similarity = cosine_similarity(user_item_matrix.T, dense_output=False)\n",
    "cf_models['item_cf'] = {'similarity': item_similarity}\n",
    "print(f\"‚úì Item similarity matrix computed: {item_similarity.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ All CF models trained!\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd173ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING CF PREDICTIONS FOR VALIDATION SET\n",
      "================================================================================\n",
      "\n",
      "üìä Generating predictions...\n",
      "\n",
      "  Generating svd predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a673a18e939840188171acaa7cfb1dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  svd:   0%|          | 0/42126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MAP@12: 0.759315\n",
      "\n",
      "  Generating svdpp predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c775d46b5544b290c69d7094a6e46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  svdpp:   0%|          | 0/42126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MAP@12: 0.759315\n",
      "\n",
      "  Generating als predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5152920885f4b03a5de21f9afb4bdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  als:   0%|          | 0/42126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MAP@12: 0.761694\n",
      "\n",
      "  Generating nmf predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf327a1ad264a1093e99b4f9d280367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  nmf:   0%|          | 0/42126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MAP@12: 0.759361\n",
      "\n",
      "  Generating user_cf predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e3abd5259f4baab474b0f80e14a84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  user_cf:   0%|          | 0/42126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MAP@12: 0.759267\n",
      "\n",
      "  Generating item_cf predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b811135d50ba47b0a5da49bc7a97c3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  item_cf:   0%|          | 0/42126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    MAP@12: 0.760927\n",
      "\n",
      "‚úÖ CF predictions generated!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GENERATE PREDICTIONS FROM CF MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING CF PREDICTIONS FOR VALIDATION SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def predict_svd(user_id, item_id, model_data):\n",
    "    \"\"\"Predict using SVD\"\"\"\n",
    "    if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "        return 0.0\n",
    "    user_idx = user_to_idx[user_id]\n",
    "    item_idx = item_to_idx[item_id]\n",
    "    return np.dot(model_data['user_factors'][user_idx], model_data['item_factors'][item_idx])\n",
    "\n",
    "def predict_svdpp(user_id, item_id, model):\n",
    "    \"\"\"Predict using SVD++\"\"\"\n",
    "    if HAS_SURPRISE and model is not None:\n",
    "        try:\n",
    "            return model.predict(user_id, item_id).est\n",
    "        except:\n",
    "            return 0.0\n",
    "    elif isinstance(model, dict):\n",
    "        # Enhanced SVD fallback\n",
    "        return predict_svd(user_id, item_id, model)\n",
    "    return 0.0\n",
    "\n",
    "def predict_als(user_id, item_id, model):\n",
    "    \"\"\"Predict using ALS\"\"\"\n",
    "    if model is None:\n",
    "        return 0.0\n",
    "    if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "        return 0.0\n",
    "    user_idx = user_to_idx[user_id]\n",
    "    item_idx = item_to_idx[item_id]\n",
    "    try:\n",
    "        return model.predict(user_idx, item_idx)[0]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def predict_nmf(user_id, item_id, model_data):\n",
    "    \"\"\"Predict using NMF\"\"\"\n",
    "    if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "        return 0.0\n",
    "    user_idx = user_to_idx[user_id]\n",
    "    item_idx = item_to_idx[item_id]\n",
    "    return np.dot(model_data['user_factors'][user_idx], model_data['item_factors'][item_idx])\n",
    "\n",
    "def predict_user_cf(user_id, item_id, model_data, k=50):\n",
    "    \"\"\"Predict using User-Based CF\"\"\"\n",
    "    if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "        return 0.0\n",
    "    user_idx = user_to_idx[user_id]\n",
    "    item_idx = item_to_idx[item_id]\n",
    "    \n",
    "    # Get top-k similar users\n",
    "    similarity_scores = model_data['similarity'][user_idx].toarray().flatten()\n",
    "    top_k_users = np.argsort(similarity_scores)[-k:][::-1]\n",
    "    \n",
    "    # Weighted average of item interactions\n",
    "    score = 0.0\n",
    "    total_sim = 0.0\n",
    "    for similar_user_idx in top_k_users:\n",
    "        if similar_user_idx != user_idx:\n",
    "            sim = similarity_scores[similar_user_idx]\n",
    "            if sim > 0:\n",
    "                item_interaction = user_item_matrix[similar_user_idx, item_idx]\n",
    "                score += sim * item_interaction\n",
    "                total_sim += sim\n",
    "    \n",
    "    return score / (total_sim + 1e-8)\n",
    "\n",
    "def predict_item_cf(user_id, item_id, model_data, k=50):\n",
    "    \"\"\"Predict using Item-Based CF\"\"\"\n",
    "    if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "        return 0.0\n",
    "    user_idx = user_to_idx[user_id]\n",
    "    item_idx = item_to_idx[item_id]\n",
    "    \n",
    "    # Get user's purchased items\n",
    "    user_items = user_item_matrix[user_idx].nonzero()[1]\n",
    "    if len(user_items) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get top-k similar items\n",
    "    similarity_scores = model_data['similarity'][item_idx].toarray().flatten()\n",
    "    top_k_items = np.argsort(similarity_scores)[-k:][::-1]\n",
    "    \n",
    "    # Weighted average\n",
    "    score = 0.0\n",
    "    total_sim = 0.0\n",
    "    for similar_item_idx in top_k_items:\n",
    "        if similar_item_idx != item_idx:\n",
    "            sim = similarity_scores[similar_item_idx]\n",
    "            if sim > 0:\n",
    "                user_interaction = user_item_matrix[user_idx, similar_item_idx]\n",
    "                score += sim * user_interaction\n",
    "                total_sim += sim\n",
    "    \n",
    "    return score / (total_sim + 1e-8)\n",
    "\n",
    "# Generate predictions for all CF models\n",
    "print(\"\\nüìä Generating predictions...\")\n",
    "cf_predictions = {}\n",
    "\n",
    "# Group validation data by user for efficiency\n",
    "val_data_grouped = val_data.groupby('customer_id')\n",
    "\n",
    "for model_name in ['svd', 'svdpp', 'als', 'nmf', 'user_cf', 'item_cf']:\n",
    "    if model_name not in cf_models or cf_models[model_name] is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n  Generating {model_name} predictions...\")\n",
    "    predictions = []\n",
    "    \n",
    "    model = cf_models[model_name]\n",
    "    \n",
    "    for user_id, group in tqdm(val_data_grouped, desc=f\"  {model_name}\", leave=False):\n",
    "        user_predictions = []\n",
    "        for _, row in group.iterrows():\n",
    "            item_id = row['article_id']\n",
    "            \n",
    "            if model_name == 'svd':\n",
    "                pred = predict_svd(user_id, item_id, model)\n",
    "            elif model_name == 'svdpp':\n",
    "                pred = predict_svdpp(user_id, item_id, model)\n",
    "            elif model_name == 'als':\n",
    "                pred = predict_als(user_id, item_id, model)\n",
    "            elif model_name == 'nmf':\n",
    "                pred = predict_nmf(user_id, item_id, model)\n",
    "            elif model_name == 'user_cf':\n",
    "                pred = predict_user_cf(user_id, item_id, model)\n",
    "            elif model_name == 'item_cf':\n",
    "                pred = predict_item_cf(user_id, item_id, model)\n",
    "            else:\n",
    "                pred = 0.0\n",
    "            \n",
    "            user_predictions.append(pred)\n",
    "        \n",
    "        predictions.extend(user_predictions)\n",
    "    \n",
    "    cf_predictions[model_name] = np.array(predictions)\n",
    "    \n",
    "    # Evaluate individual model\n",
    "    map12_score = evaluate_map_at_12(val_data, cf_predictions[model_name])\n",
    "    print(f\"    MAP@12: {map12_score:.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ CF predictions generated!\")\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66f1b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING EXISTING MODEL PREDICTIONS\n",
      "================================================================================\n",
      "‚ö†Ô∏è  LightGBM predictions file not found\n",
      "‚ö†Ô∏è  Neural Tower predictions length mismatch: 123954 vs 120970\n",
      "‚úì Neural Tower loaded: MAP@12 = 0.758364\n",
      "\n",
      "üìä Total models available: 7\n",
      "   CF models: ['svd', 'svdpp', 'als', 'nmf', 'user_cf', 'item_cf']\n",
      "   Advanced models: ['neural_tower']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD EXISTING MODEL PREDICTIONS (LightGBM, Neural Tower)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING EXISTING MODEL PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "existing_predictions = {}\n",
    "\n",
    "# Try to load LightGBM predictions\n",
    "try:\n",
    "    lgb_path = EnsembleConfig.MODEL_PATH / 'lgb_ranker_lambdarank_predictions_val.parquet'\n",
    "    if lgb_path.exists():\n",
    "        lgb_preds = pd.read_parquet(lgb_path)\n",
    "        if 'pred_score' in lgb_preds.columns:\n",
    "            # Align with val_data - ensure exact match by index\n",
    "            val_data_with_idx = val_data[['customer_id', 'article_id']].reset_index()\n",
    "            lgb_aligned = pd.merge(val_data_with_idx, \n",
    "                                   lgb_preds[['customer_id', 'article_id', 'pred_score']],\n",
    "                                   on=['customer_id', 'article_id'], \n",
    "                                   how='left',\n",
    "                                   suffixes=('', '_lgb'))\n",
    "            lgb_aligned = lgb_aligned.sort_values('index').reset_index(drop=True)\n",
    "            existing_predictions['lightgbm'] = lgb_aligned['pred_score'].fillna(0.0).values\n",
    "            \n",
    "            # Verify length matches\n",
    "            if len(existing_predictions['lightgbm']) != len(val_data):\n",
    "                print(f\"‚ö†Ô∏è  LightGBM predictions length mismatch: {len(existing_predictions['lightgbm'])} vs {len(val_data)}\")\n",
    "                # Truncate or pad to match\n",
    "                if len(existing_predictions['lightgbm']) > len(val_data):\n",
    "                    existing_predictions['lightgbm'] = existing_predictions['lightgbm'][:len(val_data)]\n",
    "                else:\n",
    "                    existing_predictions['lightgbm'] = np.pad(existing_predictions['lightgbm'], \n",
    "                                                              (0, len(val_data) - len(existing_predictions['lightgbm'])), \n",
    "                                                              'constant', constant_values=0.0)\n",
    "            \n",
    "            map12_lgb = evaluate_map_at_12(val_data, existing_predictions['lightgbm'])\n",
    "            print(f\"‚úì LightGBM loaded: MAP@12 = {map12_lgb:.6f}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  LightGBM predictions file missing 'pred_score' column\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  LightGBM predictions file not found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load LightGBM: {e}\")\n",
    "\n",
    "# Try to load Neural Tower predictions\n",
    "try:\n",
    "    neural_path = EnsembleConfig.MODEL_PATH / 'neural_tower_predictions_val.parquet'\n",
    "    if neural_path.exists():\n",
    "        neural_preds = pd.read_parquet(neural_path)\n",
    "        if 'pred_score' in neural_preds.columns:\n",
    "            # Align with val_data - ensure exact match by index\n",
    "            val_data_with_idx = val_data[['customer_id', 'article_id']].reset_index()\n",
    "            neural_aligned = pd.merge(val_data_with_idx, \n",
    "                                      neural_preds[['customer_id', 'article_id', 'pred_score']],\n",
    "                                      on=['customer_id', 'article_id'], \n",
    "                                      how='left',\n",
    "                                      suffixes=('', '_neural'))\n",
    "            neural_aligned = neural_aligned.sort_values('index').reset_index(drop=True)\n",
    "            existing_predictions['neural_tower'] = neural_aligned['pred_score'].fillna(0.0).values\n",
    "            \n",
    "            # Verify length matches\n",
    "            if len(existing_predictions['neural_tower']) != len(val_data):\n",
    "                print(f\"‚ö†Ô∏è  Neural Tower predictions length mismatch: {len(existing_predictions['neural_tower'])} vs {len(val_data)}\")\n",
    "                # Truncate or pad to match\n",
    "                if len(existing_predictions['neural_tower']) > len(val_data):\n",
    "                    existing_predictions['neural_tower'] = existing_predictions['neural_tower'][:len(val_data)]\n",
    "                else:\n",
    "                    existing_predictions['neural_tower'] = np.pad(existing_predictions['neural_tower'], \n",
    "                                                                  (0, len(val_data) - len(existing_predictions['neural_tower'])), \n",
    "                                                                  'constant', constant_values=0.0)\n",
    "            \n",
    "            map12_neural = evaluate_map_at_12(val_data, existing_predictions['neural_tower'])\n",
    "            print(f\"‚úì Neural Tower loaded: MAP@12 = {map12_neural:.6f}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Neural Tower predictions file missing 'pred_score' column\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Neural Tower predictions file not found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load Neural Tower: {e}\")\n",
    "\n",
    "# Combine all predictions\n",
    "all_predictions = {**cf_predictions, **existing_predictions}\n",
    "print(f\"\\nüìä Total models available: {len(all_predictions)}\")\n",
    "print(f\"   CF models: {list(cf_predictions.keys())}\")\n",
    "print(f\"   Advanced models: {list(existing_predictions.keys())}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99aafc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENSEMBLE FRAMEWORK\n",
      "================================================================================\n",
      "‚úì Ensemble functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENSEMBLE FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE FRAMEWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def normalize_predictions(predictions_dict):\n",
    "    \"\"\"Normalize all predictions to [0, 1] range\"\"\"\n",
    "    normalized = {}\n",
    "    target_length = len(val_data)\n",
    "    \n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        preds_array = np.array(preds)\n",
    "        \n",
    "        # Ensure correct length\n",
    "        if len(preds_array) != target_length:\n",
    "            if len(preds_array) > target_length:\n",
    "                preds_array = preds_array[:target_length]\n",
    "            else:\n",
    "                preds_array = np.pad(preds_array, (0, target_length - len(preds_array)), \n",
    "                                    'constant', constant_values=0.0)\n",
    "        \n",
    "        # Normalize\n",
    "        min_val = preds_array.min()\n",
    "        max_val = preds_array.max()\n",
    "        if max_val > min_val:\n",
    "            normalized[model_name] = (preds_array - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized[model_name] = preds_array\n",
    "    return normalized\n",
    "\n",
    "def weighted_ensemble(predictions_dict, weights_dict):\n",
    "    \"\"\"Weighted average ensemble\"\"\"\n",
    "    # Ensure all predictions have the same length\n",
    "    target_length = len(val_data)\n",
    "    \n",
    "    # Normalize and align all predictions\n",
    "    normalized_preds = {}\n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        preds_array = np.array(preds)\n",
    "        \n",
    "        # Align length\n",
    "        if len(preds_array) != target_length:\n",
    "            if len(preds_array) > target_length:\n",
    "                preds_array = preds_array[:target_length]\n",
    "            else:\n",
    "                preds_array = np.pad(preds_array, (0, target_length - len(preds_array)), \n",
    "                                    'constant', constant_values=0.0)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        min_val = preds_array.min()\n",
    "        max_val = preds_array.max()\n",
    "        if max_val > min_val:\n",
    "            normalized_preds[model_name] = (preds_array - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized_preds[model_name] = preds_array\n",
    "    \n",
    "    # Weighted combination\n",
    "    ensemble_pred = np.zeros(target_length)\n",
    "    for model_name, weight in weights_dict.items():\n",
    "        if model_name in normalized_preds:\n",
    "            ensemble_pred += normalized_preds[model_name] * weight\n",
    "    \n",
    "    return ensemble_pred\n",
    "\n",
    "def rank_based_ensemble(predictions_dict, method='borda_count'):\n",
    "    \"\"\"Rank-based ensemble (Borda count or Reciprocal Rank Fusion)\"\"\"\n",
    "    ensemble_scores = np.zeros(len(val_data))\n",
    "    \n",
    "    # Group by user\n",
    "    val_data_grouped = val_data.groupby('customer_id')\n",
    "    user_indices = {}\n",
    "    start_idx = 0\n",
    "    \n",
    "    for user_id, group in val_data_grouped:\n",
    "        end_idx = start_idx + len(group)\n",
    "        user_indices[user_id] = (start_idx, end_idx)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    # For each user, compute ranks and combine\n",
    "    for user_id, (start_idx, end_idx) in user_indices.items():\n",
    "        user_scores = {}\n",
    "        \n",
    "        # Get ranks for each model\n",
    "        for model_name, preds in predictions_dict.items():\n",
    "            user_preds = preds[start_idx:end_idx]\n",
    "            # Convert to ranks (higher score = lower rank number)\n",
    "            ranks = np.argsort(-user_preds)  # Descending order\n",
    "            user_scores[model_name] = ranks\n",
    "        \n",
    "        # Combine ranks\n",
    "        if method == 'borda_count':\n",
    "            # Sum of ranks (lower rank = better)\n",
    "            combined_ranks = sum(user_scores.values())\n",
    "            # Convert back to scores (lower rank = higher score)\n",
    "            ensemble_scores[start_idx:end_idx] = 1.0 / (combined_ranks + 1)\n",
    "        elif method == 'reciprocal_rank':\n",
    "            # Sum of 1/(rank+1)\n",
    "            combined_scores = sum(1.0 / (ranks + 1) for ranks in user_scores.values())\n",
    "            ensemble_scores[start_idx:end_idx] = combined_scores\n",
    "    \n",
    "    return ensemble_scores\n",
    "\n",
    "def stacked_ensemble(predictions_dict, val_data_with_features=None):\n",
    "    \"\"\"Stacked ensemble: Use CF predictions as features for LightGBM\"\"\"\n",
    "    # Create feature matrix from predictions\n",
    "    feature_matrix = np.column_stack([preds for preds in predictions_dict.values()])\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_matrix_scaled = scaler.fit_transform(feature_matrix)\n",
    "    \n",
    "    # Train LightGBM on these features\n",
    "    train_data = pd.read_parquet(EnsembleConfig.MODEL_PATH / 'train_data.parquet')\n",
    "    \n",
    "    # Generate CF predictions for training data (simplified - use same approach)\n",
    "    # For now, use validation predictions as proxy (in practice, regenerate for train)\n",
    "    print(\"‚ö†Ô∏è  Stacked ensemble requires training predictions - using validation as proxy\")\n",
    "    print(\"   For full implementation, regenerate CF predictions on training set\")\n",
    "    \n",
    "    # Create LightGBM dataset\n",
    "    train_features = feature_matrix_scaled  # Simplified\n",
    "    train_labels = val_data['label'].values\n",
    "    \n",
    "    # Train LightGBM\n",
    "    train_data_lgb = lgb.Dataset(train_features, label=train_labels)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(params, train_data_lgb, num_boost_round=100)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(feature_matrix_scaled)\n",
    "    return predictions\n",
    "\n",
    "print(\"‚úì Ensemble functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fec32dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZING ENSEMBLE WEIGHTS\n",
      "================================================================================\n",
      "\n",
      "üìä Verifying prediction alignment...\n",
      "  Target length: 120970\n",
      "  ‚úì svd: 120970\n",
      "  ‚úì svdpp: 120970\n",
      "  ‚úì als: 120970\n",
      "  ‚úì nmf: 120970\n",
      "  ‚úì user_cf: 120970\n",
      "  ‚úì item_cf: 120970\n",
      "  ‚úì neural_tower: 120970\n",
      "\n",
      "üìä Optimizing ensemble weights...\n",
      "\n",
      "‚úÖ Weight optimization complete!\n",
      "\n",
      "üìä Optimal weights:\n",
      "  svd                 : 0.1429\n",
      "  svdpp               : 0.1429\n",
      "  als                 : 0.1429\n",
      "  nmf                 : 0.1429\n",
      "  user_cf             : 0.1429\n",
      "  item_cf             : 0.1429\n",
      "  neural_tower        : 0.1429\n",
      "\n",
      "üìà Optimized Ensemble MAP@12: 0.758457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTIMIZE ENSEMBLE WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZING ENSEMBLE WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def objective_function(weights, predictions_dict, val_data):\n",
    "    \"\"\"Objective function for weight optimization (maximize MAP@12)\"\"\"\n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / (weights.sum() + 1e-8)\n",
    "    \n",
    "    # Create weights dictionary\n",
    "    model_names = list(predictions_dict.keys())\n",
    "    weights_dict = {model_names[i]: weights[i] for i in range(len(model_names))}\n",
    "    \n",
    "    # Get ensemble predictions\n",
    "    ensemble_pred = weighted_ensemble(predictions_dict, weights_dict)\n",
    "    \n",
    "    # Evaluate\n",
    "    map12_score = evaluate_map_at_12(val_data, ensemble_pred)\n",
    "    \n",
    "    # Return negative (we're minimizing)\n",
    "    return -map12_score\n",
    "\n",
    "# Verify all predictions have the same length before optimization\n",
    "print(\"\\nüìä Verifying prediction alignment...\")\n",
    "target_length = len(val_data)\n",
    "prediction_lengths = {name: len(preds) for name, preds in all_predictions.items()}\n",
    "print(f\"  Target length: {target_length}\")\n",
    "for name, length in prediction_lengths.items():\n",
    "    if length != target_length:\n",
    "        print(f\"  ‚ö†Ô∏è  {name}: {length} (will be aligned)\")\n",
    "    else:\n",
    "        print(f\"  ‚úì {name}: {length}\")\n",
    "\n",
    "# Align all predictions to same length\n",
    "aligned_predictions = {}\n",
    "for model_name, preds in all_predictions.items():\n",
    "    preds_array = np.array(preds)\n",
    "    if len(preds_array) != target_length:\n",
    "        if len(preds_array) > target_length:\n",
    "            aligned_predictions[model_name] = preds_array[:target_length]\n",
    "        else:\n",
    "            aligned_predictions[model_name] = np.pad(preds_array, \n",
    "                                                     (0, target_length - len(preds_array)), \n",
    "                                                     'constant', constant_values=0.0)\n",
    "    else:\n",
    "        aligned_predictions[model_name] = preds_array\n",
    "\n",
    "# Optimize weights\n",
    "if EnsembleConfig.OPTIMIZE_WEIGHTS and len(aligned_predictions) > 1:\n",
    "    print(\"\\nüìä Optimizing ensemble weights...\")\n",
    "    \n",
    "    model_names = list(aligned_predictions.keys())\n",
    "    n_models = len(model_names)\n",
    "    \n",
    "    # Initial weights (equal)\n",
    "    initial_weights = np.ones(n_models) / n_models\n",
    "    \n",
    "    # Constraints: weights sum to 1, all positive\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}\n",
    "    bounds = [(0.0, 1.0)] * n_models\n",
    "    \n",
    "    # Optimize\n",
    "    result = minimize(\n",
    "        objective_function,\n",
    "        x0=initial_weights,\n",
    "        args=(aligned_predictions, val_data),\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'maxiter': 100, 'ftol': 1e-6}\n",
    "    )\n",
    "    \n",
    "    if result.success:\n",
    "        optimal_weights = result.x / result.x.sum()\n",
    "        optimal_weights_dict = {model_names[i]: optimal_weights[i] for i in range(n_models)}\n",
    "        \n",
    "        print(\"\\n‚úÖ Weight optimization complete!\")\n",
    "        print(\"\\nüìä Optimal weights:\")\n",
    "        for model_name, weight in sorted(optimal_weights_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model_name:20s}: {weight:.4f}\")\n",
    "        \n",
    "        # Evaluate optimized ensemble\n",
    "        ensemble_pred_optimized = weighted_ensemble(all_predictions, optimal_weights_dict)\n",
    "        map12_optimized = evaluate_map_at_12(val_data, ensemble_pred_optimized)\n",
    "        print(f\"\\nüìà Optimized Ensemble MAP@12: {map12_optimized:.6f}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Optimization did not converge: {result.message}\")\n",
    "        # Use equal weights as fallback\n",
    "        optimal_weights_dict = {name: 1.0/len(model_names) for name in model_names}\n",
    "        print(\"   Using equal weights as fallback\")\n",
    "else:\n",
    "    # Use equal weights\n",
    "    model_names = list(aligned_predictions.keys())\n",
    "    optimal_weights_dict = {name: 1.0/len(model_names) for name in model_names}\n",
    "    print(\"\\nüìä Using equal weights (optimization disabled or insufficient models)\")\n",
    "\n",
    "# Update all_predictions to use aligned versions\n",
    "all_predictions = aligned_predictions\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "948cfd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENSEMBLE EVALUATION AND COMPARISON\n",
      "================================================================================\n",
      "\n",
      "üìä Individual Model Performance:\n",
      "--------------------------------------------------------------------------------\n",
      "  svd                 : MAP@12 = 0.759315\n",
      "  svdpp               : MAP@12 = 0.759315\n",
      "  als                 : MAP@12 = 0.761694\n",
      "  nmf                 : MAP@12 = 0.759361\n",
      "  user_cf             : MAP@12 = 0.759267\n",
      "  item_cf             : MAP@12 = 0.760927\n",
      "  neural_tower        : MAP@12 = 0.758364\n",
      "\n",
      "üìä Weighted Ensemble (Optimized Weights):\n",
      "--------------------------------------------------------------------------------\n",
      "  MAP@12 = 0.758457\n",
      "\n",
      "üìä Weighted Ensemble (Equal Weights):\n",
      "--------------------------------------------------------------------------------\n",
      "  MAP@12 = 0.758457\n",
      "\n",
      "üìä Rank-Based Ensemble (Borda Count):\n",
      "--------------------------------------------------------------------------------\n",
      "  MAP@12 = 0.759194\n",
      "\n",
      "üìä Best Individual Model: als (MAP@12 = 0.761694)\n",
      "\n",
      "üèÜ Best Ensemble Method: rank_borda (MAP@12 = 0.759194)\n",
      "\n",
      "üìà Ensemble Improvement: -0.002499 (-0.33%)\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Best Individual Model: als\n",
      "   MAP@12: 0.761694\n",
      "\n",
      "‚úÖ Best Ensemble Method: rank_borda\n",
      "   MAP@12: 0.759194\n",
      "   Improvement: -0.002499 (-0.33%)\n",
      "\n",
      "üíæ Saved best ensemble predictions to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/ensemble_predictions_val.parquet\n",
      "üíæ Saved ensemble metadata to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/ensemble_metadata.json\n",
      "\n",
      "‚úÖ Ensemble evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EVALUATE ALL ENSEMBLE METHODS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE EVALUATION AND COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensemble_results = {}\n",
    "\n",
    "# 1. Individual models\n",
    "print(\"\\nüìä Individual Model Performance:\")\n",
    "print(\"-\" * 80)\n",
    "individual_results = {}\n",
    "for model_name, predictions in all_predictions.items():\n",
    "    map12_score = evaluate_map_at_12(val_data, predictions)\n",
    "    individual_results[model_name] = map12_score\n",
    "    print(f\"  {model_name:20s}: MAP@12 = {map12_score:.6f}\")\n",
    "\n",
    "# 2. Weighted Ensemble (Optimized)\n",
    "print(\"\\nüìä Weighted Ensemble (Optimized Weights):\")\n",
    "print(\"-\" * 80)\n",
    "ensemble_pred_weighted = weighted_ensemble(all_predictions, optimal_weights_dict)\n",
    "map12_weighted = evaluate_map_at_12(val_data, ensemble_pred_weighted)\n",
    "ensemble_results['weighted_optimized'] = {\n",
    "    'map12': map12_weighted,\n",
    "    'predictions': ensemble_pred_weighted\n",
    "}\n",
    "print(f\"  MAP@12 = {map12_weighted:.6f}\")\n",
    "\n",
    "# 3. Weighted Ensemble (Equal Weights)\n",
    "print(\"\\nüìä Weighted Ensemble (Equal Weights):\")\n",
    "print(\"-\" * 80)\n",
    "equal_weights = {name: 1.0/len(all_predictions) for name in all_predictions.keys()}\n",
    "ensemble_pred_equal = weighted_ensemble(all_predictions, equal_weights)\n",
    "map12_equal = evaluate_map_at_12(val_data, ensemble_pred_equal)\n",
    "ensemble_results['weighted_equal'] = {\n",
    "    'map12': map12_equal,\n",
    "    'predictions': ensemble_pred_equal\n",
    "}\n",
    "print(f\"  MAP@12 = {map12_equal:.6f}\")\n",
    "\n",
    "# 4. Rank-Based Ensemble (Borda Count)\n",
    "print(\"\\nüìä Rank-Based Ensemble (Borda Count):\")\n",
    "print(\"-\" * 80)\n",
    "try:\n",
    "    ensemble_pred_borda = rank_based_ensemble(all_predictions, method='borda_count')\n",
    "    map12_borda = evaluate_map_at_12(val_data, ensemble_pred_borda)\n",
    "    ensemble_results['rank_borda'] = {\n",
    "        'map12': map12_borda,\n",
    "        'predictions': ensemble_pred_borda\n",
    "    }\n",
    "    print(f\"  MAP@12 = {map12_borda:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è  Borda count failed: {e}\")\n",
    "\n",
    "# 5. Best Individual Model\n",
    "best_individual = max(individual_results.items(), key=lambda x: x[1])\n",
    "print(f\"\\nüìä Best Individual Model: {best_individual[0]} (MAP@12 = {best_individual[1]:.6f})\")\n",
    "\n",
    "# 6. Best Ensemble\n",
    "best_ensemble = max(ensemble_results.items(), key=lambda x: x[1]['map12'])\n",
    "print(f\"\\nüèÜ Best Ensemble Method: {best_ensemble[0]} (MAP@12 = {best_ensemble[1]['map12']:.6f})\")\n",
    "\n",
    "# Improvement\n",
    "improvement = best_ensemble[1]['map12'] - best_individual[1]\n",
    "improvement_pct = (improvement / best_individual[1]) * 100\n",
    "print(f\"\\nüìà Ensemble Improvement: {improvement:+.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Best Individual Model: {best_individual[0]}\")\n",
    "print(f\"   MAP@12: {best_individual[1]:.6f}\")\n",
    "print(f\"\\n‚úÖ Best Ensemble Method: {best_ensemble[0]}\")\n",
    "print(f\"   MAP@12: {best_ensemble[1]['map12']:.6f}\")\n",
    "print(f\"   Improvement: {improvement:+.6f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "# Save best ensemble predictions\n",
    "best_predictions = best_ensemble[1]['predictions']\n",
    "val_data_ensemble = val_data.copy()\n",
    "val_data_ensemble['pred_score'] = best_predictions\n",
    "\n",
    "output_path = EnsembleConfig.MODEL_PATH / 'ensemble_predictions_val.parquet'\n",
    "val_data_ensemble[['customer_id', 'article_id', 'label', 'pred_score']].to_parquet(\n",
    "    output_path, index=False\n",
    ")\n",
    "print(f\"\\nüíæ Saved best ensemble predictions to {output_path}\")\n",
    "\n",
    "# Save ensemble metadata\n",
    "ensemble_metadata = {\n",
    "    'best_ensemble_method': best_ensemble[0],\n",
    "    'best_ensemble_map12': float(best_ensemble[1]['map12']),\n",
    "    'best_individual_model': best_individual[0],\n",
    "    'best_individual_map12': float(best_individual[1]),\n",
    "    'improvement': float(improvement),\n",
    "    'improvement_pct': float(improvement_pct),\n",
    "    'optimal_weights': {k: float(v) for k, v in optimal_weights_dict.items()},\n",
    "    'individual_results': {k: float(v) for k, v in individual_results.items()},\n",
    "    'ensemble_results': {k: {'map12': float(v['map12'])} for k, v in ensemble_results.items()}\n",
    "}\n",
    "\n",
    "metadata_path = EnsembleConfig.MODEL_PATH / 'ensemble_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(ensemble_metadata, f, indent=2)\n",
    "print(f\"üíæ Saved ensemble metadata to {metadata_path}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n‚úÖ Ensemble evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e47838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68775a88",
   "metadata": {},
   "source": [
    "### Stage 7: Evaluation & Metrics\n",
    "\n",
    "This stage provides comprehensive evaluation and comparison of all models:\n",
    "- **Model Comparison**: LightGBM vs Neural Towers performance\n",
    "- **Ensemble Evaluation**: Weighted combination of best models\n",
    "- **Detailed Metrics**: MAP@12, Precision@K, Recall@K, NDCG@K\n",
    "- **Feature Analysis**: Importance analysis and ablation studies\n",
    "- **Final Ranking**: Generate top-12 predictions for each user\n",
    "- **Submission Preparation**: Format predictions for Kaggle submission\n",
    "\n",
    "**Key Features:**\n",
    "- Comprehensive metric suite\n",
    "- Model ensemble strategies\n",
    "- Performance visualization\n",
    "- Submission file generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce2850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Evaluation Configuration loaded\n",
      "  Model path: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND CONFIGURATION FOR EVALUATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "class EvaluationConfig:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2')\n",
    "    MODEL_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models')\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    K_VALUES = [1, 3, 5, 10, 12]  # Different K values for evaluation\n",
    "    \n",
    "    # Ensemble weights (can be tuned)\n",
    "    ENSEMBLE_WEIGHTS = {\n",
    "        'lgb_classifier': 0.2,\n",
    "        'lgb_ranker_lambdarank': 0.3,\n",
    "        'lgb_ranker_xendcg': 0.2,\n",
    "        'neural_tower': 0.3\n",
    "    }\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "print(\"‚úì Evaluation Configuration loaded\")\n",
    "print(f\"  Model path: {EvaluationConfig.MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686896b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Evaluation metrics functions defined\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE EVALUATION METRICS\n",
    "\n",
    "def calculate_map_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Mean Average Precision at K (MAP@K)\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    aps = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate AP for this user\n",
    "        hits = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        for i, pred_item in enumerate(pred_items):\n",
    "            if pred_item in true_items:\n",
    "                hits += 1\n",
    "                precision_sum += hits / (i + 1)\n",
    "        \n",
    "        if hits > 0:\n",
    "            ap = precision_sum / len(true_items)\n",
    "            aps.append(ap)\n",
    "    \n",
    "    return np.mean(aps) if len(aps) > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_precision_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Precision@K\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    precisions = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(pred_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        hits = sum(1 for item in pred_items if item in true_items)\n",
    "        precision = hits / len(pred_items)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    return np.mean(precisions) if len(precisions) > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_recall_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Recall@K\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    recalls = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        hits = sum(1 for item in pred_items if item in true_items)\n",
    "        recall = hits / len(true_items)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return np.mean(recalls) if len(recalls) > 0 else 0.0\n",
    "\n",
    "\n",
    "def calculate_ndcg_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Normalized Discounted Cumulative Gain at K (NDCG@K)\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    ndcgs = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0.0\n",
    "        for i, pred_item in enumerate(pred_items):\n",
    "            if pred_item in true_items:\n",
    "                dcg += 1.0 / np.log2(i + 2)  # i+2 because log2(1) = 0\n",
    "        \n",
    "        # Calculate IDCG (ideal DCG)\n",
    "        idcg = 0.0\n",
    "        num_relevant = min(len(true_items), len(pred_items))\n",
    "        for i in range(num_relevant):\n",
    "            idcg += 1.0 / np.log2(i + 2)\n",
    "        \n",
    "        if idcg > 0:\n",
    "            ndcg = dcg / idcg\n",
    "            ndcgs.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcgs) if len(ndcgs) > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_all_metrics(df, predictions, k_values=[1, 3, 5, 10, 12]):\n",
    "    \"\"\"\n",
    "    Evaluate all metrics for different K values\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['customer_id', 'article_id', 'label']\n",
    "        predictions: Array of prediction scores\n",
    "        k_values: List of K values to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Group by customer\n",
    "    grouped = df.groupby('customer_id')\n",
    "    \n",
    "    # Prepare true and predicted items for each user\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for customer_id, group in grouped:\n",
    "        # True items (purchased articles)\n",
    "        true_items = set(group[group['label'] == 1]['article_id'].values)\n",
    "        y_true.append(true_items)\n",
    "        \n",
    "        # Predicted items (sorted by score)\n",
    "        customer_df = group.copy()\n",
    "        customer_df['pred_score'] = predictions[:len(customer_df)]\n",
    "        customer_df = customer_df.sort_values('pred_score', ascending=False)\n",
    "        pred_items = customer_df['article_id'].values.tolist()\n",
    "        y_pred.append(pred_items)\n",
    "        \n",
    "        # Remove used predictions\n",
    "        predictions = predictions[len(customer_df):]\n",
    "    \n",
    "    # Calculate metrics for each K\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        results[f'MAP@{k}'] = calculate_map_at_k(y_true, y_pred, k)\n",
    "        results[f'Precision@{k}'] = calculate_precision_at_k(y_true, y_pred, k)\n",
    "        results[f'Recall@{k}'] = calculate_recall_at_k(y_true, y_pred, k)\n",
    "        results[f'NDCG@{k}'] = calculate_ndcg_at_k(y_true, y_pred, k)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úì Evaluation metrics functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038ea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING MODEL PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Loading validation data...\n",
      "‚úì Loaded 120,970 validation samples\n",
      "\n",
      "Loading LightGBM predictions...\n",
      "‚úì Loaded ensemble predictions\n",
      "\n",
      "Loading Neural Tower predictions...\n",
      "‚úì Loaded Neural Tower predictions\n",
      "\n",
      "‚úì Total models loaded: 1\n",
      "  Models: ['neural_tower']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD ALL MODEL PREDICTIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING MODEL PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load validation data\n",
    "print(\"\\nLoading validation data...\")\n",
    "val_data = pd.read_parquet(EvaluationConfig.MODEL_PATH / 'val_data.parquet')\n",
    "print(f\"‚úì Loaded {len(val_data):,} validation samples\")\n",
    "\n",
    "# Load LightGBM predictions\n",
    "lgb_predictions = {}\n",
    "print(\"\\nLoading LightGBM predictions...\")\n",
    "try:\n",
    "    ensemble_preds = pd.read_parquet(EvaluationConfig.MODEL_PATH / 'ensemble_predictions_val.parquet')\n",
    "    if 'ensemble_weighted' in ensemble_preds.columns:\n",
    "        lgb_predictions['ensemble_weighted'] = ensemble_preds['ensemble_weighted'].values\n",
    "    if 'ensemble_average' in ensemble_preds.columns:\n",
    "        lgb_predictions['ensemble_average'] = ensemble_preds['ensemble_average'].values\n",
    "    print(f\"‚úì Loaded ensemble predictions\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load ensemble predictions: {e}\")\n",
    "\n",
    "# Load individual LightGBM model predictions if available\n",
    "lgb_models = ['lgb_classifier', 'lgb_ranker_lambdarank', 'lgb_ranker_xendcg', 'lgb_classifier_deep']\n",
    "for model_name in lgb_models:\n",
    "    try:\n",
    "        pred_file = EvaluationConfig.MODEL_PATH / f'{model_name}_predictions_val.parquet'\n",
    "        if pred_file.exists():\n",
    "            preds = pd.read_parquet(pred_file)\n",
    "            if 'pred_score' in preds.columns:\n",
    "                lgb_predictions[model_name] = preds['pred_score'].values\n",
    "                print(f\"‚úì Loaded {model_name} predictions\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not load {model_name}: {e}\")\n",
    "\n",
    "# Load Neural Tower predictions\n",
    "neural_predictions = {}\n",
    "print(\"\\nLoading Neural Tower predictions...\")\n",
    "try:\n",
    "    neural_preds = pd.read_parquet(EvaluationConfig.MODEL_PATH / 'neural_tower_predictions_val.parquet')\n",
    "    if 'pred_score' in neural_preds.columns:\n",
    "        neural_predictions['neural_tower'] = neural_preds['pred_score'].values\n",
    "        print(f\"‚úì Loaded Neural Tower predictions\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load Neural Tower predictions: {e}\")\n",
    "\n",
    "# Combine all predictions\n",
    "all_predictions = {**lgb_predictions, **neural_predictions}\n",
    "\n",
    "print(f\"\\n‚úì Total models loaded: {len(all_predictions)}\")\n",
    "print(f\"  Models: {list(all_predictions.keys())}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966495fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATING ALL MODELS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cb0f9c6c4d45569dc7aa4c8f235d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating models:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating neural_tower...\n",
      "  MAP@12: 0.776586\n",
      "  Precision@12: 0.332563\n",
      "  Recall@12: 1.022496\n",
      "  NDCG@12: 0.842860\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "                 MAP@1  Precision@1  Recall@1    NDCG@1     MAP@3  Precision@3  Recall@3  NDCG@3     MAP@5  Precision@5  Recall@5    NDCG@5   MAP@10  Precision@10  Recall@10   NDCG@10    MAP@12  Precision@12  Recall@12  NDCG@12\n",
      "neural_tower  0.634642     0.331862  0.385099  0.606797  0.693216     0.332012  0.835952  0.7677  0.748244     0.332417  0.977017  0.822857  0.77599      0.332564   1.021678  0.842445  0.776586      0.332563   1.022496  0.84286\n",
      "\n",
      "‚úì Saved comparison results to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/model_comparison.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EVALUATE ALL MODELS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store all evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, predictions in tqdm(all_predictions.items(), desc=\"Evaluating models\"):\n",
    "    print(f\"\\nüìä Evaluating {model_name}...\")\n",
    "    \n",
    "    # Ensure predictions match validation data length\n",
    "    if len(predictions) != len(val_data):\n",
    "        print(f\"‚ö†Ô∏è  Prediction length mismatch: {len(predictions)} vs {len(val_data)}\")\n",
    "        min_len = min(len(predictions), len(val_data))\n",
    "        predictions = predictions[:min_len]\n",
    "        val_data_eval = val_data.iloc[:min_len].copy()\n",
    "    else:\n",
    "        val_data_eval = val_data.copy()\n",
    "    \n",
    "    # Evaluate all metrics\n",
    "    metrics = evaluate_all_metrics(val_data_eval, predictions.copy(), k_values=EvaluationConfig.K_VALUES)\n",
    "    evaluation_results[model_name] = metrics\n",
    "    \n",
    "    # Print key metrics\n",
    "    print(f\"  MAP@12: {metrics['MAP@12']:.6f}\")\n",
    "    print(f\"  Precision@12: {metrics['Precision@12']:.6f}\")\n",
    "    print(f\"  Recall@12: {metrics['Recall@12']:.6f}\")\n",
    "    print(f\"  NDCG@12: {metrics['NDCG@12']:.6f}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(evaluation_results).T\n",
    "comparison_df = comparison_df.sort_values('MAP@12', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + comparison_df.to_string())\n",
    "\n",
    "# Save comparison results\n",
    "comparison_path = EvaluationConfig.MODEL_PATH / 'model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path)\n",
    "print(f\"\\n‚úì Saved comparison results to {comparison_path}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74361d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING FINAL ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "üì¶ Available models for ensemble: ['neural_tower']\n",
      "  neural_tower: [0.0000, 1.0000] -> [0, 1]\n",
      "\n",
      "‚öñÔ∏è  Ensemble weights: {'neural_tower': 1.0}\n",
      "\n",
      "üìä Evaluating final ensemble...\n",
      "  MAP@12: 0.776586\n",
      "  Precision@12: 0.332563\n",
      "  Recall@12: 1.022496\n",
      "  NDCG@12: 0.842860\n",
      "\n",
      "‚úì Saved ensemble predictions to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/final_ensemble_predictions_val.parquet\n",
      "\n",
      "================================================================================\n",
      "UPDATED MODEL COMPARISON (with ensemble)\n",
      "================================================================================\n",
      "\n",
      "                   MAP@1  Precision@1  Recall@1    NDCG@1     MAP@3  Precision@3  Recall@3  NDCG@3     MAP@5  Precision@5  Recall@5    NDCG@5   MAP@10  Precision@10  Recall@10   NDCG@10    MAP@12  Precision@12  Recall@12  NDCG@12\n",
      "neural_tower    0.634642     0.331862  0.385099  0.606797  0.693216     0.332012  0.835952  0.7677  0.748244     0.332417  0.977017  0.822857  0.77599      0.332564   1.021678  0.842445  0.776586      0.332563   1.022496  0.84286\n",
      "final_ensemble  0.634642     0.331862  0.385099  0.606797  0.693216     0.332012  0.835952  0.7677  0.748244     0.332417  0.977017  0.822857  0.77599      0.332564   1.021678  0.842445  0.776586      0.332563   1.022496  0.84286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE FINAL ENSEMBLE\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING FINAL ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select best models for ensemble\n",
    "best_models = ['neural_tower', 'lgb_ranker_lambdarank', 'lgb_classifier']\n",
    "\n",
    "# Filter to available models\n",
    "available_models = [m for m in best_models if m in all_predictions]\n",
    "print(f\"\\nüì¶ Available models for ensemble: {available_models}\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    print(\"‚ö†Ô∏è  No models available for ensemble!\")\n",
    "else:\n",
    "    # Normalize predictions to [0, 1] range\n",
    "    normalized_preds = {}\n",
    "    for model_name in available_models:\n",
    "        preds = all_predictions[model_name].copy()\n",
    "        min_pred = preds.min()\n",
    "        max_pred = preds.max()\n",
    "        if max_pred > min_pred:\n",
    "            normalized = (preds - min_pred) / (max_pred - min_pred)\n",
    "        else:\n",
    "            normalized = preds\n",
    "        normalized_preds[model_name] = normalized\n",
    "        print(f\"  {model_name}: [{preds.min():.4f}, {preds.max():.4f}] -> [0, 1]\")\n",
    "    \n",
    "    # Create ensemble with equal weights (can be tuned)\n",
    "    ensemble_weights = {m: 1.0 / len(available_models) for m in available_models}\n",
    "    print(f\"\\n‚öñÔ∏è  Ensemble weights: {ensemble_weights}\")\n",
    "    \n",
    "    # Calculate weighted ensemble\n",
    "    ensemble_pred = np.zeros(len(normalized_preds[available_models[0]]))\n",
    "    for model_name, weight in ensemble_weights.items():\n",
    "        ensemble_pred += weight * normalized_preds[model_name]\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    print(\"\\nüìä Evaluating final ensemble...\")\n",
    "    ensemble_metrics = evaluate_all_metrics(val_data, ensemble_pred.copy(), k_values=EvaluationConfig.K_VALUES)\n",
    "    evaluation_results['final_ensemble'] = ensemble_metrics\n",
    "    \n",
    "    print(f\"  MAP@12: {ensemble_metrics['MAP@12']:.6f}\")\n",
    "    print(f\"  Precision@12: {ensemble_metrics['Precision@12']:.6f}\")\n",
    "    print(f\"  Recall@12: {ensemble_metrics['Recall@12']:.6f}\")\n",
    "    print(f\"  NDCG@12: {ensemble_metrics['NDCG@12']:.6f}\")\n",
    "    \n",
    "    # Save ensemble predictions\n",
    "    ensemble_df = val_data[['customer_id', 'article_id', 'label']].copy()\n",
    "    ensemble_df['pred_score'] = ensemble_pred\n",
    "    ensemble_path = EvaluationConfig.MODEL_PATH / 'final_ensemble_predictions_val.parquet'\n",
    "    ensemble_df.to_parquet(ensemble_path, index=False)\n",
    "    print(f\"\\n‚úì Saved ensemble predictions to {ensemble_path}\")\n",
    "    \n",
    "    # Update comparison\n",
    "    comparison_df = pd.DataFrame(evaluation_results).T\n",
    "    comparison_df = comparison_df.sort_values('MAP@12', ascending=False)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UPDATED MODEL COMPARISON (with ensemble)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n\" + comparison_df.to_string())\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638b626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING FINAL RANKINGS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Using final ensemble for submission\n",
      "\n",
      "üìä Generating top-12 rankings per user...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ac2b70011b446b82325dea45d0243c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking users:   0%|          | 0/42126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Generated rankings for 42,126 users\n",
      "  Average articles per user: 2.87\n",
      "\n",
      "‚úì Saved submission file to /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/submission.csv\n",
      "\n",
      "üìÑ Sample submission (first 5 rows):\n",
      "                                                     customer_id                              prediction\n",
      "0000945f66de1a11d9447609b8b41b1bc987ba185a5496ae8831e8493afa24ff                               811899002\n",
      "00012315fd38859ff2c446876ca507abbcbcf582d0e266b1b696941c16e777a2                               872600009\n",
      "00061a04f030bdf3665b09829192ca8c13c4de6dd9ae9d38d0d0b5ce3a1cfc6f                     799365013 883724001\n",
      "00089f13f465ec902e5c49a3bb408c5e31205096d6f267543f1893303e456016                               858052005\n",
      "000e3f587242eb077685a487ad27dad632a4801576dfd16967280f0da3a78c2e 706016001 620425012 857713001 684209004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210688"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GENERATE FINAL RANKINGS FOR SUBMISSION\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING FINAL RANKINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use best model (or ensemble if available)\n",
    "if 'final_ensemble' in evaluation_results:\n",
    "    best_model_name = 'final_ensemble'\n",
    "    best_predictions = ensemble_pred\n",
    "    print(f\"\\n‚úÖ Using final ensemble for submission\")\n",
    "elif 'neural_tower' in all_predictions:\n",
    "    best_model_name = 'neural_tower'\n",
    "    best_predictions = all_predictions['neural_tower']\n",
    "    print(f\"\\n‚úÖ Using Neural Tower for submission\")\n",
    "elif len(all_predictions) > 0:\n",
    "    # Use model with best MAP@12\n",
    "    best_model_name = comparison_df.index[0]\n",
    "    best_predictions = all_predictions[best_model_name]\n",
    "    print(f\"\\n‚úÖ Using {best_model_name} for submission\")\n",
    "else:\n",
    "    raise ValueError(\"No predictions available!\")\n",
    "\n",
    "# Create predictions DataFrame\n",
    "pred_df = val_data[['customer_id', 'article_id']].copy()\n",
    "pred_df['pred_score'] = best_predictions[:len(pred_df)]\n",
    "\n",
    "# Generate top-12 predictions for each user\n",
    "print(\"\\nüìä Generating top-12 rankings per user...\")\n",
    "rankings = []\n",
    "for customer_id, group in tqdm(pred_df.groupby('customer_id'), desc=\"Ranking users\"):\n",
    "    # Sort by prediction score (descending)\n",
    "    group_sorted = group.sort_values('pred_score', ascending=False)\n",
    "    \n",
    "    # Get top 12 article IDs\n",
    "    top_articles = group_sorted.head(12)['article_id'].values\n",
    "    \n",
    "    # Format as space-separated string\n",
    "    predictions_str = ' '.join([str(art) for art in top_articles])\n",
    "    \n",
    "    rankings.append({\n",
    "        'customer_id': customer_id,\n",
    "        'prediction': predictions_str\n",
    "    })\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(rankings)\n",
    "submission_df = submission_df.sort_values('customer_id')\n",
    "\n",
    "print(f\"\\n‚úì Generated rankings for {len(submission_df):,} users\")\n",
    "print(f\"  Average articles per user: {submission_df['prediction'].str.split().str.len().mean():.2f}\")\n",
    "\n",
    "# Save submission file\n",
    "submission_path = EvaluationConfig.MODEL_PATH / 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úì Saved submission file to {submission_path}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìÑ Sample submission (first 5 rows):\")\n",
    "print(submission_df.head().to_string(index=False))\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f70db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üèÜ Best Model: neural_tower\n",
      "   MAP@12: 0.776586\n",
      "\n",
      "üìä Model Rankings (by MAP@12):\n",
      "--------------------------------------------------------------------------------\n",
      "ü•á 1. neural_tower                   MAP@12: 0.776586\n",
      "ü•à 2. final_ensemble                 MAP@12: 0.776586\n",
      "\n",
      "üìà Detailed Metrics for Best Model (neural_tower):\n",
      "--------------------------------------------------------------------------------\n",
      "  MAP@12              : 0.776586\n",
      "  Precision@12        : 0.332563\n",
      "  Recall@12           : 1.022496\n",
      "  NDCG@12             : 0.842860\n",
      "\n",
      "üíæ Generated Files:\n",
      "--------------------------------------------------------------------------------\n",
      "  Model Comparison: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/model_comparison.csv\n",
      "  Ensemble Predictions: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/final_ensemble_predictions_val.parquet\n",
      "  Submission File: /Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models/submission.csv\n",
      "\n",
      "üìä Performance Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "  Total Models Evaluated: 2\n",
      "  Best MAP@12: 0.776586\n",
      "  Improvement over baseline: 0.00%\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Step 3 Complete: Evaluation & Metrics\n",
      "   Ready for final submission!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL SUMMARY AND RESULTS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best model\n",
    "best_model = comparison_df.index[0]\n",
    "best_map12 = comparison_df.loc[best_model, 'MAP@12']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   MAP@12: {best_map12:.6f}\")\n",
    "\n",
    "# Model rankings\n",
    "print(f\"\\nüìä Model Rankings (by MAP@12):\")\n",
    "print(\"-\" * 80)\n",
    "for idx, (model_name, row) in enumerate(comparison_df.iterrows(), 1):\n",
    "    marker = \"ü•á\" if idx == 1 else \"ü•à\" if idx == 2 else \"ü•â\" if idx == 3 else \"  \"\n",
    "    print(f\"{marker} {idx}. {model_name:30s} MAP@12: {row['MAP@12']:.6f}\")\n",
    "\n",
    "# Key metrics for best model\n",
    "print(f\"\\nüìà Detailed Metrics for Best Model ({best_model}):\")\n",
    "print(\"-\" * 80)\n",
    "best_metrics = comparison_df.loc[best_model]\n",
    "for metric_name in ['MAP@12', 'Precision@12', 'Recall@12', 'NDCG@12']:\n",
    "    print(f\"  {metric_name:20s}: {best_metrics[metric_name]:.6f}\")\n",
    "\n",
    "# Files saved\n",
    "print(f\"\\nüíæ Generated Files:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Model Comparison: {EvaluationConfig.MODEL_PATH / 'model_comparison.csv'}\")\n",
    "if 'final_ensemble' in evaluation_results:\n",
    "    print(f\"  Ensemble Predictions: {EvaluationConfig.MODEL_PATH / 'final_ensemble_predictions_val.parquet'}\")\n",
    "print(f\"  Submission File: {EvaluationConfig.MODEL_PATH / 'submission.csv'}\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Total Models Evaluated: {len(comparison_df)}\")\n",
    "print(f\"  Best MAP@12: {best_map12:.6f}\")\n",
    "print(f\"  Improvement over baseline: {((best_map12 - comparison_df['MAP@12'].min()) / comparison_df['MAP@12'].min() * 100):.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Step 3 Complete: Evaluation & Metrics\")\n",
    "print(\"   Ready for final submission!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921f055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "443067c7",
   "metadata": {},
   "source": [
    "### Ablation Study: Impact of Image Features\n",
    "\n",
    "This ablation study evaluates the contribution of image features by training models **without** image embeddings and comparing performance with the full models.\n",
    "\n",
    "**Study Design:**\n",
    "- Train LightGBM Ranker without image features\n",
    "- Train Neural Tower without image features (2-tower: User + Item only)\n",
    "- Compare MAP@12, Precision@12, Recall@12, NDCG@12\n",
    "- Quantify the impact of image features on recommendation quality\n",
    "\n",
    "**Hypothesis:** Image features should improve recommendation quality, especially for visual fashion items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd310cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ABLATION STUDY: CONFIGURATION AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration (reuse from previous cells)\n",
    "ABLATION_CONFIG = {\n",
    "    'DATA_PATH': Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2'),\n",
    "    'MODEL_PATH': Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation_2/models'),\n",
    "    'RANDOM_STATE': 42,\n",
    "    'BATCH_SIZE': 2048,\n",
    "    'N_EPOCHS': 20,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'EARLY_STOPPING_PATIENCE': 5\n",
    "}\n",
    "\n",
    "# Device for neural network\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "    print(\"üöÄ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è  Using CPU\")\n",
    "\n",
    "print(\"‚úì Ablation study configuration loaded\")\n",
    "print(f\"  Model path: {ABLATION_CONFIG['MODEL_PATH']}\")\n",
    "print(f\"  Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA WITHOUT IMAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA FOR ABLATION STUDY (NO IMAGE FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load train and validation datasets\n",
    "print(\"\\nLoading train_data.parquet...\")\n",
    "train_data = pd.read_parquet(ABLATION_CONFIG['MODEL_PATH'] / 'train_data.parquet')\n",
    "print(f\"‚úì Loaded {len(train_data):,} training samples\")\n",
    "\n",
    "print(\"\\nLoading val_data.parquet...\")\n",
    "val_data = pd.read_parquet(ABLATION_CONFIG['MODEL_PATH'] / 'val_data.parquet')\n",
    "print(f\"‚úì Loaded {len(val_data):,} validation samples\")\n",
    "\n",
    "# Identify feature groups (EXCLUDE image features)\n",
    "exclude_cols = ['customer_id', 'article_id', 'label', 'user_type', 'train_label', 'val_label']\n",
    "all_feature_cols = [col for col in train_data.columns if col not in exclude_cols]\n",
    "\n",
    "# Separate features (NO IMAGE FEATURES)\n",
    "user_feature_cols = [col for col in all_feature_cols if any(col.startswith(prefix) for prefix in \n",
    "    ['n_', 'avg_', 'std_', 'min_', 'max_', 'days_', 'purchase_', 'exploration_', 'age', 'FN', 'Active', 'unique_'])]\n",
    "\n",
    "item_feature_cols = [col for col in all_feature_cols if any(col.startswith(prefix) for prefix in \n",
    "    ['product_', 'graphical_', 'colour_', 'perceived_', 'department_', 'index_', 'section_', \n",
    "     'garment_', 'popularity_', 'sales_', 'buyers_'])]\n",
    "\n",
    "# Explicitly EXCLUDE image features\n",
    "image_feature_cols = [col for col in all_feature_cols if col.startswith('image_emb_')]\n",
    "print(f\"\\n‚ö†Ô∏è  EXCLUDING {len(image_feature_cols)} image features from ablation study\")\n",
    "\n",
    "# Combined features (user + item, NO image)\n",
    "feature_cols = user_feature_cols + item_feature_cols\n",
    "\n",
    "print(f\"\\n‚úì Feature separation (NO IMAGE):\")\n",
    "print(f\"  User features: {len(user_feature_cols)}\")\n",
    "print(f\"  Item features: {len(item_feature_cols)}\")\n",
    "print(f\"  Image features (EXCLUDED): {len(image_feature_cols)}\")\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = train_data[feature_cols].copy()\n",
    "y_train = train_data['label'].copy()\n",
    "X_val = val_data[feature_cols].copy()\n",
    "y_val = val_data['label'].copy()\n",
    "\n",
    "# Handle categorical and missing values\n",
    "for col in feature_cols:\n",
    "    if X_train[col].dtype.name == 'category':\n",
    "        all_values = pd.concat([X_train[col], X_val[col]]).unique()\n",
    "        train_cat = pd.Categorical(X_train[col], categories=all_values)\n",
    "        val_cat = pd.Categorical(X_val[col], categories=all_values)\n",
    "        X_train[col] = train_cat.codes\n",
    "        X_val[col] = val_cat.codes\n",
    "        X_train[col] = X_train[col].replace(-1, 0)\n",
    "        X_val[col] = X_val[col].replace(-1, 0)\n",
    "    else:\n",
    "        X_train[col] = X_train[col].fillna(0)\n",
    "        X_val[col] = X_val[col].fillna(0)\n",
    "\n",
    "print(f\"\\n‚úì Feature matrices prepared:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Val: {X_val.shape}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea135718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAP@12 EVALUATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_map_at_k(y_true, y_pred, k=12):\n",
    "    \"\"\"Calculate Mean Average Precision at K (MAP@K)\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    y_pred = [pred[:k] for pred in y_pred]\n",
    "    \n",
    "    aps = []\n",
    "    for true_items, pred_items in zip(y_true, y_pred):\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        hits = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        for i, pred_item in enumerate(pred_items):\n",
    "            if pred_item in true_items:\n",
    "                hits += 1\n",
    "                precision_sum += hits / (i + 1)\n",
    "        \n",
    "        if hits > 0:\n",
    "            ap = precision_sum / len(true_items)\n",
    "            aps.append(ap)\n",
    "    \n",
    "    return np.mean(aps) if len(aps) > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_map_at_12(df, predictions):\n",
    "    \"\"\"Evaluate MAP@12 on validation set\"\"\"\n",
    "    grouped = df.groupby('customer_id')\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for customer_id, group in grouped:\n",
    "        true_items = set(group[group['label'] == 1]['article_id'].values)\n",
    "        y_true.append(true_items)\n",
    "        \n",
    "        customer_df = group.copy()\n",
    "        customer_df['pred_score'] = predictions[:len(customer_df)]\n",
    "        customer_df = customer_df.sort_values('pred_score', ascending=False)\n",
    "        pred_items = customer_df['article_id'].values.tolist()\n",
    "        y_pred.append(pred_items)\n",
    "        \n",
    "        predictions = predictions[len(customer_df):]\n",
    "    \n",
    "    return calculate_map_at_k(y_true, y_pred, k=12)\n",
    "\n",
    "\n",
    "print(\"‚úì MAP@12 evaluation function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e77130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN LIGHTGBM RANKER WITHOUT IMAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING LIGHTGBM RANKER (NO IMAGE FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for ranking (group by customer_id)\n",
    "train_customer_ids = train_data['customer_id'].values\n",
    "val_customer_ids = val_data['customer_id'].values\n",
    "\n",
    "# Sort data by customer_id for group information\n",
    "train_sort_idx = train_customer_ids.argsort()\n",
    "val_sort_idx = val_customer_ids.argsort()\n",
    "\n",
    "X_train_sorted = X_train.iloc[train_sort_idx].reset_index(drop=True)\n",
    "y_train_sorted = y_train.iloc[train_sort_idx].reset_index(drop=True)\n",
    "train_customer_ids_sorted = train_customer_ids[train_sort_idx]\n",
    "\n",
    "X_val_sorted = X_val.iloc[val_sort_idx].reset_index(drop=True)\n",
    "y_val_sorted = y_val.iloc[val_sort_idx].reset_index(drop=True)\n",
    "val_customer_ids_sorted = val_customer_ids[val_sort_idx]\n",
    "\n",
    "# Group information for ranking\n",
    "train_groups = pd.Series(train_customer_ids_sorted).value_counts().sort_index().values\n",
    "val_groups = pd.Series(val_customer_ids_sorted).value_counts().sort_index().values\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_dataset = lgb.Dataset(\n",
    "    X_train_sorted,\n",
    "    label=y_train_sorted,\n",
    "    group=train_groups,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "val_dataset = lgb.Dataset(\n",
    "    X_val_sorted,\n",
    "    label=y_val_sorted,\n",
    "    group=val_groups,\n",
    "    reference=train_dataset,\n",
    "    free_raw_data=False\n",
    ")\n",
    "\n",
    "# Model configuration (LambdaRank)\n",
    "params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'ndcg',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'seed': ABLATION_CONFIG['RANDOM_STATE'],\n",
    "    'force_col_wise': True,\n",
    "    'label_gain': [0, 1],  # 0 for negative, 1 for positive\n",
    "}\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining LightGBM Ranker...\")\n",
    "model_lgb_no_image = lgb.train(\n",
    "    params,\n",
    "    train_dataset,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[val_dataset],\n",
    "    valid_names=['val'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "preds_lgb_no_image = model_lgb_no_image.predict(X_val_sorted, num_iteration=model_lgb_no_image.best_iteration)\n",
    "\n",
    "# Evaluate\n",
    "map12_lgb_no_image = evaluate_map_at_12(val_data.iloc[val_sort_idx].reset_index(drop=True), preds_lgb_no_image.copy())\n",
    "\n",
    "print(f\"\\n‚úì LightGBM Ranker (NO IMAGE) Results:\")\n",
    "print(f\"  MAP@12: {map12_lgb_no_image:.6f}\")\n",
    "print(f\"  Best iteration: {model_lgb_no_image.best_iteration}\")\n",
    "\n",
    "# Save predictions\n",
    "pred_df_lgb = val_data.iloc[val_sort_idx].reset_index(drop=True)[['customer_id', 'article_id', 'label']].copy()\n",
    "pred_df_lgb['pred_score'] = preds_lgb_no_image\n",
    "pred_path_lgb = ABLATION_CONFIG['MODEL_PATH'] / 'lgb_ranker_no_image_predictions.parquet'\n",
    "pred_df_lgb.to_parquet(pred_path_lgb, index=False)\n",
    "print(f\"‚úì Saved predictions to {pred_path_lgb}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fa4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TWO-TOWER NEURAL NETWORK (USER + ITEM, NO IMAGE)\n",
    "# ============================================================================\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-tower neural network for recommendation (NO IMAGE):\n",
    "    - User Tower: User features -> User embedding\n",
    "    - Item Tower: Item features -> Item embedding\n",
    "    - Fusion: Concatenated embeddings -> Final prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 user_feature_dim,\n",
    "                 item_feature_dim,\n",
    "                 user_embedding_dim=128,\n",
    "                 item_embedding_dim=64,\n",
    "                 fusion_hidden_dims=[256, 128, 64],\n",
    "                 dropout_rate=0.3):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        # User Tower\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(user_feature_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, user_embedding_dim),\n",
    "            nn.BatchNorm1d(user_embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Item Tower\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(item_feature_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, item_embedding_dim),\n",
    "            nn.BatchNorm1d(item_embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fusion Layer\n",
    "        fusion_layers = []\n",
    "        input_dim = user_embedding_dim + item_embedding_dim\n",
    "        \n",
    "        for hidden_dim in fusion_hidden_dims:\n",
    "            fusion_layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        fusion_layers.append(nn.Linear(input_dim, 1))\n",
    "        fusion_layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.fusion = nn.Sequential(*fusion_layers)\n",
    "    \n",
    "    def forward(self, user_features, item_features):\n",
    "        # User embedding\n",
    "        user_emb = self.user_tower(user_features)\n",
    "        \n",
    "        # Item embedding\n",
    "        item_emb = self.item_tower(item_features)\n",
    "        \n",
    "        # Concatenate\n",
    "        fused = torch.cat([user_emb, item_emb], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.fusion(fused)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "print(\"‚úì TwoTowerModel class defined (NO IMAGE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS AND DATA PREPARATION FOR 2-TOWER MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class TwoTowerDataset(Dataset):\n",
    "    \"\"\"Dataset for 2-tower training (NO IMAGE)\"\"\"\n",
    "    \n",
    "    def __init__(self, df, user_features, item_features, labels=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.user_features = user_features.values.astype(np.float32)\n",
    "        self.item_features = item_features.values.astype(np.float32)\n",
    "        self.labels = labels.values.astype(np.float32) if labels is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user_feat = torch.FloatTensor(self.user_features[idx])\n",
    "        item_feat = torch.FloatTensor(self.item_features[idx])\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = torch.FloatTensor([self.labels[idx]])\n",
    "            return user_feat, item_feat, label\n",
    "        else:\n",
    "            return user_feat, item_feat\n",
    "\n",
    "# Prepare user and item features separately\n",
    "X_train_user = train_data[user_feature_cols].copy()\n",
    "X_train_item = train_data[item_feature_cols].copy()\n",
    "X_val_user = val_data[user_feature_cols].copy()\n",
    "X_val_item = val_data[item_feature_cols].copy()\n",
    "\n",
    "# Fill missing values and handle categorical\n",
    "for df in [X_train_user, X_train_item, X_val_user, X_val_item]:\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name == 'category':\n",
    "            df[col] = pd.Categorical(df[col]).codes\n",
    "            df[col] = df[col].replace(-1, 0)\n",
    "        else:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "# Standardize features\n",
    "scaler_user = StandardScaler()\n",
    "scaler_item = StandardScaler()\n",
    "\n",
    "X_train_user_scaled = pd.DataFrame(\n",
    "    scaler_user.fit_transform(X_train_user),\n",
    "    columns=user_feature_cols\n",
    ")\n",
    "X_val_user_scaled = pd.DataFrame(\n",
    "    scaler_user.transform(X_val_user),\n",
    "    columns=user_feature_cols\n",
    ")\n",
    "\n",
    "X_train_item_scaled = pd.DataFrame(\n",
    "    scaler_item.fit_transform(X_train_item),\n",
    "    columns=item_feature_cols\n",
    ")\n",
    "X_val_item_scaled = pd.DataFrame(\n",
    "    scaler_item.transform(X_val_item),\n",
    "    columns=item_feature_cols\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_2tower = TwoTowerDataset(\n",
    "    train_data,\n",
    "    X_train_user_scaled,\n",
    "    X_train_item_scaled,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "val_dataset_2tower = TwoTowerDataset(\n",
    "    val_data,\n",
    "    X_val_user_scaled,\n",
    "    X_val_item_scaled,\n",
    "    y_val\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_2tower = DataLoader(\n",
    "    train_dataset_2tower,\n",
    "    batch_size=ABLATION_CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader_2tower = DataLoader(\n",
    "    val_dataset_2tower,\n",
    "    batch_size=ABLATION_CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Data loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader_2tower)}\")\n",
    "print(f\"  Val batches: {len(val_loader_2tower)}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN 2-TOWER NEURAL NETWORK (NO IMAGE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING 2-TOWER NEURAL NETWORK (NO IMAGE FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model\n",
    "model_2tower = TwoTowerModel(\n",
    "    user_feature_dim=len(user_feature_cols),\n",
    "    item_feature_dim=len(item_feature_cols),\n",
    "    user_embedding_dim=128,\n",
    "    item_embedding_dim=64,\n",
    "    fusion_hidden_dims=[256, 128, 64],\n",
    "    dropout_rate=0.3\n",
    ").to(DEVICE)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model_2tower.parameters(),\n",
    "    lr=ABLATION_CONFIG['LEARNING_RATE'],\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',\n",
    "    factor=0.5,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history_2tower = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_map12': []\n",
    "}\n",
    "\n",
    "best_map12_2tower = 0.0\n",
    "best_epoch_2tower = 0\n",
    "patience_counter_2tower = 0\n",
    "\n",
    "val_customer_ids_list = val_data['customer_id'].values\n",
    "val_article_ids_list = val_data['article_id'].values\n",
    "\n",
    "for epoch in range(ABLATION_CONFIG['N_EPOCHS']):\n",
    "    # Training phase\n",
    "    model_2tower.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for user_feat, item_feat, labels in tqdm(train_loader_2tower, desc=f\"Epoch {epoch+1}/{ABLATION_CONFIG['N_EPOCHS']} [Train]\"):\n",
    "        user_feat = user_feat.to(DEVICE)\n",
    "        item_feat = item_feat.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).squeeze()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_2tower(user_feat, item_feat)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    # Validation phase\n",
    "    model_2tower.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (user_feat, item_feat, labels) in enumerate(tqdm(val_loader_2tower, desc=f\"Epoch {epoch+1}/{ABLATION_CONFIG['N_EPOCHS']} [Val]\")):\n",
    "            user_feat = user_feat.to(DEVICE)\n",
    "            item_feat = item_feat.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).squeeze()\n",
    "            \n",
    "            outputs = model_2tower(user_feat, item_feat)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "            \n",
    "            predictions = outputs.cpu().numpy().flatten()\n",
    "            labels_np = labels.cpu().numpy().flatten()\n",
    "            \n",
    "            all_predictions.extend(predictions.tolist())\n",
    "            all_labels.extend(labels_np.tolist())\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    \n",
    "    # Calculate MAP@12\n",
    "    val_eval_df = pd.DataFrame({\n",
    "        'customer_id': val_customer_ids_list[:len(all_predictions)],\n",
    "        'article_id': val_article_ids_list[:len(all_predictions)],\n",
    "        'label': all_labels[:len(all_predictions)],\n",
    "        'pred_score': all_predictions\n",
    "    })\n",
    "    \n",
    "    map12_score = evaluate_map_at_12(val_eval_df, np.array(all_predictions))\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(map12_score)\n",
    "    \n",
    "    # Store history\n",
    "    history_2tower['train_loss'].append(avg_train_loss)\n",
    "    history_2tower['val_loss'].append(avg_val_loss)\n",
    "    history_2tower['val_map12'].append(map12_score)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{ABLATION_CONFIG['N_EPOCHS']}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.6f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.6f}\")\n",
    "    print(f\"  Val MAP@12: {map12_score:.6f}\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if map12_score > best_map12_2tower:\n",
    "        best_map12_2tower = map12_score\n",
    "        best_epoch_2tower = epoch + 1\n",
    "        patience_counter_2tower = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_dir = ABLATION_CONFIG['MODEL_PATH'] / 'checkpoints'\n",
    "        checkpoint_dir.mkdir(exist_ok=True)\n",
    "        torch.save({\n",
    "            'model_state_dict': model_2tower.state_dict(),\n",
    "            'map12': map12_score,\n",
    "            'epoch': epoch + 1\n",
    "        }, checkpoint_dir / '2tower_no_image_best.pt')\n",
    "        \n",
    "        print(f\"  ‚úì Saved best model (MAP@12: {map12_score:.6f})\")\n",
    "    else:\n",
    "        patience_counter_2tower += 1\n",
    "        print(f\"  No improvement ({patience_counter_2tower}/{ABLATION_CONFIG['EARLY_STOPPING_PATIENCE']})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter_2tower >= ABLATION_CONFIG['EARLY_STOPPING_PATIENCE']:\n",
    "        print(f\"\\n‚ö†Ô∏è  Early stopping triggered after {epoch+1} epochs\")\n",
    "        print(f\"   Best MAP@12: {best_map12_2tower:.6f} at epoch {best_epoch_2tower}\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(checkpoint_dir / '2tower_no_image_best.pt', weights_only=False)\n",
    "model_2tower.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Final evaluation\n",
    "model_2tower.eval()\n",
    "all_predictions_final = []\n",
    "val_customer_ids_final = []\n",
    "val_article_ids_final = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for user_feat, item_feat, labels in val_loader_2tower:\n",
    "        user_feat = user_feat.to(DEVICE)\n",
    "        item_feat = item_feat.to(DEVICE)\n",
    "        \n",
    "        outputs = model_2tower(user_feat, item_feat)\n",
    "        predictions = outputs.cpu().numpy().flatten()\n",
    "        \n",
    "        all_predictions_final.extend(predictions.tolist())\n",
    "\n",
    "# Save predictions\n",
    "pred_df_2tower = val_data[['customer_id', 'article_id', 'label']].copy()\n",
    "pred_df_2tower['pred_score'] = all_predictions_final[:len(pred_df_2tower)]\n",
    "pred_path_2tower = ABLATION_CONFIG['MODEL_PATH'] / '2tower_no_image_predictions.parquet'\n",
    "pred_df_2tower.to_parquet(pred_path_2tower, index=False)\n",
    "\n",
    "print(f\"\\n‚úì 2-Tower Model (NO IMAGE) Results:\")\n",
    "print(f\"  MAP@12: {best_map12_2tower:.6f}\")\n",
    "print(f\"  Best epoch: {best_epoch_2tower}\")\n",
    "print(f\"‚úì Saved predictions to {pred_path_2tower}\")\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPARE RESULTS: WITH vs WITHOUT IMAGE FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION STUDY: COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load full model results (if available)\n",
    "print(\"\\nüìä Loading full model results...\")\n",
    "full_model_results = {}\n",
    "\n",
    "# Try to load LightGBM with image features\n",
    "try:\n",
    "    lgb_full_preds = pd.read_parquet(ABLATION_CONFIG['MODEL_PATH'] / 'lgb_ranker_lambdarank_predictions_val.parquet')\n",
    "    if 'pred_score' in lgb_full_preds.columns:\n",
    "        map12_lgb_full = evaluate_map_at_12(lgb_full_preds, lgb_full_preds['pred_score'].values)\n",
    "        full_model_results['LightGBM_Ranker (WITH Image)'] = map12_lgb_full\n",
    "        print(f\"‚úì Loaded LightGBM Ranker (WITH Image): MAP@12 = {map12_lgb_full:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load LightGBM full model: {e}\")\n",
    "\n",
    "# Try to load Neural Tower with image features\n",
    "try:\n",
    "    neural_full_preds = pd.read_parquet(ABLATION_CONFIG['MODEL_PATH'] / 'neural_tower_predictions_val.parquet')\n",
    "    if 'pred_score' in neural_full_preds.columns:\n",
    "        map12_neural_full = evaluate_map_at_12(neural_full_preds, neural_full_preds['pred_score'].values)\n",
    "        full_model_results['Neural_Tower_3Tower (WITH Image)'] = map12_neural_full\n",
    "        print(f\"‚úì Loaded Neural Tower 3-Tower (WITH Image): MAP@12 = {map12_neural_full:.6f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load Neural Tower full model: {e}\")\n",
    "\n",
    "# Ablation study results\n",
    "ablation_results = {\n",
    "    'LightGBM_Ranker (NO Image)': map12_lgb_no_image,\n",
    "    'Neural_Tower_2Tower (NO Image)': best_map12_2tower\n",
    "}\n",
    "\n",
    "# Combine all results\n",
    "all_results = {**full_model_results, **ablation_results}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_results = []\n",
    "for model_name, map12_score in all_results.items():\n",
    "    comparison_results.append({\n",
    "        'Model': model_name,\n",
    "        'MAP@12': map12_score,\n",
    "        'Has_Image_Features': 'WITH Image' in model_name\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "comparison_df = comparison_df.sort_values('MAP@12', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: WITH vs WITHOUT IMAGE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate impact of image features\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPACT ANALYSIS: IMAGE FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# LightGBM comparison\n",
    "if 'LightGBM_Ranker (WITH Image)' in full_model_results and 'LightGBM_Ranker (NO Image)' in ablation_results:\n",
    "    lgb_with = full_model_results['LightGBM_Ranker (WITH Image)']\n",
    "    lgb_without = ablation_results['LightGBM_Ranker (NO Image)']\n",
    "    lgb_improvement = ((lgb_with - lgb_without) / lgb_without) * 100\n",
    "    print(f\"\\nüìä LightGBM Ranker:\")\n",
    "    print(f\"  WITH Image Features:  {lgb_with:.6f}\")\n",
    "    print(f\"  WITHOUT Image Features: {lgb_without:.6f}\")\n",
    "    print(f\"  Improvement: {lgb_improvement:+.2f}%\")\n",
    "    print(f\"  Absolute Gain: {lgb_with - lgb_without:+.6f}\")\n",
    "\n",
    "# Neural Tower comparison\n",
    "if 'Neural_Tower_3Tower (WITH Image)' in full_model_results and 'Neural_Tower_2Tower (NO Image)' in ablation_results:\n",
    "    neural_with = full_model_results['Neural_Tower_3Tower (WITH Image)']\n",
    "    neural_without = ablation_results['Neural_Tower_2Tower (NO Image)']\n",
    "    neural_improvement = ((neural_with - neural_without) / neural_without) * 100\n",
    "    print(f\"\\nüìä Neural Tower:\")\n",
    "    print(f\"  3-Tower (WITH Image):  {neural_with:.6f}\")\n",
    "    print(f\"  2-Tower (NO Image):    {neural_without:.6f}\")\n",
    "    print(f\"  Improvement: {neural_improvement:+.2f}%\")\n",
    "    print(f\"  Absolute Gain: {neural_with - neural_without:+.6f}\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_path = ABLATION_CONFIG['MODEL_PATH'] / 'ablation_study_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f\"\\n‚úì Saved comparison results to {comparison_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION STUDY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚úÖ Ablation study complete!\")\n",
    "print(f\"   Total models compared: {len(all_results)}\")\n",
    "print(f\"   Best model: {comparison_df.iloc[0]['Model']} (MAP@12: {comparison_df.iloc[0]['MAP@12']:.6f})\")\n",
    "\n",
    "if len(full_model_results) > 0:\n",
    "    print(f\"\\nüí° Key Finding:\")\n",
    "    avg_improvement = 0.0\n",
    "    count = 0\n",
    "    if 'LightGBM_Ranker (WITH Image)' in full_model_results:\n",
    "        avg_improvement += lgb_improvement\n",
    "        count += 1\n",
    "    if 'Neural_Tower_3Tower (WITH Image)' in full_model_results:\n",
    "        avg_improvement += neural_improvement\n",
    "        count += 1\n",
    "    if count > 0:\n",
    "        avg_improvement /= count\n",
    "        print(f\"   Average improvement with image features: {avg_improvement:+.2f}%\")\n",
    "        if avg_improvement > 0:\n",
    "            print(f\"   ‚úì Image features provide significant value!\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Image features may not be critical for this dataset\")\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "assignment2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
