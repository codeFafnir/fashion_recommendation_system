{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01ee4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable tqdm for pandas operations\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c71f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths (adjust based on your Kaggle dataset location)\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/h-and-m-personalized-fashion-recommendations')\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender')\n",
    "    \n",
    "    # Temporal configuration\n",
    "    N_TRAIN_WEEKS = 12  # Number of weeks for training\n",
    "    N_VAL_WEEKS = 1     # Validation week\n",
    "    \n",
    "    # Sampling configuration\n",
    "    MIN_ITEM_PURCHASES = 2  # Minimum purchases for an item to be included\n",
    "    USER_SAMPLE_RATE = None  # None = use active user strategy (recommended)\n",
    "    \n",
    "    # Memory optimization\n",
    "    CHUNK_SIZE = 500000  # Process transactions in chunks\n",
    "    \n",
    "    # Random seed\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()\n",
    "config.OUTPUT_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bbb9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "Loading transactions...\n",
      "âœ“ Loaded 31,788,324 transactions\n",
      "  Date range: 2018-09-20 00:00:00 to 2020-09-22 00:00:00\n",
      "  Unique customers: 1,362,281\n",
      "  Unique articles: 104,547\n",
      "\n",
      "Loading customers...\n",
      "âœ“ Loaded 1,371,980 customers\n",
      "\n",
      "Loading articles...\n",
      "âœ“ Loaded 105,542 articles\n",
      "\n",
      "================================================================================\n",
      "  STEP 2: SELECTING TEMPORAL WINDOW\n",
      "================================================================================\n",
      "Last transaction date: 2020-09-22 00:00:00\n",
      "\n",
      "Temporal splits:\n",
      "  Training:   2020-06-22 to 2020-09-14 (12 weeks)\n",
      "  Validation: 2020-09-15 to 2020-09-22 (1 week)\n",
      "\n",
      "Filtering transactions from 2020-06-23 onwards...\n",
      "âœ“ Retained 3,981,458 transactions (3.98M)\n",
      "  Week range: 0 to 13\n",
      "\n",
      "Dataset split:\n",
      "  Training transactions: 3,715,094\n",
      "  Validation transactions: 266,364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduce memory usage of a dataframe by optimizing dtypes\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in tqdm(df.columns, desc=\"Optimizing dtypes\", disable=not verbose):\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Skip datetime columns\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            continue\n",
    "        \n",
    "        # Skip object/string columns (convert to category instead)\n",
    "        if col_type == object:\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if num_unique_values / num_total_values < 0.5:  # If less than 50% unique, convert to category\n",
    "                df[col] = df[col].astype('category')\n",
    "            continue\n",
    "        \n",
    "        # Optimize numeric columns\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Integer columns\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            # Float columns\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Memory usage decreased from {start_mem:.2f} MB to {end_mem:.2f} MB '\n",
    "              f'({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"Pretty print section headers\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD AND EXPLORE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 1: LOADING DATA\")\n",
    "\n",
    "# Load transactions\n",
    "print(\"Loading transactions...\")\n",
    "transactions = pd.read_csv(\n",
    "    config.DATA_PATH / 'transactions_train.csv',\n",
    "    dtype={\n",
    "        'article_id': 'int32',\n",
    "        'price': 'float32',\n",
    "        'sales_channel_id': 'int8'\n",
    "    },\n",
    "    parse_dates=['t_dat']\n",
    ")\n",
    "print(f\"âœ“ Loaded {len(transactions):,} transactions\")\n",
    "print(f\"  Date range: {transactions['t_dat'].min()} to {transactions['t_dat'].max()}\")\n",
    "print(f\"  Unique customers: {transactions['customer_id'].nunique():,}\")\n",
    "print(f\"  Unique articles: {transactions['article_id'].nunique():,}\")\n",
    "\n",
    "# Load customers\n",
    "print(\"\\nLoading customers...\")\n",
    "customers = pd.read_csv(\n",
    "    config.DATA_PATH / 'customers.csv',\n",
    "    dtype={\n",
    "        'FN': 'float32',\n",
    "        'Active': 'float32',\n",
    "        'age': 'float32'\n",
    "    }\n",
    ")\n",
    "customers = reduce_mem_usage(customers, verbose=False)\n",
    "print(f\"âœ“ Loaded {len(customers):,} customers\")\n",
    "\n",
    "# Load articles\n",
    "print(\"\\nLoading articles...\")\n",
    "articles = pd.read_csv(\n",
    "    config.DATA_PATH / 'articles.csv',\n",
    "    dtype={'article_id': 'int32'}\n",
    ")\n",
    "articles = reduce_mem_usage(articles, verbose=False)\n",
    "print(f\"âœ“ Loaded {len(articles):,} articles\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: TEMPORAL WINDOW SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 2: SELECTING TEMPORAL WINDOW\")\n",
    "\n",
    "# Get the last date in transactions\n",
    "max_date = transactions['t_dat'].max()\n",
    "print(f\"Last transaction date: {max_date}\")\n",
    "\n",
    "# Calculate cutoff dates\n",
    "val_end_date = max_date\n",
    "val_start_date = val_end_date - timedelta(weeks=config.N_VAL_WEEKS)\n",
    "train_end_date = val_start_date - timedelta(days=1)\n",
    "train_start_date = train_end_date - timedelta(weeks=config.N_TRAIN_WEEKS)\n",
    "\n",
    "print(f\"\\nTemporal splits:\")\n",
    "print(f\"  Training:   {train_start_date.date()} to {train_end_date.date()} ({config.N_TRAIN_WEEKS} weeks)\")\n",
    "print(f\"  Validation: {val_start_date.date()} to {val_end_date.date()} ({config.N_VAL_WEEKS} week)\")\n",
    "\n",
    "# Filter transactions to our window\n",
    "total_weeks = config.N_TRAIN_WEEKS + config.N_VAL_WEEKS\n",
    "window_start = max_date - timedelta(weeks=total_weeks)\n",
    "\n",
    "print(f\"\\nFiltering transactions from {window_start.date()} onwards...\")\n",
    "transactions = transactions[transactions['t_dat'] >= window_start].copy()\n",
    "print(f\"âœ“ Retained {len(transactions):,} transactions ({len(transactions)/1e6:.2f}M)\")\n",
    "\n",
    "# Add week number (relative to window start)\n",
    "transactions['week'] = ((transactions['t_dat'] - window_start).dt.days // 7).astype(np.int8)\n",
    "print(f\"  Week range: {transactions['week'].min()} to {transactions['week'].max()}\")\n",
    "\n",
    "# Split into train and validation\n",
    "train_transactions = transactions[transactions['t_dat'] <= train_end_date].copy()\n",
    "val_transactions = transactions[transactions['t_dat'] > train_end_date].copy()\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training transactions: {len(train_transactions):,}\")\n",
    "print(f\"  Validation transactions: {len(val_transactions):,}\")\n",
    "\n",
    "del transactions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb329f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 3: SMART USER SAMPLING\n",
      "================================================================================\n",
      "Users in validation week: 75,481\n",
      "Active users in last 3 weeks of training: 200,756\n",
      "\n",
      "Total selected users: 245,945\n",
      "  Reduction: 82.1%\n",
      "\n",
      "After user filtering:\n",
      "  Training transactions: 2,129,323\n",
      "  Validation transactions: 266,364\n",
      "  Customers retained: 245,945\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: SMART USER SAMPLING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 3: SMART USER SAMPLING\")\n",
    "\n",
    "# Strategy: Keep ALL users who purchased in validation week + active users in train\n",
    "\n",
    "# Get users who purchased in validation week (these are who we need to predict for)\n",
    "val_users = set(val_transactions['customer_id'].unique())\n",
    "print(f\"Users in validation week: {len(val_users):,}\")\n",
    "\n",
    "# Get active users in last 3 weeks of training (recent activity indicates likely future purchase)\n",
    "last_3_weeks_date = train_end_date - timedelta(weeks=3)\n",
    "active_train_users = set(\n",
    "    train_transactions[train_transactions['t_dat'] >= last_3_weeks_date]['customer_id'].unique()\n",
    ")\n",
    "print(f\"Active users in last 3 weeks of training: {len(active_train_users):,}\")\n",
    "\n",
    "# Combine: validation users + active training users\n",
    "selected_users = val_users.union(active_train_users)\n",
    "print(f\"\\nTotal selected users: {len(selected_users):,}\")\n",
    "print(f\"  Reduction: {100 * (1 - len(selected_users)/customers['customer_id'].nunique()):.1f}%\")\n",
    "\n",
    "# Filter transactions to selected users\n",
    "train_transactions = train_transactions[train_transactions['customer_id'].isin(selected_users)].copy()\n",
    "val_transactions = val_transactions[val_transactions['customer_id'].isin(selected_users)].copy()\n",
    "\n",
    "print(f\"\\nAfter user filtering:\")\n",
    "print(f\"  Training transactions: {len(train_transactions):,}\")\n",
    "print(f\"  Validation transactions: {len(val_transactions):,}\")\n",
    "\n",
    "# Filter customers table\n",
    "customers = customers[customers['customer_id'].isin(selected_users)].copy()\n",
    "print(f\"  Customers retained: {len(customers):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f82741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 4: ITEM FILTERING\n",
      "================================================================================\n",
      "Unique items in training: 37,470\n",
      "Items with >= 2 purchases: 30,967\n",
      "Items in validation: 18,684\n",
      "\n",
      "Total selected items: 32,714\n",
      "  Reduction: 69.0%\n",
      "\n",
      "After item filtering:\n",
      "  Training transactions: 2,123,394\n",
      "  Validation transactions: 266,364\n",
      "  Articles retained: 32,714\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: ITEM FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 4: ITEM FILTERING\")\n",
    "\n",
    "# Count purchases per item in training window\n",
    "item_counts = train_transactions['article_id'].value_counts()\n",
    "print(f\"Unique items in training: {len(item_counts):,}\")\n",
    "\n",
    "# Keep items with minimum purchases\n",
    "valid_items = set(item_counts[item_counts >= config.MIN_ITEM_PURCHASES].index)\n",
    "print(f\"Items with >= {config.MIN_ITEM_PURCHASES} purchases: {len(valid_items):,}\")\n",
    "\n",
    "# Also include all items from validation (even if rare in training)\n",
    "val_items = set(val_transactions['article_id'].unique())\n",
    "print(f\"Items in validation: {len(val_items):,}\")\n",
    "\n",
    "# Combine\n",
    "selected_items = valid_items.union(val_items)\n",
    "print(f\"\\nTotal selected items: {len(selected_items):,}\")\n",
    "print(f\"  Reduction: {100 * (1 - len(selected_items)/articles['article_id'].nunique()):.1f}%\")\n",
    "\n",
    "# Filter transactions\n",
    "train_transactions = train_transactions[train_transactions['article_id'].isin(selected_items)].copy()\n",
    "val_transactions = val_transactions[val_transactions['article_id'].isin(selected_items)].copy()\n",
    "\n",
    "print(f\"\\nAfter item filtering:\")\n",
    "print(f\"  Training transactions: {len(train_transactions):,}\")\n",
    "print(f\"  Validation transactions: {len(val_transactions):,}\")\n",
    "\n",
    "# Filter articles table\n",
    "articles = articles[articles['article_id'].isin(selected_items)].copy()\n",
    "print(f\"  Articles retained: {len(articles):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbe7a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 5: MEMORY OPTIMIZATION\n",
      "================================================================================\n",
      "Before optimization:\n",
      "  train_transactions: 297.68 MB\n",
      "  val_transactions: 37.34 MB\n",
      "  customers: 83.26 MB\n",
      "  articles: 16.11 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ae9ca5e16a4668a3ce529c57f8d12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimizing dtypes:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage decreased from 68.85 MB to 70.52 MB (-2.4% reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23aacfd5ea1345e588d3435089778c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimizing dtypes:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage decreased from 8.64 MB to 10.21 MB (-18.2% reduction)\n",
      "\n",
      "Converting article categorical columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153001c669284c419425fe26c3d4e997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Article categories:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting customer categorical columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117cb29906a94e9b8c3678019c89e1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Customer categories:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After optimization:\n",
      "  train_transactions: 94.70 MB\n",
      "  val_transactions: 18.35 MB\n",
      "  customers: 83.26 MB\n",
      "  articles: 16.34 MB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: DATA TYPE OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 5: MEMORY OPTIMIZATION\")\n",
    "\n",
    "print(\"Before optimization:\")\n",
    "print(f\"  train_transactions: {train_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  val_transactions: {val_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  customers: {customers.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  articles: {articles.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Optimize transactions\n",
    "train_transactions = reduce_mem_usage(train_transactions)\n",
    "val_transactions = reduce_mem_usage(val_transactions)\n",
    "\n",
    "# Convert categorical columns\n",
    "print(\"\\nConverting article categorical columns...\")\n",
    "for col in tqdm(['product_code', 'product_type_no', 'graphical_appearance_no', \n",
    "                 'colour_group_code', 'perceived_colour_value_id', 'perceived_colour_master_id',\n",
    "                 'department_no', 'index_code', 'index_group_no', 'section_no', 'garment_group_no'],\n",
    "                desc=\"Article categories\"):\n",
    "    if col in articles.columns:\n",
    "        articles[col] = articles[col].astype('category')\n",
    "\n",
    "# Optimize customer categoricals\n",
    "print(\"\\nConverting customer categorical columns...\")\n",
    "for col in tqdm(['club_member_status', 'fashion_news_frequency', 'postal_code'],\n",
    "                desc=\"Customer categories\"):\n",
    "    if col in customers.columns:\n",
    "        customers[col] = customers[col].astype('category')\n",
    "\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(f\"  train_transactions: {train_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  val_transactions: {val_transactions.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  customers: {customers.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  articles: {articles.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cea08662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 6: DATA VALIDATION & EDA\n",
      "================================================================================\n",
      "Data validation:\n",
      "  âœ“ No null customer_ids in train: True\n",
      "  âœ“ No null article_ids in train: True\n",
      "  âœ“ All validation users in training: False\n",
      "\n",
      "Transaction statistics:\n",
      "  Training period: 2020-06-23 to 2020-09-14\n",
      "  Validation period: 2020-09-15 to 2020-09-22\n",
      "  Avg transactions per user (train): 8.63\n",
      "  Avg transactions per item (train): 64.91\n",
      "\n",
      "Purchase distribution in validation week:\n",
      "  Mean purchases per user: 3.53\n",
      "  Median purchases per user: 2\n",
      "  Users with 1 purchase: 21,600 (28.6%)\n",
      "  Users with 2-5 purchases: 40,847\n",
      "  Users with 6+ purchases: 13,034\n",
      "\n",
      "Weekly transaction counts:\n",
      "  Week 0: 225,490 transactions\n",
      "  Week 1: 151,983 transactions\n",
      "  Week 2: 129,666 transactions\n",
      "  Week 3: 125,384 transactions\n",
      "  Week 4: 131,386 transactions\n",
      "  Week 5: 139,736 transactions\n",
      "  Week 6: 135,173 transactions\n",
      "  Week 7: 125,580 transactions\n",
      "  Week 8: 141,604 transactions\n",
      "  Week 9: 286,430 transactions\n",
      "  Week 10: 265,769 transactions\n",
      "  Week 11: 265,193 transactions\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: BASIC EDA AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 6: DATA VALIDATION & EDA\")\n",
    "\n",
    "# Validation checks\n",
    "print(\"Data validation:\")\n",
    "print(f\"  âœ“ No null customer_ids in train: {train_transactions['customer_id'].isnull().sum() == 0}\")\n",
    "print(f\"  âœ“ No null article_ids in train: {train_transactions['article_id'].isnull().sum() == 0}\")\n",
    "print(f\"  âœ“ All validation users in training: {val_users.issubset(set(train_transactions['customer_id']))}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nTransaction statistics:\")\n",
    "print(f\"  Training period: {train_transactions['t_dat'].min().date()} to {train_transactions['t_dat'].max().date()}\")\n",
    "print(f\"  Validation period: {val_transactions['t_dat'].min().date()} to {val_transactions['t_dat'].max().date()}\")\n",
    "print(f\"  Avg transactions per user (train): {len(train_transactions) / len(selected_users):.2f}\")\n",
    "print(f\"  Avg transactions per item (train): {len(train_transactions) / len(selected_items):.2f}\")\n",
    "\n",
    "# Purchase distribution\n",
    "print(\"\\nPurchase distribution in validation week:\")\n",
    "val_user_purchases = val_transactions.groupby('customer_id').size()\n",
    "print(f\"  Mean purchases per user: {val_user_purchases.mean():.2f}\")\n",
    "print(f\"  Median purchases per user: {val_user_purchases.median():.0f}\")\n",
    "print(f\"  Users with 1 purchase: {(val_user_purchases == 1).sum():,} ({100*(val_user_purchases == 1).sum()/len(val_user_purchases):.1f}%)\")\n",
    "print(f\"  Users with 2-5 purchases: {((val_user_purchases >= 2) & (val_user_purchases <= 5)).sum():,}\")\n",
    "print(f\"  Users with 6+ purchases: {(val_user_purchases >= 6).sum():,}\")\n",
    "\n",
    "# Weekly transaction trend\n",
    "weekly_counts = train_transactions.groupby('week').size()\n",
    "print(\"\\nWeekly transaction counts:\")\n",
    "for week, count in weekly_counts.items():\n",
    "    print(f\"  Week {week}: {count:,} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a661e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 7: CREATING VALIDATION GROUND TRUTH\n",
      "================================================================================\n",
      "Validation ground truth:\n",
      "  Users: 75,481\n",
      "  Total purchases: 266,364\n",
      "  Avg purchases per user: 3.53\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: CREATE VALIDATION GROUND TRUTH\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 7: CREATING VALIDATION GROUND TRUTH\")\n",
    "\n",
    "# Create validation ground truth (actual purchases in validation week)\n",
    "val_ground_truth = (\n",
    "    val_transactions\n",
    "    .groupby('customer_id')['article_id']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .rename(columns={'article_id': 'purchased_articles'})\n",
    ")\n",
    "\n",
    "print(f\"Validation ground truth:\")\n",
    "print(f\"  Users: {len(val_ground_truth):,}\")\n",
    "print(f\"  Total purchases: {val_ground_truth['purchased_articles'].apply(len).sum():,}\")\n",
    "print(f\"  Avg purchases per user: {val_ground_truth['purchased_articles'].apply(len).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b79d2472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STEP 8: SAVING PROCESSED DATA\n",
      "================================================================================\n",
      "Saving files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e169cfd26e4924b685163c3afac5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ train_transactions.parquet (2,123,394 rows)\n",
      "  âœ“ val_transactions.parquet (266,364 rows)\n",
      "  âœ“ customers.parquet (245,945 rows)\n",
      "  âœ“ articles.parquet (32,714 rows)\n",
      "  âœ“ val_ground_truth.parquet (75,481 rows)\n",
      "  âœ“ metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: SAVE PROCESSED DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STEP 8: SAVING PROCESSED DATA\")\n",
    "\n",
    "# Save to parquet (much more efficient than CSV)\n",
    "print(\"Saving files...\")\n",
    "\n",
    "files_to_save = [\n",
    "    (train_transactions, 'train_transactions.parquet'),\n",
    "    (val_transactions, 'val_transactions.parquet'),\n",
    "    (customers, 'customers.parquet'),\n",
    "    (articles, 'articles.parquet'),\n",
    "    (val_ground_truth, 'val_ground_truth.parquet')\n",
    "]\n",
    "\n",
    "for df, filename in tqdm(files_to_save, desc=\"Saving files\"):\n",
    "    df.to_parquet(config.OUTPUT_PATH / filename, index=False)\n",
    "    print(f\"  âœ“ {filename} ({len(df):,} rows)\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'train_start_date': str(train_start_date.date()),\n",
    "    'train_end_date': str(train_end_date.date()),\n",
    "    'val_start_date': str(val_start_date.date()),\n",
    "    'val_end_date': str(val_end_date.date()),\n",
    "    'n_users': len(selected_users),\n",
    "    'n_items': len(selected_items),\n",
    "    'n_train_transactions': len(train_transactions),\n",
    "    'n_val_transactions': len(val_transactions),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(config.OUTPUT_PATH / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"  âœ“ metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368bc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  PREPROCESSING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Final dataset summary:\n",
      "  ðŸ“… Training weeks: 12\n",
      "  ðŸ“… Validation weeks: 1\n",
      "  ðŸ‘¥ Users: 245,945\n",
      "  ðŸ›ï¸  Items: 32,714\n",
      "  ðŸ“Š Train transactions: 2,123,394\n",
      "  ðŸ“Š Val transactions: 266,364\n",
      "  ðŸ’¾ Total disk space: 83.13 MB\n",
      "\n",
      "âœ… Ready for Stage 2: Recall Strategies!\n",
      "\n",
      "Next steps:\n",
      "  1. Review the saved files in /kaggle/working/\n",
      "  2. Check metadata.json for dataset info\n",
      "  3. Proceed to Stage 2 when ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"PREPROCESSING COMPLETE!\")\n",
    "\n",
    "print(\"\\nFinal dataset summary:\")\n",
    "print(f\"  ðŸ“… Training weeks: {config.N_TRAIN_WEEKS}\")\n",
    "print(f\"  ðŸ“… Validation weeks: {config.N_VAL_WEEKS}\")\n",
    "print(f\"  ðŸ‘¥ Users: {len(selected_users):,}\")\n",
    "print(f\"  ðŸ›ï¸  Items: {len(selected_items):,}\")\n",
    "print(f\"  ðŸ“Š Train transactions: {len(train_transactions):,}\")\n",
    "print(f\"  ðŸ“Š Val transactions: {len(val_transactions):,}\")\n",
    "print(f\"  ðŸ’¾ Total disk space: {sum((config.OUTPUT_PATH / f).stat().st_size for f in ['train_transactions.parquet', 'val_transactions.parquet', 'customers.parquet', 'articles.parquet', 'val_ground_truth.parquet']) / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nâœ… Ready for Stage 2: Recall Strategies!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review the saved files in /kaggle/working/\")\n",
    "print(\"  2. Check metadata.json for dataset info\")\n",
    "print(\"  3. Proceed to Stage 2 when ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20899a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0ec03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in GB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024**3\n",
    "\n",
    "def print_memory():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    mem = get_memory_usage()\n",
    "    print(f\"  ðŸ’¾ Memory: {mem:.2f} GB\")\n",
    "\n",
    "def force_garbage_collection():\n",
    "    \"\"\"Aggressive garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c05a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender')\n",
    "    OUTPUT_PATH = Path('/Users/raghu/Desktop/Quarter_1/CSE_258R/assignment2/fashion_recommender_candidate_generation')\n",
    "    \n",
    "    # Recall configuration - REDUCED for memory\n",
    "    N_REPURCHASE_CANDIDATES = 25  # Reduced from 30\n",
    "    N_POPULARITY_CANDIDATES = 25  # Reduced from 30\n",
    "    N_COPURCHASE_CANDIDATES = 15  # Reduced from 20\n",
    "    N_USERKNN_CANDIDATES = 15     # Reduced from 20\n",
    "    N_CATEGORY_CANDIDATES = 15    # Reduced from 20\n",
    "    \n",
    "    # Processing parameters\n",
    "    USER_CHUNK_SIZE = 1000  # Process users in chunks\n",
    "    BASKET_CHUNK_SIZE = 5000  # Process baskets in chunks\n",
    "    \n",
    "    # EMERGENCY MODE: Use only recent data for repurchase\n",
    "    USE_RECENT_ONLY_REPURCHASE = True  # Set to True if kernel keeps crashing\n",
    "    REPURCHASE_RECENT_WEEKS = 8  # Only use last 8 weeks for repurchase\n",
    "    \n",
    "    # Item-to-Item CF parameters\n",
    "    MIN_ITEM_SUPPORT = 3\n",
    "    MAX_ITEM_NEIGHBORS = 30  # Reduced from 50\n",
    "    \n",
    "    # User-KNN parameters (ONLY for validation users)\n",
    "    N_SIMILAR_USERS = 20  # Reduced from 30\n",
    "    MIN_COMMON_ITEMS = 2\n",
    "    \n",
    "    # Time decay\n",
    "    REPURCHASE_DECAY_RATE = 0.05\n",
    "    POPULARITY_WINDOW_WEEKS = 2\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3573b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def time_decay_score(days_ago, decay_rate=0.05):\n",
    "    \"\"\"Vectorized time decay\"\"\"\n",
    "    return np.exp(-decay_rate * days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce5bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Loading data files...\n",
      "âœ“ Train transactions: 2,123,394\n",
      "  ðŸ’¾ Memory: 0.32 GB\n",
      "âœ“ Users: 224,306, Items: 31,541, Val users: 75,481\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"LOADING DATA\")\n",
    "\n",
    "print(\"Loading data files...\")\n",
    "train_transactions = pd.read_parquet(config.DATA_PATH / 'train_transactions.parquet')\n",
    "val_ground_truth = pd.read_parquet(config.DATA_PATH / 'val_ground_truth.parquet')\n",
    "articles = pd.read_parquet(config.DATA_PATH / 'articles.parquet')\n",
    "\n",
    "print(f\"âœ“ Train transactions: {len(train_transactions):,}\")\n",
    "print_memory()\n",
    "\n",
    "all_users = train_transactions['customer_id'].unique()\n",
    "all_items = train_transactions['article_id'].unique()\n",
    "val_users = set(val_ground_truth['customer_id'].unique())\n",
    "max_date = train_transactions['t_dat'].max()\n",
    "\n",
    "print(f\"âœ“ Users: {len(all_users):,}, Items: {len(all_items):,}, Val users: {len(val_users):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ec0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "  STRATEGY 1: REPURCHASE (CHUNKED)\n",
      "================================================================================\n",
      "\n",
      "Processing in chunks to save memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8299956100d64966bcb7ead5e822d1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "User chunks:   0%|          | 0/224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     15\u001b[0m chunk_trans \u001b[38;5;241m=\u001b[39m train_transactions[\n\u001b[1;32m     16\u001b[0m     train_transactions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(user_chunk)\n\u001b[1;32m     17\u001b[0m ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get last purchase per user-item\u001b[39;00m\n\u001b[1;32m     20\u001b[0m user_item_last \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     21\u001b[0m     \u001b[43mchunk_trans\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcustomer_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt_dat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m---> 23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculate scores (vectorized) - with NaN handling\u001b[39;00m\n\u001b[1;32m     27\u001b[0m user_item_last[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdays_ago\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (max_date \u001b[38;5;241m-\u001b[39m user_item_last[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_dat\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdays\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:3337\u001b[0m, in \u001b[0;36mGroupBy.max\u001b[0;34m(self, numeric_only, min_count, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   3329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   3330\u001b[0m         grouped_min_max,\n\u001b[1;32m   3331\u001b[0m         executor\u001b[38;5;241m.\u001b[39midentity_dtype_mapping,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3334\u001b[0m         is_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   3335\u001b[0m     )\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3340\u001b[0m \u001b[43m        \u001b[49m\u001b[43malias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnpfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1908\u001b[0m, in \u001b[0;36mGroupBy._agg_general\u001b[0;34m(self, numeric_only, min_count, alias, npfunc, **kwargs)\u001b[0m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_agg_general\u001b[39m(\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1906\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1907\u001b[0m ):\n\u001b[0;32m-> 1908\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnpfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:2009\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   2008\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_idxmax_idxmin(res)\n\u001b[0;32m-> 2009\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_aggregated_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2011\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39minfer_objects(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1620\u001b[0m, in \u001b[0;36mGroupBy._wrap_aggregated_output\u001b[0;34m(self, result, qs)\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"_maybe_transpose_result\" of \"GroupBy\" has\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;66;03m# incompatible type \"Union[Series, DataFrame]\"; expected \"NDFrameT\"\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_transpose_result(result)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:5678\u001b[0m, in \u001b[0;36mGroupBy._reindex_output\u001b[0;34m(self, output, fill_value, qs, method)\u001b[0m\n\u001b[1;32m   5675\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdrop(labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(g_names), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5677\u001b[0m \u001b[38;5;66;03m# Set a temp index and reindex (possibly expanding)\u001b[39;00m\n\u001b[0;32m-> 5678\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   5680\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5682\u001b[0m \u001b[38;5;66;03m# Reset in-axis grouper columns\u001b[39;00m\n\u001b[1;32m   5683\u001b[0m \u001b[38;5;66;03m# (using level numbers `g_nums` because level names may not be unique)\u001b[39;00m\n\u001b[1;32m   5684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(in_axis_grps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/frame.py:5400\u001b[0m, in \u001b[0;36mDataFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5381\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   5382\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,\n\u001b[1;32m   5383\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5398\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5399\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m-> 5400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/generic.py:5632\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[1;32m   5631\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[0;32m-> 5632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5633\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m   5634\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/generic.py:5655\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5652\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   5654\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[0;32m-> 5655\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[1;32m   5657\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5659\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n\u001b[1;32m   5660\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   5661\u001b[0m     {axis: [new_index, indexer]},\n\u001b[1;32m   5662\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   5663\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   5664\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5665\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/indexes/base.py:4429\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 4429\u001b[0m         indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4430\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\n\u001b[1;32m   4431\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4432\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi:\n\u001b[1;32m   4433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot handle a non-unique multi-index!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/indexes/base.py:3960\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3955\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m this\u001b[38;5;241m.\u001b[39m_get_indexer(\n\u001b[1;32m   3957\u001b[0m         target, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[1;32m   3958\u001b[0m     )\n\u001b[0;32m-> 3960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/indexes/base.py:3981\u001b[0m, in \u001b[0;36mIndex._get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3978\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\n\u001b[1;32m   3979\u001b[0m     \u001b[38;5;66;03m# error: Item \"IndexEngine\" of \"Union[IndexEngine, ExtensionEngine]\"\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m     \u001b[38;5;66;03m# has no attribute \"_extract_level_codes\"\u001b[39;00m\n\u001b[0;32m-> 3981\u001b[0m     tgt_values \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_level_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[1;32m   3982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3985\u001b[0m     tgt_values \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39m_get_engine_target()\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:761\u001b[0m, in \u001b[0;36mpandas._libs.index.BaseMultiIndexCodesEngine._extract_level_codes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/assignment2/lib/python3.10/site-packages/pandas/core/indexes/multi.py:160\u001b[0m, in \u001b[0;36mMultiIndexUIntEngine._codes_to_ints\u001b[0;34m(self, codes)\u001b[0m\n\u001b[1;32m    155\u001b[0m codes \u001b[38;5;241m<<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffsets\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Now sum and OR are in fact interchangeable. This is a simple\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# composition of the (disjunct) significant bits of each level (i.e.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# each column in \"codes\") in a single positive integer:\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m codes\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Single key\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mbitwise_or\u001b[38;5;241m.\u001b[39mreduce(codes)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Multiple keys\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 1: REPURCHASE - CHUNKED PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 1: REPURCHASE (CHUNKED)\")\n",
    "\n",
    "print(\"Processing in chunks to save memory...\")\n",
    "repurchase_chunks = []\n",
    "\n",
    "# Split users into chunks\n",
    "user_chunks = np.array_split(all_users, max(1, len(all_users) // config.USER_CHUNK_SIZE))\n",
    "\n",
    "for i, user_chunk in enumerate(tqdm(user_chunks, desc=\"User chunks\")):\n",
    "    # Filter transactions for this chunk\n",
    "    chunk_trans = train_transactions[\n",
    "        train_transactions['customer_id'].isin(user_chunk)\n",
    "    ].copy()\n",
    "    \n",
    "    # Get last purchase per user-item\n",
    "    user_item_last = (\n",
    "        chunk_trans\n",
    "        .groupby(['customer_id', 'article_id'], as_index=False)['t_dat']\n",
    "        .max()\n",
    "    )\n",
    "    \n",
    "    # Calculate scores (vectorized) - with NaN handling\n",
    "    user_item_last['days_ago'] = (max_date - user_item_last['t_dat']).dt.days\n",
    "    \n",
    "    # Drop any NaN values before converting to int\n",
    "    user_item_last = user_item_last.dropna(subset=['days_ago'])\n",
    "    \n",
    "    # Now safe to convert to int\n",
    "    user_item_last['days_ago'] = user_item_last['days_ago'].astype(np.int16)\n",
    "    user_item_last['repurchase_score'] = time_decay_score(\n",
    "        user_item_last['days_ago'].values, \n",
    "        config.REPURCHASE_DECAY_RATE\n",
    "    ).astype(np.float32)\n",
    "    \n",
    "    # Get top N per user\n",
    "    top_candidates = (\n",
    "        user_item_last\n",
    "        .sort_values(['customer_id', 'repurchase_score'], ascending=[True, False])\n",
    "        .groupby('customer_id', as_index=False)\n",
    "        .head(config.N_REPURCHASE_CANDIDATES)\n",
    "        [['customer_id', 'article_id', 'repurchase_score']]\n",
    "    )\n",
    "    \n",
    "    repurchase_chunks.append(top_candidates)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk_trans, user_item_last, top_candidates\n",
    "    force_garbage_collection()\n",
    "\n",
    "# Combine chunks\n",
    "repurchase_candidates = pd.concat(repurchase_chunks, ignore_index=True)\n",
    "del repurchase_chunks\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"âœ“ Generated {len(repurchase_candidates):,} repurchase candidates\")\n",
    "print_memory()\n",
    "\n",
    "# Save intermediate result\n",
    "repurchase_candidates.to_parquet(config.OUTPUT_PATH / 'temp_repurchase.parquet', index=False)\n",
    "del repurchase_candidates\n",
    "force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 2: POPULARITY\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 2: POPULARITY\")\n",
    "\n",
    "cutoff_date = max_date - timedelta(weeks=config.POPULARITY_WINDOW_WEEKS)\n",
    "recent_trans = train_transactions[train_transactions['t_dat'] >= cutoff_date].copy()\n",
    "\n",
    "print(f\"Using {len(recent_trans):,} recent transactions\")\n",
    "\n",
    "# Vectorized calculations\n",
    "recent_trans['days_ago'] = (max_date - recent_trans['t_dat']).dt.days\n",
    "\n",
    "# Drop NaN values\n",
    "recent_trans = recent_trans.dropna(subset=['days_ago'])\n",
    "\n",
    "# Convert to int\n",
    "recent_trans['days_ago'] = recent_trans['days_ago'].astype(np.int16)\n",
    "recent_trans['weight'] = time_decay_score(recent_trans['days_ago'].values, 0.1).astype(np.float32)\n",
    "\n",
    "# Aggregate\n",
    "item_popularity = (\n",
    "    recent_trans\n",
    "    .groupby('article_id', as_index=False)\n",
    "    .agg({'weight': 'sum', 'customer_id': 'nunique'})\n",
    "    .rename(columns={'weight': 'weighted_purchases', 'customer_id': 'unique_buyers'})\n",
    ")\n",
    "\n",
    "item_popularity['popularity_score'] = (\n",
    "    0.7 * item_popularity['weighted_purchases'] + \n",
    "    0.3 * item_popularity['unique_buyers']\n",
    ")\n",
    "item_popularity['popularity_score'] = (\n",
    "    item_popularity['popularity_score'] / item_popularity['popularity_score'].max()\n",
    ").astype(np.float32)\n",
    "\n",
    "# Get top items\n",
    "top_items = item_popularity.nlargest(config.N_POPULARITY_CANDIDATES, 'popularity_score')\n",
    "\n",
    "print(f\"âœ“ Top {len(top_items)} popular items\")\n",
    "\n",
    "# Create candidates - CHUNKED\n",
    "print(\"Creating popularity candidates in chunks...\")\n",
    "pop_chunks = []\n",
    "\n",
    "for user_chunk in tqdm(np.array_split(all_users, 20), desc=\"Popularity chunks\"):\n",
    "    chunk_df = pd.DataFrame({\n",
    "        'customer_id': np.repeat(user_chunk, len(top_items)),\n",
    "        'article_id': np.tile(top_items['article_id'].values, len(user_chunk))\n",
    "    })\n",
    "    \n",
    "    rank_penalty = np.tile(1 - np.arange(len(top_items)) * 0.01, len(user_chunk))\n",
    "    scores = np.tile(top_items['popularity_score'].values, len(user_chunk))\n",
    "    chunk_df['popularity_score'] = (scores * rank_penalty).astype(np.float32)\n",
    "    \n",
    "    pop_chunks.append(chunk_df)\n",
    "\n",
    "popularity_candidates = pd.concat(pop_chunks, ignore_index=True)\n",
    "del pop_chunks, recent_trans\n",
    "force_garbage_collection()\n",
    "\n",
    "print(f\"âœ“ Generated {len(popularity_candidates):,} popularity candidates\")\n",
    "print_memory()\n",
    "\n",
    "# Save\n",
    "popularity_candidates.to_parquet(config.OUTPUT_PATH / 'temp_popularity.parquet', index=False)\n",
    "item_popularity.to_parquet(config.OUTPUT_PATH / 'item_popularity.parquet', index=False)\n",
    "del popularity_candidates\n",
    "force_garbage_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 3: CO-PURCHASE (Item-to-Item CF)\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 3: CO-PURCHASE (Item-to-Item CF)\")\n",
    "\n",
    "print(\"Building co-purchase matrix...\")\n",
    "\n",
    "# Create item-to-item co-purchase matrix\n",
    "# Group by transaction/basket (same user, same day)\n",
    "train_transactions['basket_id'] = (\n",
    "    train_transactions['customer_id'].astype(str) + '_' + \n",
    "    train_transactions['t_dat'].astype(str)\n",
    ")\n",
    "\n",
    "# Get baskets with multiple items\n",
    "basket_items = (\n",
    "    train_transactions\n",
    "    .groupby('basket_id')['article_id']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Filter baskets with at least 2 items\n",
    "basket_items = basket_items[basket_items['article_id'].apply(len) >= 2]\n",
    "print(f\"  Baskets with 2+ items: {len(basket_items):,}\")\n",
    "\n",
    "# Build co-purchase counts\n",
    "print(\"Computing co-purchase frequencies...\")\n",
    "copurchase_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for items in tqdm(basket_items['article_id'], desc=\"Processing baskets\"):\n",
    "    # For each pair of items in the basket\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):\n",
    "            item1, item2 = items[i], items[j]\n",
    "            copurchase_counts[item1][item2] += 1\n",
    "            copurchase_counts[item2][item1] += 1\n",
    "\n",
    "print(f\"âœ“ Built co-purchase matrix for {len(copurchase_counts):,} items\")\n",
    "\n",
    "# Convert to item-to-item similarity scores\n",
    "print(\"Computing item-to-item similarity scores...\")\n",
    "item_to_items = {}\n",
    "\n",
    "for item1 in tqdm(copurchase_counts.keys(), desc=\"Computing similarities\"):\n",
    "    # Get co-purchased items\n",
    "    copurchased = copurchase_counts[item1]\n",
    "    \n",
    "    # Filter by minimum support\n",
    "    copurchased = {\n",
    "        item2: count \n",
    "        for item2, count in copurchased.items() \n",
    "        if count >= config.MIN_ITEM_SUPPORT\n",
    "    }\n",
    "    \n",
    "    if copurchased:\n",
    "        # Sort by count and take top K\n",
    "        top_items = sorted(\n",
    "            copurchased.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:config.MAX_ITEM_NEIGHBORS]\n",
    "        \n",
    "        # Normalize scores\n",
    "        max_count = top_items[0][1]\n",
    "        item_to_items[item1] = [\n",
    "            (item2, count / max_count) \n",
    "            for item2, count in top_items\n",
    "        ]\n",
    "\n",
    "print(f\"âœ“ Computed similarities for {len(item_to_items):,} items\")\n",
    "\n",
    "# Generate co-purchase candidates for each user\n",
    "print(\"Generating co-purchase candidates...\")\n",
    "copurchase_candidates = []\n",
    "\n",
    "# Get recent purchases for each user (last 10)\n",
    "user_recent_items = (\n",
    "    train_transactions\n",
    "    .sort_values('t_dat', ascending=False)\n",
    "    .groupby('customer_id')['article_id']\n",
    "    .apply(lambda x: list(x.unique()[:10]))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "for user in tqdm(all_users, desc=\"User co-purchase recommendations\"):\n",
    "    if user not in user_recent_items:\n",
    "        continue\n",
    "    \n",
    "    user_items = user_recent_items[user]\n",
    "    candidate_scores = defaultdict(float)\n",
    "    \n",
    "    # Aggregate scores from all user's items\n",
    "    for user_item in user_items:\n",
    "        if user_item in item_to_items:\n",
    "            for similar_item, score in item_to_items[user_item]:\n",
    "                if similar_item not in user_items:  # Don't recommend already purchased\n",
    "                    candidate_scores[similar_item] += score\n",
    "    \n",
    "    # Get top N candidates\n",
    "    if candidate_scores:\n",
    "        top_candidates = sorted(\n",
    "            candidate_scores.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )[:config.N_COPURCHASE_CANDIDATES]\n",
    "        \n",
    "        for item, score in top_candidates:\n",
    "            copurchase_candidates.append({\n",
    "                'customer_id': user,\n",
    "                'article_id': item,\n",
    "                'copurchase_score': score\n",
    "            })\n",
    "\n",
    "copurchase_candidates = pd.DataFrame(copurchase_candidates)\n",
    "print(f\"âœ“ Generated {len(copurchase_candidates):,} co-purchase candidates\")\n",
    "print(f\"  Users with candidates: {copurchase_candidates['customer_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 4: USER-KNN COLLABORATIVE FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 4: USER-KNN COLLABORATIVE FILTERING\")\n",
    "\n",
    "print(\"Building user-item matrix...\")\n",
    "\n",
    "# Create sparse user-item matrix (binary: 1 if purchased, 0 otherwise)\n",
    "# Map users and items to indices\n",
    "user_to_idx = {user: idx for idx, user in enumerate(all_users)}\n",
    "item_to_idx = {item: idx for idx, item in enumerate(all_items)}\n",
    "\n",
    "# Create matrix\n",
    "n_users = len(all_users)\n",
    "n_items = len(all_items)\n",
    "\n",
    "print(f\"  Matrix size: {n_users:,} users x {n_items:,} items\")\n",
    "\n",
    "# Use last 4 weeks for user similarity (more recent = more relevant)\n",
    "recent_date = max_date - timedelta(weeks=4)\n",
    "recent_user_items = train_transactions[train_transactions['t_dat'] >= recent_date].copy()\n",
    "\n",
    "user_item_matrix = lil_matrix((n_users, n_items), dtype=np.int8)\n",
    "\n",
    "print(\"Populating user-item matrix...\")\n",
    "for _, row in tqdm(recent_user_items.iterrows(), total=len(recent_user_items), desc=\"Building matrix\"):\n",
    "    user_idx = user_to_idx[row['customer_id']]\n",
    "    item_idx = item_to_idx[row['article_id']]\n",
    "    user_item_matrix[user_idx, item_idx] = 1\n",
    "\n",
    "# Convert to CSR for efficient operations\n",
    "user_item_matrix = user_item_matrix.tocsr()\n",
    "print(f\"âœ“ Matrix density: {user_item_matrix.nnz / (n_users * n_items) * 100:.4f}%\")\n",
    "\n",
    "# Compute user-user similarity (only for validation users to save memory)\n",
    "print(f\"Computing user similarities for {len(val_users):,} validation users...\")\n",
    "\n",
    "val_user_indices = [user_to_idx[user] for user in val_users if user in user_to_idx]\n",
    "val_user_matrix = user_item_matrix[val_user_indices]\n",
    "\n",
    "# Normalize rows\n",
    "val_user_matrix_norm = normalize(val_user_matrix, norm='l2', axis=1)\n",
    "user_item_matrix_norm = normalize(user_item_matrix, norm='l2', axis=1)\n",
    "\n",
    "# Compute similarity (batch processing to avoid memory issues)\n",
    "print(\"Computing cosine similarities...\")\n",
    "batch_size = 1000\n",
    "userknn_candidates = []\n",
    "\n",
    "for i in tqdm(range(0, len(val_user_indices), batch_size), desc=\"Similarity batches\"):\n",
    "    batch_indices = val_user_indices[i:i+batch_size]\n",
    "    batch_matrix = val_user_matrix_norm[i:i+batch_size]\n",
    "    \n",
    "    # Compute similarity with all users\n",
    "    similarities = cosine_similarity(batch_matrix, user_item_matrix_norm)\n",
    "    \n",
    "    # For each user in batch\n",
    "    for j, user_idx in enumerate(batch_indices):\n",
    "        user = all_users[user_idx]\n",
    "        user_sims = similarities[j]\n",
    "        \n",
    "        # Get top similar users (exclude self)\n",
    "        similar_user_indices = np.argsort(user_sims)[::-1][1:config.N_SIMILAR_USERS+1]\n",
    "        \n",
    "        # Get items purchased by similar users\n",
    "        candidate_scores = defaultdict(float)\n",
    "        user_purchased = set(\n",
    "            train_transactions[train_transactions['customer_id'] == user]['article_id']\n",
    "        )\n",
    "        \n",
    "        for sim_user_idx in similar_user_indices:\n",
    "            sim_score = user_sims[sim_user_idx]\n",
    "            if sim_score < 0.01:  # Skip very dissimilar users\n",
    "                continue\n",
    "            \n",
    "            sim_user = all_users[sim_user_idx]\n",
    "            sim_user_items = train_transactions[\n",
    "                train_transactions['customer_id'] == sim_user\n",
    "            ]['article_id'].unique()\n",
    "            \n",
    "            for item in sim_user_items:\n",
    "                if item not in user_purchased:\n",
    "                    candidate_scores[item] += sim_score\n",
    "        \n",
    "        # Get top N candidates\n",
    "        if candidate_scores:\n",
    "            top_candidates = sorted(\n",
    "                candidate_scores.items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            )[:config.N_USERKNN_CANDIDATES]\n",
    "            \n",
    "            for item, score in top_candidates:\n",
    "                userknn_candidates.append({\n",
    "                    'customer_id': user,\n",
    "                    'article_id': item,\n",
    "                    'userknn_score': score\n",
    "                })\n",
    "\n",
    "userknn_candidates = pd.DataFrame(userknn_candidates)\n",
    "print(f\"âœ“ Generated {len(userknn_candidates):,} user-KNN candidates\")\n",
    "\n",
    "# Clean up memory\n",
    "del user_item_matrix, val_user_matrix, val_user_matrix_norm, user_item_matrix_norm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STRATEGY 5: CATEGORY-BASED RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"STRATEGY 5: CATEGORY-BASED RECOMMENDATIONS\")\n",
    "\n",
    "print(\"Computing user category preferences...\")\n",
    "\n",
    "# Get user's category preferences\n",
    "user_categories = (\n",
    "    train_transactions\n",
    "    .merge(articles[['article_id', 'product_type_no', 'product_group_name']], on='article_id')\n",
    "    .groupby(['customer_id', 'product_type_no'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "\n",
    "# Get top 3 categories per user\n",
    "user_top_categories = (\n",
    "    user_categories\n",
    "    .sort_values(['customer_id', 'count'], ascending=[True, False])\n",
    "    .groupby('customer_id')\n",
    "    .head(3)\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Computed preferences for {user_top_categories['customer_id'].nunique():,} users\")\n",
    "\n",
    "# Get popular items per category\n",
    "category_popular_items = (\n",
    "    train_transactions[train_transactions['t_dat'] >= cutoff_date]\n",
    "    .merge(articles[['article_id', 'product_type_no']], on='article_id')\n",
    "    .groupby(['product_type_no', 'article_id'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values(['product_type_no', 'count'], ascending=[True, False])\n",
    "    .groupby('product_type_no')\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"Generating category-based candidates...\")\n",
    "category_candidates = []\n",
    "\n",
    "for _, row in tqdm(user_top_categories.iterrows(), total=len(user_top_categories), desc=\"Category recommendations\"):\n",
    "    user = row['customer_id']\n",
    "    category = row['product_type_no']\n",
    "    \n",
    "    # Get popular items in this category\n",
    "    category_items = category_popular_items[\n",
    "        category_popular_items['product_type_no'] == category\n",
    "    ]['article_id'].tolist()\n",
    "    \n",
    "    # Get user's purchased items\n",
    "    user_items = set(\n",
    "        train_transactions[train_transactions['customer_id'] == user]['article_id']\n",
    "    )\n",
    "    \n",
    "    # Recommend items not yet purchased\n",
    "    for rank, item in enumerate(category_items):\n",
    "        if item not in user_items and rank < config.N_CATEGORY_CANDIDATES:\n",
    "            category_candidates.append({\n",
    "                'customer_id': user,\n",
    "                'article_id': item,\n",
    "                'category_score': 1.0 / (rank + 1)  # Rank-based score\n",
    "            })\n",
    "\n",
    "category_candidates = pd.DataFrame(category_candidates)\n",
    "print(f\"âœ“ Generated {len(category_candidates):,} category-based candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c763245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMBINE ALL CANDIDATES\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"COMBINING ALL RECALL STRATEGIES\")\n",
    "\n",
    "print(\"Merging candidates from all strategies...\")\n",
    "\n",
    "# Merge all candidates\n",
    "all_candidates = repurchase_candidates.copy()\n",
    "\n",
    "# Merge popularity\n",
    "all_candidates = all_candidates.merge(\n",
    "    popularity_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Merge co-purchase\n",
    "all_candidates = all_candidates.merge(\n",
    "    copurchase_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Merge user-KNN\n",
    "all_candidates = all_candidates.merge(\n",
    "    userknn_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Merge category\n",
    "all_candidates = all_candidates.merge(\n",
    "    category_candidates,\n",
    "    on=['customer_id', 'article_id'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Fill NaN scores with 0\n",
    "score_columns = [\n",
    "    'repurchase_score', 'popularity_score', 'copurchase_score', \n",
    "    'userknn_score', 'category_score'\n",
    "]\n",
    "all_candidates[score_columns] = all_candidates[score_columns].fillna(0)\n",
    "\n",
    "print(f\"âœ“ Total unique user-item pairs: {len(all_candidates):,}\")\n",
    "\n",
    "# Count how many strategies recommend each item\n",
    "all_candidates['n_strategies'] = (all_candidates[score_columns] > 0).sum(axis=1)\n",
    "\n",
    "print(\"\\nCandidate statistics:\")\n",
    "print(f\"  Candidates per user: {len(all_candidates) / len(all_users):.2f}\")\n",
    "print(f\"  Avg strategies per candidate: {all_candidates['n_strategies'].mean():.2f}\")\n",
    "print(\"\\n  Candidates by number of strategies:\")\n",
    "for n in sorted(all_candidates['n_strategies'].unique()):\n",
    "    count = (all_candidates['n_strategies'] == n).sum()\n",
    "    pct = count / len(all_candidates) * 100\n",
    "    print(f\"    {n} strategies: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de452ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATE RECALL ON VALIDATION SET\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"EVALUATING RECALL PERFORMANCE\")\n",
    "\n",
    "# Filter to validation users only\n",
    "val_candidates = all_candidates[all_candidates['customer_id'].isin(val_users)].copy()\n",
    "\n",
    "print(f\"Validation candidates: {len(val_candidates):,}\")\n",
    "print(f\"  Avg per user: {len(val_candidates) / len(val_users):.2f}\")\n",
    "\n",
    "# Calculate recall@K for different K values\n",
    "print(\"\\nRecall@K metrics:\")\n",
    "\n",
    "val_ground_truth_dict = dict(zip(\n",
    "    val_ground_truth['customer_id'], \n",
    "    val_ground_truth['purchased_articles']\n",
    "))\n",
    "\n",
    "for k in [12, 50, 100]:\n",
    "    recalls = []\n",
    "    \n",
    "    for user in val_users:\n",
    "        if user in val_ground_truth_dict:\n",
    "            true_items = set(val_ground_truth_dict[user])\n",
    "            \n",
    "            # Get top K candidates for this user\n",
    "            user_candidates = (\n",
    "                val_candidates[val_candidates['customer_id'] == user]\n",
    "                .nlargest(k, 'n_strategies')  # Simple ranking by coverage\n",
    "                ['article_id']\n",
    "                .tolist()\n",
    "            )\n",
    "            \n",
    "            if user_candidates and true_items:\n",
    "                recall = len(set(user_candidates) & true_items) / len(true_items)\n",
    "                recalls.append(recall)\n",
    "            else:\n",
    "                recalls.append(0)\n",
    "    \n",
    "    print(f\"  Recall@{k}: {np.mean(recalls):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE CANDIDATES\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"SAVING CANDIDATES\")\n",
    "\n",
    "print(\"Saving candidate files...\")\n",
    "\n",
    "# Save all candidates\n",
    "all_candidates.to_parquet(config.OUTPUT_PATH / 'recall_candidates.parquet', index=False)\n",
    "print(f\"  âœ“ recall_candidates.parquet ({len(all_candidates):,} rows, {len(all_candidates) * len(score_columns) * 4 / 1024**2:.2f} MB)\")\n",
    "\n",
    "# Save item-to-item similarity for later use\n",
    "with open(config.OUTPUT_PATH / 'item_to_items.pkl', 'wb') as f:\n",
    "    pickle.dump(item_to_items, f)\n",
    "print(f\"  âœ“ item_to_items.pkl\")\n",
    "\n",
    "# Save popularity scores\n",
    "item_popularity.to_parquet(config.OUTPUT_PATH / 'item_popularity.parquet', index=False)\n",
    "print(f\"  âœ“ item_popularity.parquet\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print_section(\"RECALL STAGE COMPLETE!\")\n",
    "\n",
    "print(\"\\nRecall strategy summary:\")\n",
    "print(f\"  Strategy 1 - Repurchase: {len(repurchase_candidates):,} candidates\")\n",
    "print(f\"  Strategy 2 - Popularity: {len(popularity_candidates):,} candidates\")\n",
    "print(f\"  Strategy 3 - Co-Purchase: {len(copurchase_candidates):,} candidates\")\n",
    "print(f\"  Strategy 4 - User-KNN: {len(userknn_candidates):,} candidates\")\n",
    "print(f\"  Strategy 5 - Category: {len(category_candidates):,} candidates\")\n",
    "print(f\"  Total unique pairs: {len(all_candidates):,}\")\n",
    "\n",
    "print(\"\\nâœ… Ready for Stage 3: Feature Engineering!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review recall_candidates.parquet\")\n",
    "print(\"  2. Check recall metrics above\")\n",
    "print(\"  3. Proceed to Stage 3 when ready\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2",
   "language": "python",
   "name": "assignment2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
